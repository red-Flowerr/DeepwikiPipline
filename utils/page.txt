"In the volcengine/verl repository, consider the following component.\n\nFile: Installation\n\nDesign intent: We need to rewrite the documentation as 2-3 sentences focusing on design intent and reasoning behind the approach. Avoid phrases like 'This section' or 'This component', instead explain motivation directly. So we need to condense the entire overview into 2-3 sentences. The content: verl is a flexible RLHF training framework for LLMs, uses HybridFlow programming model, Ray orchestration, supports multiple backends (FSDP, Megatron, vLLM, SGLang), modular, efficient, scalable, resource management, etc. So we need to explain why it's designed this way: to enable flexible, efficient, scalable RLHF training across a range of model sizes and hardware, with modularity and hybrid programming model to combine single- and multi-controller paradigms, Ray for distributed orchestration, backend abstraction to support various training and inference engines, dynamic resource allocation to reduce memory usage, etc. 2-3 sentences. Let's craft.\n\nSentence 1: \"verl was built to make RLHF training of large language models both flexible and efficient, so researchers can experiment with a wide range of algorithms and model sizes while still scaling to hundreds of GPUs.\" Sentence 2: \"By combining a hybrid controller programming model with Rayâ€‘based\n\nSource: README.md:1-80\n```markdown\n<div align=\"center\">\n ðŸ‘‹ Hi, everyone!\n    verl is a RL training library initiated by <b>ByteDance Seed team</b> and maintained by the verl community.\n    <br>\n    <br>\n</div>\n\n<div align=\"center\">\n\n<a href=\" src=\" alt=\"Ask DeepWiki.com\" style=\"height:20px;\"></a>\nGitHub Repo stars\nTwitter\n<a href=\" src=\"\n<a href=\" src=\"\nDocumentation\n<a href=\" src=\"\n\n</div>\n\nseed logo\n\n<h1 style=\"text-align: center;\">verl: Volcano Engine Reinforcement Learning for LLMs</h1>\n\nverl is a flexible, efficient and production-ready RL training library for large language models (LLMs).\n\nverl is the open-source version of **HybridFlow: A Flexible and Efficient RLHF Framework** paper.\n\nverl is flexible and easy to use with:\n\n- **Easy extension of diverse RL algorithms**: The hybrid-controller programming model enables flexible representation and efficient execution of complex post-training dataflows. Build RL dataflows such as GRPO, PPO in a few lines of code.\n\n- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as FSDP, Megatron-LM, vLLM, SGLang, etc\n\n- **Flexible device mapping**: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.\n\n- Ready integration with popular HuggingFace models\n\nverl is fast with:\n\n- **State-of-the-art throughput**: SOTA LLM training and inference engine integrations and SOTA RL throughput.\n\n- **Efficient actor model resharding with 3D-HybridEngine**: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.\n\n</p>\n\n## News\n- 2025/11 recipes have been moved to a new repository: verl-recipe\n- 2025/10 verl is presented in the PyTorch Conference 2025.\n- 2025/08 verl is presented in the PyTorch Expert Exchange Webinar. Slides available.\n- 2025/07 The ReTool recipe is fully open sourced. Blog\n- 2025/07 The first verl meetup will be held at ICML Vancouver on July 16th! Please join us if you are at ICML! (onsite only)\n- 2025/06 verl with Megatron backend enables large MoE models such as DeepSeek-671B and Qwen3-235B.\n- 2025/03 DAPO is the open-sourced SOTA RL algorithm that achieves 50 points on AIME 2024 based on the Qwen2.5-32B pre-trained model, surpassing the previous SOTA achieved by DeepSeek's GRPO (DeepSeek-R1-Zero-Qwen-32B). DAPO's training is fully powered by verl and the reproduction code is available in `recipe/dapo` now.\n<details><summary> more... </summary>\n<ul>\n  <li>2025/04 Seed-Thinking-v1.5 tech report is released! Trained with verl, Seed-Thinking-v1.5 achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on GPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond reasoning tasks, the method demonstrates notable generalization across diverse domains.</li>\n  <li>2025/07 verl keynote at AWS AI Hours Singapore on 7/8, verl & verl-agent project updates at Agent for SWE meetup by LF AI & Data Singapore on 7/11.</li>\n  <li>2025/06 verl team will provide latest project updates at PyTorch Day China on June 7th. Meet our dev team in Beijing!</li>\n  <li> 2025/04 VAPO (value-based augmented PPO) paper covers our latest RL method for reasoning models. Trained from Qwen-32B-base model, VAPO achieves 60.4 on AIME 2024, outperforming DAPO-32B.</li>\n  <li>2025/05 PF-PPO, accepted to ICML 2025, is now supported in verl! PF-PPO enhances policy learning efficiency and robustness by filtering potentially noisy reward signals and reusing high-quality experiences via a replay buffer.</li>\n  <li>2025/04 We will give a tutorial about latest post-training techniques and programming guide for verl at ICLR 2025 Expo, SCI-FM workshop and LMSys afterparty. Talk materials available here. </li>\n  <li>2025/03 verl v0.3.0.post1 is released! See release note for details. It achieves ~1.4x speedup compared to prev versions.</li>\n  <li>2025/05 verl will be presented at A2M Shanghai on 5/16 - 5/17.</li>\n  <li>2025/05 verl will be presented at GOSIM x PyTorch Day 2025. See you in Paris! </li>\n  <li>2025/03 We introduced the programming model of verl at the vLLM Beijing Meetup and verl intro and updates at the SGLang-LMSYS Org Meetup in Sunnyvale mid-March.</li>\n  <li>2025/03 We will present verl(HybridFlow) at EuroSys 2025. See you in Rotterdam!</li>\n  <li>2025/02 verl v0.2.0.post2 is released!</li>\n  <li>2025/02 We presented verl in the <a href=\" Ray Meetup</a>. See you in San Jose!</li>\n  <li>2025/01 Doubao-1.5-pro is released with SOTA-level performance on LLM & VLM. The RL scaling preview model is trained using verl, reaching OpenAI O1-level performance on math benchmarks (70.0 pass@1 on AIME).</li>\n  <li>2024/12 verl is presented at Ray Forward 2024. Slides available <a href=\"\n  <li>2024/12 The team presented <a href=\" LLMs: From Algorithms to Infrastructure</a> at NeurIPS 2024. <a href=\" and <a href=\" available.</li>\n  <li>2024/10 verl is presented at Ray Summit. <a href=\" video</a> available.</li>\n  <li>2024/08 HybridFlow (verl) is accepted to EuroSys 2025.</li>\n</ul>\n</details>\n\n## Key Features\n\n- **FSDP**, **FSDP2** and **Megatron-LM** for training.\n- **vLLM**, **SGLang** and **HF Transformers** for rollout generation.\n```\n\nSource: docs/examples/gsm8k_example.rst:1-80\n```text\nGSM8K Example\n=============\n\nLast updated: 03/25/2025.\n\nIntroduction\n------------\n\nIn this example, we train an LLM to tackle the GSM8k task.\n\nPaper: \n\nDataset: \n\nNote that the original paper mainly focuses on training a verifier (a\nreward model) to solve math problems via Best-of-N sampling. In this\nexample, we train an RLHF agent using a rule-based reward model.\n\nDataset Introduction\n--------------------\n\nGSM8k is a math problem dataset. The prompt is an elementary school\nproblem. The LLM model is required to answer the math problem.\n\nThe training set contains 7473 samples and the test set contains 1319\nsamples.\n\n**An example**\n\nPrompt\n\n   Katy makes coffee using teaspoons of sugar and cups of water in the\n   ratio of 7:13. If she used a total of 120 teaspoons of sugar and cups\n   of water, calculate the number of teaspoonfuls of sugar she used.\n\nSolution\n\n   The total ratio representing the ingredients she used to make the\n   coffee is 7+13 = <<7+13=20>>20 Since the fraction representing the\n   number of teaspoons she used is 7/20, she used 7/20\\ *120 =\n   <<7/20*\\ 120=42>>42 #### 42\n\nStep 1: Prepare dataset\n-----------------------\n\n.. code:: bash\n\n   cd examples/data_preprocess\n   python3 gsm8k.py --local_save_dir ~/data/gsm8k\n\nStep 2: Download Model\n----------------------\n\nThere're three ways to prepare the model checkpoints for post-training:\n\n- Download the required models from huggingface or modelscope\n\n.. code:: bash\n\n   huggingface-cli download deepseek-ai/deepseek-math-7b-instruct --local-dir ~/models/deepseek-math-7b-instruct --local-dir-use-symlinks False\n   # or\n   modelscope download --model deepseek-ai/deepseek-math-7b-instruct --local_dir ~/models/deepseek-math-7b-instruct\n\n- Already store your store model in the local directory or HDFS path.\n- Also, you can directly use the model name in huggingface (e.g.,\n  deepseek-ai/deepseek-math-7b-instruct) in\n  ``actor_rollout_ref.model.path`` and ``critic.model.path`` field in\n  the run script. You can also download models from modelscope by setting environmental variable ``VERL_USE_MODELSCOPE=True``.\n  See examples/ppo_trainer/run_deepseek7b_llm_modelscope.sh for example.\n\nNoted that users should prepare checkpoints for actor, critic and reward\nmodel.\n\n[Optional] Step 3: SFT your Model\n---------------------------------\n\nWe provide a SFT Trainer using PyTorch FSDP in\n`fsdp_sft_trainer.py < \nUsers can customize their own SFT\nscript using our FSDP SFT Trainer.\n```\n\nSource: docs/index.rst:1-80\n```text\nWelcome to verl's documentation!\n================================================\n\nverl is a flexible, efficient and production-ready RL training framework designed for large language models (LLMs) post-training. It is an open source implementation of the `HybridFlow < paper.\n\nverl is flexible and easy to use with:\n\n- **Easy extension of diverse RL algorithms**: The hybrid programming model combines the strengths of single-controller and multi-controller paradigms to enable flexible representation and efficient execution of complex Post-Training dataflows. Allowing users to build RL dataflows in a few lines of code.\n\n- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as PyTorch FSDP, Megatron-LM, vLLM and SGLang. Moreover, users can easily extend to other LLM training and inference frameworks.\n\n- **Flexible device mapping and parallelism**: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.\n\n- Ready integration with popular HuggingFace models\n\n\nverl is fast with:\n\n- **State-of-the-art throughput**: By seamlessly integrating existing SOTA LLM training and inference frameworks, verl achieves high generation and training throughput.\n\n- **Efficient actor model resharding with 3D-HybridEngine**: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.\n\n--------------------------------------------\n\n.. _Contents:\n\n.. toctree::\n   :maxdepth: 2\n   :caption: Quickstart\n\n   start/install\n   start/quickstart\n   start/multinode\n   start/ray_debug_tutorial\n   start/more_resources\n   start/agentic_rl\n\n.. toctree::\n   :maxdepth: 2\n   :caption: Programming guide\n\n   hybrid_flow\n   single_controller\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Data Preparation\n\n   preparation/prepare_data\n   preparation/reward_function\n\n.. toctree::\n   :maxdepth: 2\n   :caption: Configurations\n\n   examples/config\n\n.. toctree::\n   :maxdepth: 1\n   :caption: PPO Example\n\n   examples/ppo_code_architecture\n   examples/gsm8k_example\n   examples/multi_modal_example\n   examples/skypilot_examples\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Algorithms\n\n   algo/ppo.md\n   algo/grpo.md\n   algo/collabllm.md\n   algo/dapo.md\n   algo/spin.md\n   algo/sppo.md\n   algo/entropy.md\n   algo/opo.md\n   algo/baseline.md\n   algo/gpg.md\n```\n\nSource: docs/start/install.rst:1-80\n```text\nInstallation\n============\n\nRequirements\n------------\n\n- **Python**: Version >= 3.10\n- **CUDA**: Version >= 12.8\n\nverl supports various backends. Currently, the following configurations are available:\n\n- **FSDP** and **Megatron-LM** (optional) for training.\n- **SGLang**, **vLLM** and **TGI** for rollout generation.\n\nChoices of Backend Engines\n----------------------------\n\n1. Training:\n\nWe recommend using **FSDP** backend to investigate, research and prototype different models, datasets and RL algorithms. The guide for using FSDP backend can be found in :doc:`FSDP Workers<../workers/fsdp_workers>`.\n\nFor users who pursue better scalability, we recommend using **Megatron-LM** backend. Currently, we support `Megatron-LM v0.13.1 < The guide for using Megatron-LM backend can be found in :doc:`Megatron-LM Workers<../workers/megatron_workers>`.\n\n\n2. Inference:\n\nFor inference, vllm 0.8.3 and later versions have been tested for stability. We recommend turning on env var `VLLM_USE_V1=1` for optimal performance.\n\nFor SGLang, refer to the :doc:`SGLang Backend<../workers/sglang_worker>` for detailed installation and usage instructions. SGLang rollout is under extensive development and offers many advanced features and optimizations. We encourage users to report any issues or provide feedback via the `SGLang Issue Tracker <\n\nFor huggingface TGI integration, it is usually used for debugging and single GPU exploration.\n\nInstall from docker image\n-------------------------\n\nStart from v0.6.0, we use vllm and sglang release image as our base image.\n\nBase Image\n::::::::::\n\n- vLLM: \n- SGLang: \n\nApplication Image\n:::::::::::::::::\n\nUpon base image, the following packages are added:\n\n- flash_attn\n- Megatron-LM\n- Apex\n- TransformerEngine\n- DeepEP\n\nLatest docker file:\n\n- `Dockerfile.stable.vllm <\n- `Dockerfile.stable.sglang <\n\nAll pre-built images are available in dockerhub: `verlai/verl < For example, ``verlai/verl:sgl055.latest``, ``verlai/verl:vllm011.latest``.\n\nYou can find the latest images used for development and ci in our github workflows:\n\n- `.github/workflows/vllm.yml <\n- `.github/workflows/sgl.yml <\n\n\nInstallation from Docker\n::::::::::::::::::::::::\n\nAfter pulling the desired Docker image and installing desired inference and training frameworks, you can run it with the following steps:\n\n1. Launch the desired Docker image and attach into it:\n\n.. code:: bash\n\n    docker create --runtime=nvidia --gpus all --net=host --shm-size=\"10g\" --cap-add=SYS_ADMIN -v .:/workspace/verl --name verl <image:tag> sleep infinity\n    docker start verl\n    docker exec -it verl bash\n```\n\nSource: docs/start/multinode.rst:1-80\n```text\nMultinode Training\n==================\n\nLast updated: 06/10/2025.\n\n.. _wuxibin89: \n\nAuthor: `Xibin Wu < `Yusheng Su <\n\nOption 1: Launch Manually\n------------------------------\n\nSet up multinode ray cluster\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n1. Start head node with ``ray start --head --dashboard-host=0.0.0.0``, there're 2 address you should care about:\n\n- GCS address: ``ray start --address=<address>``, where worker node should connect to.\n- Dashboard address: ``<address>:8265``, where you should submit job to the cluster.\n\n.. image:: \n\n2. Start worker node with ``ray start --address=<address>`` you get above.\n\n.. image:: \n\n3. Now you should see the cluster have 2 nodes with ``ray status``.\n\n.. image:: \n\n4. Additionally, you can access dashboard in the browser with the address you get above. \n\n*Firewall rules maybe need configure to access the dashboard, if there's any trouble, please contact your network administrator.*\n\n.. image:: \n\nSubmit job to ray cluster\n~~~~~~~~~~~~~~~~~~~~~~~~~\n1. Submit ray job to cluster with the dashboard address you get above.\n\n.. code-block:: bash\n\n    ray job submit --address=\" \\\n        --runtime-env=verl/trainer/runtime_env.yaml \\\n        --no-wait \\\n        -- \\\n        python3 -m verl.trainer.main_ppo \\\n        trainer.n_gpus_per_node=8 \\\n        trainer.nnodes=2 \\\n        ...\n\n.. image:: \n\n2. Then you can check the job status with the following commands:\n\n- ray job list: list all jobs submitted to the cluster.\n- ray job logs <Submission ID>: query the logs of the job.\n- ray job status <Submission ID>: query the status of the job.\n- ray job stop <Submission ID>: request the job to be stopped.\n- ray job list | grep submission_id | grep JobStatus | grep RUNNING | grep -oP 'raysubmit_^'\\''\"+' | head -n 1: get the latest job submission ID of the running job.\n- ray job logs <Submission ID> --follow: added ``--follow`` parameter to ray job logs command to enable continuous log streaming.\n\n3. You can also access driver/task/actor logs in ``/tmp/ray/session_latest/logs/``, driver log is ``job-driver-raysubmit_<Submission ID>.log``.\n\n4. We strongly recommend you to view job detail from dashboard in multinode training, because it provide more structure way to view the job information.\n\n.. image:: \n.. image:: \n\nOption 2: Launch via SkyPilot on Kubernetes or clouds\n------------------------------------------------------\n\n.. note::\n   Ready-to-use SkyPilot example configurations are available in the `examples/skypilot/ < directory:\n   \n   - ``verl-ppo.yaml`` - PPO training with GSM8K dataset\n   - ``verl-grpo.yaml`` - GRPO training with MATH dataset  \n   - ``verl-multiturn-tools.yaml`` - Multi-turn tool usage training\n   \n   See the `SkyPilot examples README < for detailed usage instructions.\n```\n\nSource: docs/start/quickstart.rst:1-80\n```text\n.. _quickstart:\n\n=========================================================\nQuickstart: PPO training on GSM8K dataset\n=========================================================\n\nPost-train a LLM using GSM8K dataset.\n\nIntroduction\n------------\n\n.. _hf_dataset_gsm8k: \n\nIn this example, we train an LLM to tackle the `GSM8k <hf_dataset_gsm8k>`_ task with function-based rewards. [1]_\n\nPrerequisite:\n\n- the latest version of ``verl`` and its dependencies installed following the installation guide. Using the docker image is recommended.\n\n- a GPU with at least 24 GB HBM\n\n\nDataset Introduction\n--------------------\n\nGSM8k is a math problem dataset. The prompt is an elementary school\nproblem. The LLM model is asked to solve the math problem. Below is an example:\n\nPrompt\n\n   Katy makes coffee using teaspoons of sugar and cups of water in the\n   ratio of 7:13. If she used a total of 120 teaspoons of sugar and cups\n   of water, calculate the number of teaspoonfuls of sugar she used.\n\nSolution\n\n   The total ratio representing the ingredients she used to make the\n   coffee is 7+13 = <<7+13=20>>20 Since the fraction representing the\n   number of teaspoons she used is 7/20, she used 7/20\\ *120 =\n   <<7/20*\\ 120=42>>42 #### 42\n\nStep 1: Prepare the dataset\n----------------------------\n\nWe preprocess the dataset in parquet format so that (1) it contains necessary fields for computing RL rewards and (2) is faster to read.\n\n.. code-block:: bash\n\n   python3 examples/data_preprocess/gsm8k.py --local_save_dir ~/data/gsm8k\n\nStep 2: Download a model for post-training\n-------------------------------------------\n\nIn this example, we start with the ``Qwen2.5-0.5B-Instruct`` model.\n\nIf you want to perform SFT before RL, refer to the :doc:`Complete GSM8K Example<../examples/gsm8k_example>`, the `sft directory < and `SFT Trainer < for further details.\n\n.. code-block:: bash\n\n   python3 -c \"import transformers; transformers.pipeline('text-generation', model='Qwen/Qwen2.5-0.5B-Instruct')\"\n\nStep 3: Perform PPO training with the instruct model\n----------------------------------------------------------------------\n\n**Reward Model/Function**\n\nWe use a pre-defined rule-based reward model. We force the model to produce a final\nanswer following 4 â€œ#â€ as shown in the solution. We extract the final\nanswer from both the solution and model's output using regular\nexpression matching. We assign a reward of 1 to correct\nanswer, 0.0 to incorrect answer and 0 to no answer. \n\nFor more details, please refer to `verl/utils/reward_score/gsm8k.py <\n\n**Training Script**\n\nNow let's run PPO training with the dataset and model above. [2]_\n\n\nSet the ``data.train_files`` ,\\ ``data.val_files``, ``actor_rollout_ref.model.path`` and ``critic.model.path`` based on your dataset and model names or paths.\n```\n\nSource: docs/start/ray_debug_tutorial.rst:1-80\n```text\nRay Debug Tutorial\n==================\n\nLast updated: 04/23/2025\n\n\n.. _wuxibin89: \n\nAuthor: `Ao Shen <\n\nHow to debug?\n---------------------\n\n\nRay Distributed Debugger VSCode Extension (Recommended)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n1. Starting with RayÂ 2.39, Anyscale has introduced the `Ray Distributed Debugger < VSCode extension. Follow the extensionâ€™s installation instructions, then add your cluster using the dashboard URL you obtained earlier.\n\n   .. image:: \n      :alt: Ray Distributed Debugger VSCode extension screenshot\n\n2. Prerequisites.\n\n   Ensure the following are installed (see the extension README for more detail):\n\n   - Visual Studio Code  \n   - `ray[default]`Â >=Â 2.9.1  \n   - `debugpy`Â >=Â 1.8.0  \n\n   .. image:: \n      :alt: VSCode with Ray prerequisites\n\n3. Environment Variables.\n\n   To enable postâ€‘mortem debugging, set:\n\n   .. code-block:: bash\n\n      export RAY_DEBUG_POST_MORTEM=1\n\n   .. admonition:: Note\n      :class: important\n\n      Be sure to remove any legacy flags before starting Ray:\n\n      - `RAY_DEBUG=legacy`  \n      - `--ray-debugger-external`\n\n4. Configuring BreakpointsSet up breakpoint() in your code, and submit job to cluster. Then the extension will show the breakpoint information.\n\n\n   1. Insert `breakpoint()` calls into your remote functions.  \n   2. Submit your job to the cluster.  \n\n   The extension will detect active breakpoints and display them in VSCode.\n\n   **Note:** Breakpoints are only supported inside functions decorated with `@ray.remote`.\n\n5. Launching the Debugger.\n\n   Run your job directly from the command line (do not use a `launch.json`):\n\n   .. code-block:: bash\n\n      python job.py\n\n6. Attaching to a Breakpoint.\n\n Once the process hits the first `breakpoint()`, click the Ray Distributed Debugger icon in the VSCode sidebar to attach the debugger.\n\n   .. image:: \n      :alt: Attaching VSCode debugger to Ray process\n\n7. Debugging With Multiple breakpoint().\n\n   For each subsequent task, first disconnect the current debugger session, then click the extension icon again to attach to the next breakpoint.\n\n   .. image:: \n      :alt: Disconnecting and reconnecting the debugger\n```\n\nSource: README.md:1-27\n```markdown\n<div align=\"center\">\n ðŸ‘‹ Hi, everyone!\n    verl is a RL training library initiated by <b>ByteDance Seed team</b> and maintained by the verl community.\n    <br>\n    <br>\n</div>\n\n<div align=\"center\">\n\n<a href=\" src=\" alt=\"Ask DeepWiki.com\" style=\"height:20px;\"></a>\nGitHub Repo stars\nTwitter\n<a href=\" src=\"\n<a href=\" src=\"\nDocumentation\n<a href=\" src=\"\n\n</div>\n\nseed logo\n\n<h1 style=\"text-align: center;\">verl: Volcano Engine Reinforcement Learning for LLMs</h1>\n\nverl is a flexible, efficient and production-ready RL training library for large language models (LLMs).\n\nverl is the open-source version of **HybridFlow: A Flexible and Efficient RLHF Framework** paper.\n```\n\nSource: docs/index.rst:1-22\n```text\nWelcome to verl's documentation!\n================================================\n\nverl is a flexible, efficient and production-ready RL training framework designed for large language models (LLMs) post-training. It is an open source implementation of the `HybridFlow < paper.\n\nverl is flexible and easy to use with:\n\n- **Easy extension of diverse RL algorithms**: The hybrid programming model combines the strengths of single-controller and multi-controller paradigms to enable flexible representation and efficient execution of complex Post-Training dataflows. Allowing users to build RL dataflows in a few lines of code.\n\n- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as PyTorch FSDP, Megatron-LM, vLLM and SGLang. Moreover, users can easily extend to other LLM training and inference frameworks.\n\n- **Flexible device mapping and parallelism**: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.\n\n- Ready integration with popular HuggingFace models\n\n\nverl is fast with:\n\n- **State-of-the-art throughput**: By seamlessly integrating existing SOTA LLM training and inference frameworks, verl achieves high generation and training throughput.\n\n- **Efficient actor model resharding with 3D-HybridEngine**: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.\n```\n\nSource: README.md:24-44\n```markdown\nverl is a flexible, efficient and production-ready RL training library for large language models (LLMs).\n\nverl is the open-source version of **HybridFlow: A Flexible and Efficient RLHF Framework** paper.\n\nverl is flexible and easy to use with:\n\n- **Easy extension of diverse RL algorithms**: The hybrid-controller programming model enables flexible representation and efficient execution of complex post-training dataflows. Build RL dataflows such as GRPO, PPO in a few lines of code.\n\n- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as FSDP, Megatron-LM, vLLM, SGLang, etc\n\n- **Flexible device mapping**: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.\n\n- Ready integration with popular HuggingFace models\n\nverl is fast with:\n\n- **State-of-the-art throughput**: SOTA LLM training and inference engine integrations and SOTA RL throughput.\n\n- **Efficient actor model resharding with 3D-HybridEngine**: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.\n\n</p>\n```\n\nSource: docs/index.rst:4-22\n```text\nverl is a flexible, efficient and production-ready RL training framework designed for large language models (LLMs) post-training. It is an open source implementation of the `HybridFlow < paper.\n\nverl is flexible and easy to use with:\n\n- **Easy extension of diverse RL algorithms**: The hybrid programming model combines the strengths of single-controller and multi-controller paradigms to enable flexible representation and efficient execution of complex Post-Training dataflows. Allowing users to build RL dataflows in a few lines of code.\n\n- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as PyTorch FSDP, Megatron-LM, vLLM and SGLang. Moreover, users can easily extend to other LLM training and inference frameworks.\n\n- **Flexible device mapping and parallelism**: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.\n\n- Ready integration with popular HuggingFace models\n\n\nverl is fast with:\n\n- **State-of-the-art throughput**: By seamlessly integrating existing SOTA LLM training and inference frameworks, verl achieves high generation and training throughput.\n\n- **Efficient actor model resharding with 3D-HybridEngine**: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.\n```\n\nSource: README.md:28-34\n```markdown\nverl is flexible and easy to use with:\n\n- **Easy extension of diverse RL algorithms**: The hybrid-controller programming model enables flexible representation and efficient execution of complex post-training dataflows. Build RL dataflows such as GRPO, PPO in a few lines of code.\n\n- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as FSDP, Megatron-LM, vLLM, SGLang, etc\n\n- **Flexible device mapping**: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.\n```\n\nSource: docs/index.rst:6-10\n```text\nverl is flexible and easy to use with:\n\n- **Easy extension of diverse RL algorithms**: The hybrid programming model combines the strengths of single-controller and multi-controller paradigms to enable flexible representation and efficient execution of complex Post-Training dataflows. Allowing users to build RL dataflows in a few lines of code.\n\n- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as PyTorch FSDP, Megatron-LM, vLLM and SGLang. Moreover, users can easily extend to other LLM training and inference frameworks.\n```\n\nSource: docs/hybrid_flow.rst:1-80\n```text\n=========================================================\nHybridFlow Programming Guide\n=========================================================\n\nLast updated: 06/02/2025.\n\n.. _vermouth: \n\nAuthor: `Chi Zhang <\n\nverl is an open source implementation of the paper `HybridFlow < [1]_. In this section, we will introduce the basic concepts of HybridFlow, the motivation and how to program with verl APIs.\n\nMotivation and Design\n------------------------\nWe use dataflow to represent RL systems. [4]_.\n\nDataFlow\n~~~~~~~~~~~~~~~~~~~~\n\nDataflow is an abstraction of computations. Neural Network training is a typical dataflow. It can be represented by computational graph. \n\n.. image:: \n   :alt: The dataflow graph from CS231n 2024 lecture 4\n\nThis figure [2]_ represents the computation graph of a polynomial function followed by a sigmoid function. In the data flow of neural network computation, each node represents an operator, and each edge represents the direction of forward/backward propagation. The computation graph determines the architecture of the neural network.\n\nRL as a dataflow problem\n++++++++++++++++++++++++++++++++++++++++++++++\n\nReinforcement learning (RL) training can also be represented as a dataflow. Below is the dataflow graph that represents the PPO algorithm used in RLHF [3]_:\n\n.. image:: \n  :alt: PPO dataflow graph, credit to Zhihu ä½Žçº§ç‚¼ä¸¹å¸ˆ\n\nHowever, the dataflow of RL has fundamental differences compared with dataflow of neural network training as follows:\n\n+--------------------------+--------------------------------------------------+---------------------+\n| Workload                 | Node                                             | Edge                |\n+--------------------------+--------------------------------------------------+---------------------+\n| Neural Network Training  | Operator (+/-/matmul/softmax)                    | Tensor movement     |\n+--------------------------+--------------------------------------------------+---------------------+\n| Reinforcement Learning   | High-level operators (rollout/model forward)     | Data Movement       |\n+--------------------------+--------------------------------------------------+---------------------+\n\nIn the case of tabular reinforcement learning, each operator is a simple scalar math operation (e.g., bellman update). In deep reinforcement learning(DRL), each operator is a high-level neural network computation such as model inference/update. This makes RL a two-level dataflow problem:\n\n- Control flow: defines how the high-level operators are executed (e.g., In PPO, we first perform rollout. Then, we perform advantage computation. Finally, we perform training). It expresses the **core logics of RL algorithms**.\n- Computation flow: defines the dataflow of **neural network computation** (e.g., model forward/backward/optimizer).\n\n\nDesign Choices\n~~~~~~~~~~~~~~~~~~~~\nThe model size used in DRL before the LLM era is typically small. Thus, the high-level neural network computation can be done in a single process. This enables embedding the computation flow inside the control flow as a single process.\n\nHowever, in the LLM era, the computation flow (e.g., training neural network) becomes a multi-process program. This naturally leads to two design choices:\n\n1. Convert the control flow into a multi-process program as well. Then colocate with computation flow (unified multi-controller)\n\n- Advantages:\n\n  - Achieves the **optimal performance** under fixed computation flow and control flow as the communication overhead in both training and data transfer is minimized.\n\n- Disadvantages:\n\n  - The computation and/or control flow is **hard to reuse** from software perspective as computation code is coupled with specific controller code. For example, the training loop of PPO is generic. Say we have an PPO training flow implemented with a specific computation flow such as FSDP. Neither the control flow or computation flow can be reused if we want to switch the computation flow from FSDP to Megatron, due to the coupling of control and computation flows.\n  - Requires more efforts from the user under flexible and dynamic control flows, due to the multi-process nature of the program.\n\n2. Separate the flows: single process for the control flow and multi-process for computation flow\n\n- Advantages:\n\n  - The computation flow defined elsewhere can be **easily reused** after the decoupling.\n  - The controller runs on a single process. Implementing a new RL algorithm with a **different control flow is simple and easy**.\n\n- Disadvantages:\n\n  - Additional **data communication overhead** each time the controller process and computatation processes interact. The data has to be sent back and forth.\n\nIn verl, the latter strategy with separate control flow and computation flow is adopted. verl is designed to decouple the control flow of RL algorithms, and the implementation of computation engines.\n```\n\nSource: verl/trainer/ppo/ray_trainer.py:1-80\n```python\n# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright 2023-2024 SGLang Team\n# Copyright 2025 ModelBest Inc. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     \n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nPPO Trainer with Ray-based single controller.\nThis trainer supports model-agonistic model initialization with huggingface\n\"\"\"\n\nimport json\nimport os\nimport uuid\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom dataclasses import dataclass, field\nfrom pprint import pprint\nfrom typing import Any, Optional\n\nimport numpy as np\nimport ray\nimport torch\nfrom omegaconf import OmegaConf, open_dict\nfrom torch.utils.data import Dataset, Sampler\nfrom torchdata.stateful_dataloader import StatefulDataLoader\nfrom tqdm import tqdm\n\nfrom verl import DataProto\nfrom verl.experimental.dataset.sampler import AbstractCurriculumSampler\nfrom verl.protocol import pad_dataproto_to_divisor, unpad_dataproto\nfrom verl.single_controller.ray import RayClassWithInitArgs, RayResourcePool, RayWorkerGroup\nfrom verl.single_controller.ray.base import create_colocated_worker_cls\nfrom verl.trainer.config import AlgoConfig\nfrom verl.trainer.ppo import core_algos\nfrom verl.trainer.ppo.core_algos import AdvantageEstimator, agg_loss\nfrom verl.trainer.ppo.metric_utils import (\n    compute_data_metrics,\n    compute_throughout_metrics,\n    compute_timing_metrics,\n    process_validation_metrics,\n)\nfrom verl.trainer.ppo.reward import compute_reward, compute_reward_async\nfrom verl.trainer.ppo.utils import Role, WorkerType, need_critic, need_reference_policy, need_reward_model\nfrom verl.utils import tensordict_utils as tu\nfrom verl.utils.checkpoint.checkpoint_manager import find_latest_ckpt_path, should_save_ckpt_esi\nfrom verl.utils.config import omega_conf_to_dataclass\nfrom verl.utils.debug import marked_timer\nfrom verl.utils.import_utils import load_class_from_fqn\nfrom verl.utils.metric import reduce_metrics\nfrom verl.utils.py_functional import rename_dict\nfrom verl.utils.rollout_skip import RolloutSkip\nfrom verl.utils.seqlen_balancing import calculate_workload, get_seqlen_balanced_partitions, log_seqlen_unbalance\nfrom verl.utils.torch_functional import masked_mean\nfrom verl.utils.tracking import ValidationGenerationsLogger\nfrom verl.workers.config import FSDPEngineConfig\nfrom verl.workers.utils.padding import left_right_2_no_padding, no_padding_2_padding\n\n\n@dataclass\nclass ResourcePoolManager:\n    \"\"\"\n    Define a resource pool specification. Resource pool will be initialized first.\n    \"\"\"\n\n    resource_pool_spec: dict[str, list[int]]\n    mapping: dict[Role, str]\n    resource_pool_dict: dict[str, RayResourcePool] = field(default_factory=dict)\n\n    def create_resource_pool(self):\n        \"\"\"Create Ray resource pools for distributed training.\n```\n\nSource: verl/trainer/main_ppo.py:1-80\n```python\n# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     \n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nNote that we don't combine the main with ray_trainer as ray_trainer is used by other mpain.\n\"\"\"\n\nimport os\nimport socket\n\nimport hydra\nimport ray\nfrom omegaconf import OmegaConf\n\nfrom verl.experimental.dataset.sampler import AbstractSampler\nfrom verl.trainer.constants_ppo import get_ppo_ray_runtime_env\nfrom verl.trainer.ppo.ray_trainer import RayPPOTrainer\nfrom verl.trainer.ppo.reward import load_reward_manager\nfrom verl.trainer.ppo.utils import need_critic, need_reference_policy\nfrom verl.utils.config import validate_config\nfrom verl.utils.device import auto_set_ascend_device_name, is_cuda_available\nfrom verl.utils.import_utils import load_extern_object\n\n\n@hydra.main(config_path=\"config\", config_name=\"ppo_trainer\", version_base=None)\ndef main(config):\n    \"\"\"Main entry point for PPO training with Hydra configuration management.\n\n    Args:\n        config_dict: Hydra configuration dictionary containing training parameters.\n    \"\"\"\n    # Automatically set `config.trainer.device = npu` when running on Ascend NPU.\n    auto_set_ascend_device_name(config)\n\n    run_ppo(config)\n\n\n# Define a function to run the PPO-like training process\ndef run_ppo(config, task_runner_class=None) -> None:\n    \"\"\"Initialize Ray cluster and run distributed PPO training process.\n\n    Args:\n        config: Training configuration object containing all necessary parameters\n                for distributed PPO training including Ray initialization settings,\n                model paths, and training hyperparameters.\n        task_runner_class: For recipe to change TaskRunner.\n    \"\"\"\n    # Check if Ray is not initialized\n    if not ray.is_initialized():\n        # Initialize Ray with a local cluster configuration\n        # Set environment variables in the runtime environment to control tokenizer parallelism,\n        # NCCL debug level, VLLM logging level, and allow runtime LoRA updating\n        # `num_cpus` specifies the number of CPU cores Ray can use, obtained from the configuration\n        default_runtime_env = get_ppo_ray_runtime_env()\n        ray_init_kwargs = config.ray_kwargs.get(\"ray_init\", {})\n        runtime_env_kwargs = ray_init_kwargs.get(\"runtime_env\", {})\n\n        if config.transfer_queue.enable:\n            # Add runtime environment variables for transfer queue\n            runtime_env_vars = runtime_env_kwargs.get(\"env_vars\", {})\n            runtime_env_vars[\"TRANSFER_QUEUE_ENABLE\"] = \"1\"\n            runtime_env_kwargs[\"env_vars\"] = runtime_env_vars\n\n        runtime_env = OmegaConf.merge(default_runtime_env, runtime_env_kwargs)\n        ray_init_kwargs = OmegaConf.create({**ray_init_kwargs, \"runtime_env\": runtime_env})\n        print(f\"ray init kwargs: {ray_init_kwargs}\")\n        ray.init(**OmegaConf.to_container(ray_init_kwargs))\n\n    if task_runner_class is None:\n        task_runner_class = ray.remote(num_cpus=1)(TaskRunner)  # please make sure main_task is not scheduled on head\n```\n\nSource: verl/utils/dataset/rl_dataset.py:1-80\n```python\n# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright 2023-2024 SGLang Team\n# Copyright 2025 ModelBest Inc. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     \n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\nimport logging\nimport os\nimport re\nimport traceback\nfrom collections import defaultdict\nfrom typing import Optional\n\nimport datasets\nimport numpy as np\nimport torch\nfrom omegaconf import DictConfig, ListConfig\nfrom torch.utils.data import Dataset\nfrom transformers import PreTrainedTokenizer, ProcessorMixin\n\nimport verl.utils.torch_functional as verl_F\nfrom verl.utils.model import compute_position_id_with_mask\n\nlogger = logging.getLogger(__name__)\n\n\ndef collate_fn(data_list: list[dict]) -> dict:\n    \"\"\"\n    Collate a batch of sample dicts into batched tensors and arrays.\n\n    Args:\n        data_list: List of dicts mapping feature names to torch.Tensor or other values.\n\n    Returns:\n        Dict where tensor entries are stacked into a torch.Tensor of shape\n        (batch_size, \\\\*dims) and non-tensor entries are converted to\n        np.ndarray of dtype object with shape (batch_size,).\n    \"\"\"\n    tensors = defaultdict(list)\n    non_tensors = defaultdict(list)\n\n    for data in data_list:\n        for key, val in data.items():\n            if isinstance(val, torch.Tensor):\n                tensors[key].append(val)\n            else:\n                non_tensors[key].append(val)\n\n    for key, val in tensors.items():\n        tensors[key] = torch.stack(val, dim=0)\n\n    for key, val in non_tensors.items():\n        non_tensors[key] = np.fromiter(val, dtype=object, count=len(val))\n\n    return {**tensors, **non_tensors}\n\n\nclass RLHFDataset(Dataset):\n    \"\"\"\n    Load and preprocess RLHF data from Parquet files.\n\n    - Caches files locally.\n    - Reads into a HuggingFace Dataset and tokenizes prompts.\n    - Optionally handles images/videos via a ProcessorMixin.\n    - Filters prompts over a max length.\n    - Supports resuming from checkpoints.\n\n    Args:\n        data_files (str or list): Path(s) to Parquet file(s).\n```\n\nSource: docs/start/install.rst:10-32\n```text\nverl supports various backends. Currently, the following configurations are available:\n\n- **FSDP** and **Megatron-LM** (optional) for training.\n- **SGLang**, **vLLM** and **TGI** for rollout generation.\n\nChoices of Backend Engines\n----------------------------\n\n1. Training:\n\nWe recommend using **FSDP** backend to investigate, research and prototype different models, datasets and RL algorithms. The guide for using FSDP backend can be found in :doc:`FSDP Workers<../workers/fsdp_workers>`.\n\nFor users who pursue better scalability, we recommend using **Megatron-LM** backend. Currently, we support `Megatron-LM v0.13.1 < The guide for using Megatron-LM backend can be found in :doc:`Megatron-LM Workers<../workers/megatron_workers>`.\n\n\n2. Inference:\n\nFor inference, vllm 0.8.3 and later versions have been tested for stability. We recommend turning on env var `VLLM_USE_V1=1` for optimal performance.\n\nFor SGLang, refer to the :doc:`SGLang Backend<../workers/sglang_worker>` for detailed installation and usage instructions. SGLang rollout is under extensive development and offers many advanced features and optimizations. We encourage users to report any issues or provide feedback via the `SGLang Issue Tracker <\n\nFor huggingface TGI integration, it is usually used for debugging and single GPU exploration.\n```\n\nSource: verl/trainer/ppo/ray_trainer.py:200-500\n```python\n    Args:\n        data (DataProto): The data containing batched model outputs and inputs.\n        adv_estimator (AdvantageEstimator): The advantage estimator to use (e.g., GAE, GRPO, REINFORCE++).\n        gamma (float, optional): Discount factor for future rewards. Defaults to 1.0.\n        lam (float, optional): Lambda parameter for GAE. Defaults to 1.0.\n        num_repeat (int, optional): Number of times to repeat the computation. Defaults to 1.\n        norm_adv_by_std_in_grpo (bool, optional): Whether to normalize advantages by standard deviation in\n            GRPO. Defaults to True.\n        config (dict, optional): Configuration dictionary for algorithm settings. Defaults to None.\n\n    Returns:\n        DataProto: The updated data with computed advantages and returns.\n    \"\"\"\n    # Back-compatible with trainers that do not compute response mask in fit\n    if \"response_mask\" not in data.batch.keys():\n        data.batch[\"response_mask\"] = compute_response_mask(data)\n    # prepare response group\n    if adv_estimator == AdvantageEstimator.GAE:\n        # Compute advantages and returns using Generalized Advantage Estimation (GAE)\n        advantages, returns = core_algos.compute_gae_advantage_return(\n            token_level_rewards=data.batch[\"token_level_rewards\"],\n            values=data.batch[\"values\"],\n            response_mask=data.batch[\"response_mask\"],\n            gamma=gamma,\n            lam=lam,\n        )\n        data.batch[\"advantages\"] = advantages\n        data.batch[\"returns\"] = returns\n        if config.get(\"use_pf_ppo\", False):\n            data = core_algos.compute_pf_ppo_reweight_data(\n                data,\n                config.pf_ppo.get(\"reweight_method\"),\n                config.pf_ppo.get(\"weight_pow\"),\n            )\n    elif adv_estimator == AdvantageEstimator.GRPO:\n        # Initialize the mask for GRPO calculation\n        grpo_calculation_mask = data.batch[\"response_mask\"]\n\n        # Call compute_grpo_outcome_advantage with parameters matching its definition\n        advantages, returns = core_algos.compute_grpo_outcome_advantage(\n            token_level_rewards=data.batch[\"token_level_rewards\"],\n            response_mask=grpo_calculation_mask,\n            index=data.non_tensor_batch[\"uid\"],\n            norm_adv_by_std_in_grpo=norm_adv_by_std_in_grpo,\n        )\n        data.batch[\"advantages\"] = advantages\n        data.batch[\"returns\"] = returns\n    else:\n        # handle all other adv estimator type other than GAE and GRPO\n        adv_estimator_fn = core_algos.get_adv_estimator_fn(adv_estimator)\n        adv_kwargs = {\n            \"token_level_rewards\": data.batch[\"token_level_rewards\"],\n            \"response_mask\": data.batch[\"response_mask\"],\n            \"config\": config,\n        }\n        if \"uid\" in data.non_tensor_batch:  # optional\n            adv_kwargs[\"index\"] = data.non_tensor_batch[\"uid\"]\n        if \"reward_baselines\" in data.batch:  # optional\n            adv_kwargs[\"reward_baselines\"] = data.batch[\"reward_baselines\"]\n\n        # calculate advantage estimator\n        advantages, returns = adv_estimator_fn(**adv_kwargs)\n        data.batch[\"advantages\"] = advantages\n        data.batch[\"returns\"] = returns\n    return data\n\n\nclass RayPPOTrainer:\n    \"\"\"Distributed PPO trainer using Ray for scalable reinforcement learning.\n\n    This trainer orchestrates distributed PPO training across multiple nodes and GPUs,\n    managing actor rollouts, critic training, and reward computation with Ray backend.\n    Supports various model architectures including FSDP, Megatron, vLLM, and SGLang integration.\n    \"\"\"\n\n    # TODO: support each role have individual ray_worker_group_cls,\n    # i.e., support different backend of different role\n    def __init__(\n        self,\n        config,\n```\n\nSource: docs/examples/config.rst:1-80\n```text\n.. _config-explain-page:\n\nConfig Explanation\n===================\n\nLast updated: 06/18/2025.\n\nppo_trainer.yaml for RL FSDP Backend\n-------------------------------------\n\nData\n~~~~\n\n.. code:: yaml\n\n   data:\n     tokenizer: null\n     train_files: ~/data/rlhf/gsm8k/train.parquet\n     val_files: ~/data/rlhf/gsm8k/test.parquet\n     train_max_samples: -1  # set to -1 to use full dataset\n     val_max_samples: -1  # set to -1 to use full dataset\n     prompt_key: prompt\n     max_prompt_length: 512\n     max_response_length: 512\n     train_batch_size: 1024\n     return_raw_input_ids: False  # This should be set to true when the tokenizer between policy and rm differs\n     return_raw_chat: False\n     return_full_prompt: False\n     shuffle: True\n     seed: 42\n     filter_overlong_prompts: False\n     filter_overlong_prompts_workers: 1\n     truncation: error\n     image_key: images\n     trust_remote_code: True\n     custom_cls:\n        path: null\n        name: null\n\n- ``data.train_files``: Training set parquet. Can be a list or a single\n  file. The program will read all files into memory, so it can't be too\n  large (< 100GB). The path can be either local path or HDFS path. For\n  HDFS path, we provide utils to download it to DRAM and convert the\n  HDFS path to local path.\n- ``data.val_files``: Validation parquet. Can be a list or a single\n  file.\n- ``data.train_max_samples``: Maximum number of samples to use from the\n  training dataset. Set to -1 to use the full dataset.\n- ``data.val_max_samples``: Maximum number of samples to use from the\n  validation dataset. Set to -1 to use the full dataset.\n- ``data.prompt_key``: The field in the dataset where the prompt is\n  located. Default is 'prompt'.\n- ``data.max_prompt_length``: Maximum prompt length. All prompts will be\n  left-padded to this length. An error will be reported if the length is\n  too long\n- ``data.max_response_length``: Maximum response length. Rollout in RL\n  algorithms (e.g. PPO) generates up to this length\n- ``data.train_batch_size``: Batch size sampled for one training\n  iteration of different RL algorithms.\n- ``data.return_raw_input_ids``: Whether to return the original\n  input_ids without adding chat template. This is mainly used to\n  accommodate situations where the reward model's chat template differs\n  from the policy. It needs to be decoded first, then apply the RM's\n  chat template. If using a model-based RM, and the policy and RM\n  chat_templates are different, this flag needs to be set\n- ``data.return_raw_chat``: Whether to return the original chat (prompt)\n  without applying chat template.\n- ``data.return_full_prompt``: Whether to return the full prompt with chat template\n- ``data.shuffle``: Whether to shuffle the data in the dataloader.\n- ``data.seed``: An integer seed to use when shuffling the data. If not set or set to\n  `null`, the data shuffling will not be seeded, resulting in a different data order on each run.\n- ``data.filter_overlong_prompts``: Default don't filter.\n- ``data.filter_overlong_prompts_workers``: For large-scale dataset, filtering\n  overlong prompts could be timeconsuming. You cat set the ``filter_overlong_prompts_workers``\n  to use multiprocessing for speed up. Default to 1.\n- ``data.truncation``: Truncate the input_ids or prompt length if they\n  exceed max_prompt_length. Default is 'error', not allow exceed the\n  max_prompt_length. The users should increase the max_prompt_length if\n  throwing the error. You can also set ``left``, ``right`` and ``middle``. \n  When ``middle`` is selected, the logic splits the allowed max length roughly in half\n```\n\nSource: docs/start/quickstart.rst:80-110\n```text\nSet the ``data.train_files`` ,\\ ``data.val_files``, ``actor_rollout_ref.model.path`` and ``critic.model.path`` based on your dataset and model names or paths.\nYou may set ``VERL_USE_MODELSCOPE=True`` to download models from `modelscope < instead of `huggingface <\n\n.. code-block:: bash\n\n   PYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \\\n    data.train_files=$HOME/data/gsm8k/train.parquet \\\n    data.val_files=$HOME/data/gsm8k/test.parquet \\\n    data.train_batch_size=256 \\\n    data.max_prompt_length=512 \\\n    data.max_response_length=512 \\\n    actor_rollout_ref.model.path=Qwen/Qwen2.5-0.5B-Instruct \\\n    actor_rollout_ref.actor.optim.lr=1e-6 \\\n    actor_rollout_ref.actor.ppo_mini_batch_size=64 \\\n    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \\\n    actor_rollout_ref.rollout.name=vllm \\\n    actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 \\\n    actor_rollout_ref.rollout.tensor_model_parallel_size=1 \\\n    actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \\\n    actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \\\n    critic.optim.lr=1e-5 \\\n    critic.model.path=Qwen/Qwen2.5-0.5B-Instruct \\\n    critic.ppo_micro_batch_size_per_gpu=4 \\\n    algorithm.kl_ctrl.kl_coef=0.001 \\\n    trainer.logger=console \\\n    trainer.val_before_train=False \\\n    trainer.n_gpus_per_node=1 \\\n    trainer.nnodes=1 \\\n    trainer.save_freq=10 \\\n    trainer.test_freq=10 \\\n    trainer.total_epochs=15 2>&1 | tee verl_demo.log\n```\n\nSource: docs/index.rst:25-37\n```text\n.. _Contents:\n\n.. toctree::\n   :maxdepth: 2\n   :caption: Quickstart\n\n   start/install\n   start/quickstart\n   start/multinode\n   start/ray_debug_tutorial\n   start/more_resources\n   start/agentic_rl\n```\n\nSource: README.md:102-157\n```markdown\n## Getting Started\n\n<a href=\"\n\n**Quickstart:**\n\n- Installation\n- Quickstart\n- Programming Guide & Tech Talk (in Chinese)\n- PPO in verl\n- GRPO in verl\n\n**Running a PPO example step-by-step:**\n\n- Prepare Data for Post-Training\n- Implement Reward Function for Dataset\n- PPO Example Architecture\n- Config Explanation\n\n**Reproducible algorithm baselines:**\n\n- RL performance on coding, math\n\n**For code explanation and advance usage (extension):**\n\n- PPO Trainer and Workers\n  - PPO Ray Trainer\n  - PyTorch FSDP Backend\n  - Megatron-LM Backend\n\n- Advanced Usage and Extension\n  - Add Models with the FSDP Backend\n  - Add Models with the Megatron-LM Backend\n  - Multi-turn Rollout Support\n  - Search Tool Integration\n  - Sandbox Fusion Integration\n  - Deployment using Separate GPU Resources\n  - Extend to Other RL(HF) algorithms\n  - Ray API design tutorial\n\n**Blogs from the community**\n\n- When Reasoning Models Break Tokenization: The Hidden Complexity of Multiturn Training\n- verl deployment on AWS SageMaker\n- verl x SGLang Multi-turn Code Walkthrough\n- Optimizing SGLang Memory Usage in verl\n- SGLang, verl, OpenBMB and Tsinghua University: Pioneering End-to-End Multi-Turn RLHF\n- Reinforcement Learning from Human Feedback on AMD GPUs with verl and ROCm Integration\n- veMLP x verl ï¼šçŽ©è½¬å¼ºåŒ–å­¦ä¹ è®­ç»ƒ\n- ä½¿ç”¨ verl è¿›è¡Œ GRPO åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœ€ä½³å®žè·µ\n- HybridFlow verl åŽŸæ–‡æµ…æž\n- æœ€é«˜æå‡ 20 å€åžåé‡ï¼è±†åŒ…å¤§æ¨¡åž‹å›¢é˜Ÿå‘å¸ƒå…¨æ–° RLHF æ¡†æž¶ï¼ŒçŽ°å·²å¼€æºï¼\n\n## Performance Tuning Guide\n\nThe performance is essential for on-policy RL algorithm. We have written a detailed performance tuning guide to help you optimize performance.\n```\n\nOriginal context:\n<details>\n<summary>Relevant source files</summary>\n\nDesign Summary:\n- README.md:1-80 â€” ðŸ‘‹ Hi, everyone! verl is a RL training library initiated by ByteDance Seed team and maintained by the verl community. GitHub Repo stars\n- docs/examples/gsm8k_example.rst:1-80 â€” GSM8K Example ============= Last updated: 03/25/2025.\n- docs/index.rst:1-80 â€” Welcome to verl's documentation! ================================================ verl is a flexible, efficient and production-ready RL training framework designed for large lan...\n- docs/start/install.rst:1-80 â€” Installation ============ Requirements\n- docs/start/multinode.rst:1-80 â€” Multinode Training ================== Last updated: 06/10/2025.\n- docs/start/quickstart.rst:1-80 â€” .. _quickstart: ========================================================= Quickstart: PPO training on GSM8K dataset\n- docs/start/ray_debug_tutorial.rst:1-80 â€” Ray Debug Tutorial ================== Last updated: 04/23/2025\n- README.md:1-27 â€” ðŸ‘‹ Hi, everyone! verl is a RL training library initiated by ByteDance Seed team and maintained by the verl community. GitHub Repo stars\n- docs/index.rst:1-22 â€” Welcome to verl's documentation! ================================================ verl is a flexible, efficient and production-ready RL training framework designed for large lan...\n- README.md:24-44 â€” verl is a flexible, efficient and production-ready RL training library for large language models (LLMs). verl is the open-source version of HybridFlow: A Flexible and Efficient...\n- docs/index.rst:4-22 â€” verl is a flexible, efficient and production-ready RL training framework designed for large language models (LLMs) post-training. It is an open source implementation of the Hybr...\n- README.md:28-34 â€” verl is flexible and easy to use with: Easy extension of diverse RL algorithms: The hybrid-controller programming model enables flexible representation and efficient execution o...\n- docs/index.rst:6-10 â€” verl is flexible and easy to use with: Easy extension of diverse RL algorithms: The hybrid programming model combines the strengths of single-controller and multi-controller par...\n- docs/hybrid_flow.rst:1-80 â€” ========================================================= HybridFlow Programming Guide =========================================================\n- verl/trainer/ppo/ray_trainer.py:1-80 â€” Copyright 2024 Bytedance Ltd. and/or its affiliates Copyright 2023-2024 SGLang Team Copyright 2025 ModelBest Inc. and/or its affiliates\n- verl/workers/fsdp_workers/actor_rollout_ref.py:1-80 â€” Referenced in section narrative below.\n- verl/trainer/main_ppo.py:1-80 â€” Copyright 2024 Bytedance Ltd. and/or its affiliates Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\n- verl/workers/fsdp_workers/:1-80 â€” Referenced in section narrative below.\n- verl/model_engine/:1-80 â€” Referenced in section narrative below.\n- verl/utils/trainer/core_algos.py:1-80 â€” Referenced in section narrative below.\n- verl/utils/dataset/rl_dataset.py:1-80 â€” Copyright 2024 Bytedance Ltd. and/or its affiliates Copyright 2023-2024 SGLang Team Copyright 2025 ModelBest Inc. and/or its affiliates\n- verl/utils/reward_score/:1-80 â€” Referenced in section narrative below.\n- verl/workers/:1-80 â€” Referenced in section narrative below.\n- verl/trainer/fsdp/:1-80 â€” Referenced in section narrative below.\n- verl/trainer/megatron/:1-80 â€” Referenced in section narrative below.\n- verl/workers/rollout/vllm/:1-80 â€” Referenced in section narrative below.\n- verl/workers/rollout/sglang/:1-80 â€” Referenced in section narrative below.\n- docs/start/install.rst:10-32 â€” verl supports various backends. Currently, the following configurations are available: FSDP and Megatron-LM (optional) for training. SGLang, vLLM and TGI for rollout generation.\n- verl/workers/rollout/:1-80 â€” Referenced in section narrative below.\n- verl/trainer/ppo/ray_trainer.py:200-500 â€” Args: data (DataProto): The data containing batched model outputs and inputs. adv_estimator (AdvantageEstimator): The advantage estimator to use (e.g., GAE, GRPO, REINFORCE++).\n- verl/utils/resource_pool_manager.py:1-80 â€” Referenced in section narrative below.\n- verl/trainer/fsdp/hybrid_engine.py:1-80 â€” Referenced in section narrative below.\n- docs/ascend_tutorial/:1-80 â€” Referenced in section narrative below.\n- verl/trainer/ppo/ppo_trainer.yaml:1-80 â€” Referenced in section narrative below.\n- docs/examples/config.rst:1-80 â€” .. _config-explain-page: Config Explanation ===================\n- docs/start/quickstart.rst:80-110 â€” Set the data.train_files ,\\ data.val_files, actor_rollout_ref.model.path and critic.model.path based on your dataset and model names or paths. You may set VERL_USE_MODELSCOPE=Tr...\n- docs/index.rst:25-37 â€” .. _Contents: .. toctree:: :maxdepth: 2\n- README.md:102-157 â€” Getting Started <a href=\" Quickstart:\n\n</details>\n\nThis document provides a high-level introduction to the verl (Volcano Engine Reinforcement Learning) framework, explaining its architecture, core components, and design principles. This overview covers:\n\n- The purpose of verl as an RLHF training system for large language models\n- The HybridFlow programming model and Ray-based orchestration\n- Main system components and their interactions\n- Supported training and inference backends\n- Basic training workflows\n\nFor detailed information about specific subsystems, see:\n- System architecture and HybridFlow design patterns: System Architecture and HybridFlow Design\n- Algorithm implementations: Supported Algorithms and Models\n- Installation and setup: Getting Started\n- Configuration details: Configuration System\n\nSources: Source: README.md:1-27\n```markdown\n<div align=\"center\">\n ðŸ‘‹ Hi, everyone!\n    verl is a RL training library initiated by <b>ByteDance Seed team</b> and maintained by the verl community.\n    <br>\n    <br>\n</div>\n\n<div align=\"center\">\n\n<a href=\" src=\" alt=\"Ask DeepWiki.com\" style=\"height:20px;\"></a>\nGitHub Repo stars\nTwitter\n<a href=\" src=\"\n<a href=\" src=\"\nDocumentation\n<a href=\" src=\"\n\n</div>\n\nseed logo\n\n<h1 style=\"text-align: center;\">verl: Volcano Engine Reinforcement Learning for LLMs</h1>\n\nverl is a flexible, efficient and production-ready RL training library for large language models (LLMs).\n\nverl is the open-source version of **HybridFlow: A Flexible and Efficient RLHF Framework** paper.\n```, Source: docs/index.rst:1-22\n```text\nWelcome to verl's documentation!\n================================================\n\nverl is a flexible, efficient and production-ready RL training framework designed for large language models (LLMs) post-training. It is an open source implementation of the `HybridFlow < paper.\n\nverl is flexible and easy to use with:\n\n- **Easy extension of diverse RL algorithms**: The hybrid programming model combines the strengths of single-controller and multi-controller paradigms to enable flexible representation and efficient execution of complex Post-Training dataflows. Allowing users to build RL dataflows in a few lines of code.\n\n- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as PyTorch FSDP, Megatron-LM, vLLM and SGLang. Moreover, users can easily extend to other LLM training and inference frameworks.\n\n- **Flexible device mapping and parallelism**: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.\n\n- Ready integration with popular HuggingFace models\n\n\nverl is fast with:\n\n- **State-of-the-art throughput**: By seamlessly integrating existing SOTA LLM training and inference frameworks, verl achieves high generation and training throughput.\n\n- **Efficient actor model resharding with 3D-HybridEngine**: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.\n```\n\n---\n\nverl is a flexible, efficient, and production-ready reinforcement learning training framework for post-training large language models. It is the open-source implementation of the HybridFlow paper presented at EuroSys 2025.\n\n**Core capabilities:**\n- Implements multiple RL algorithms including PPO, GRPO, DAPO, REINFORCE++, RLOO, and PRIME\n- Supports models ranging from small (0.5B parameters) to very large (671B+ parameters)\n- Compatible with HuggingFace models: Qwen, Llama, Gemma, DeepSeek, and others\n- Provides both rule-based (function) and model-based reward computation\n- Enables multi-turn interactions and tool calling for agentic RL\n- Supports vision-language models (VLMs) for multi-modal RL\n\n**Design philosophy:**\n- **Flexibility**: Easy extension of RL algorithms through a hybrid programming model\n- **Modularity**: Seamless integration with existing LLM frameworks via decoupled APIs\n- **Efficiency**: State-of-the-art throughput through optimized backend integrations\n- **Scalability**: Flexible device mapping and resource utilization across cluster sizes\n\nSources: Source: README.md:24-44\n```markdown\nverl is a flexible, efficient and production-ready RL training library for large language models (LLMs).\n\nverl is the open-source version of **HybridFlow: A Flexible and Efficient RLHF Framework** paper.\n\nverl is flexible and easy to use with:\n\n- **Easy extension of diverse RL algorithms**: The hybrid-controller programming model enables flexible representation and efficient execution of complex post-training dataflows. Build RL dataflows such as GRPO, PPO in a few lines of code.\n\n- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as FSDP, Megatron-LM, vLLM, SGLang, etc\n\n- **Flexible device mapping**: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.\n\n- Ready integration with popular HuggingFace models\n\nverl is fast with:\n\n- **State-of-the-art throughput**: SOTA LLM training and inference engine integrations and SOTA RL throughput.\n\n- **Efficient actor model resharding with 3D-HybridEngine**: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.\n\n</p>\n```, Source: docs/index.rst:4-22\n```text\nverl is a flexible, efficient and production-ready RL training framework designed for large language models (LLMs) post-training. It is an open source implementation of the `HybridFlow < paper.\n\nverl is flexible and easy to use with:\n\n- **Easy extension of diverse RL algorithms**: The hybrid programming model combines the strengths of single-controller and multi-controller paradigms to enable flexible representation and efficient execution of complex Post-Training dataflows. Allowing users to build RL dataflows in a few lines of code.\n\n- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as PyTorch FSDP, Megatron-LM, vLLM and SGLang. Moreover, users can easily extend to other LLM training and inference frameworks.\n\n- **Flexible device mapping and parallelism**: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.\n\n- Ready integration with popular HuggingFace models\n\n\nverl is fast with:\n\n- **State-of-the-art throughput**: By seamlessly integrating existing SOTA LLM training and inference frameworks, verl achieves high generation and training throughput.\n\n- **Efficient actor model resharding with 3D-HybridEngine**: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.\n```\n\n---\n\nverl implements a **hybrid-controller programming model** that combines single-controller and multi-controller paradigms. This architecture enables flexible representation of complex post-training dataflows while maintaining efficient execution.\n\n**Key characteristics:**\n\n| Aspect | Description |\n|--------|-------------|\n| **Orchestration** | Ray-based distributed execution with centralized control |\n| **Modularity** | Clean separation between training, inference, and data processing |\n| **Backend Abstraction** | Unified interfaces for different training and inference engines |\n| **Resource Management** | Dynamic GPU allocation and memory-efficient mode switching |\n\nThe system uses Ray as the distributed runtime, with a central `RayPPOTrainer` orchestrating distributed workers that execute training, rollout generation, and reward computation in parallel.\n\nFor detailed architectural patterns and the HybridFlow programming model, see System Architecture and HybridFlow Design.\n\nSources: Source: README.md:28-34\n```markdown\nverl is flexible and easy to use with:\n\n- **Easy extension of diverse RL algorithms**: The hybrid-controller programming model enables flexible representation and efficient execution of complex post-training dataflows. Build RL dataflows such as GRPO, PPO in a few lines of code.\n\n- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as FSDP, Megatron-LM, vLLM, SGLang, etc\n\n- **Flexible device mapping**: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.\n```, Source: docs/index.rst:6-10\n```text\nverl is flexible and easy to use with:\n\n- **Easy extension of diverse RL algorithms**: The hybrid programming model combines the strengths of single-controller and multi-controller paradigms to enable flexible representation and efficient execution of complex Post-Training dataflows. Allowing users to build RL dataflows in a few lines of code.\n\n- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as PyTorch FSDP, Megatron-LM, vLLM and SGLang. Moreover, users can easily extend to other LLM training and inference frameworks.\n```, Source: docs/hybrid_flow.rst:1-80\n```text\n=========================================================\nHybridFlow Programming Guide\n=========================================================\n\nLast updated: 06/02/2025.\n\n.. _vermouth: \n\nAuthor: `Chi Zhang <\n\nverl is an open source implementation of the paper `HybridFlow < [1]_. In this section, we will introduce the basic concepts of HybridFlow, the motivation and how to program with verl APIs.\n\nMotivation and Design\n------------------------\nWe use dataflow to represent RL systems. [4]_.\n\nDataFlow\n~~~~~~~~~~~~~~~~~~~~\n\nDataflow is an abstraction of computations. Neural Network training is a typical dataflow. It can be represented by computational graph. \n\n.. image:: \n   :alt: The dataflow graph from CS231n 2024 lecture 4\n\nThis figure [2]_ represents the computation graph of a polynomial function followed by a sigmoid function. In the data flow of neural network computation, each node represents an operator, and each edge represents the direction of forward/backward propagation. The computation graph determines the architecture of the neural network.\n\nRL as a dataflow problem\n++++++++++++++++++++++++++++++++++++++++++++++\n\nReinforcement learning (RL) training can also be represented as a dataflow. Below is the dataflow graph that represents the PPO algorithm used in RLHF [3]_:\n\n.. image:: \n  :alt: PPO dataflow graph, credit to Zhihu ä½Žçº§ç‚¼ä¸¹å¸ˆ\n\nHowever, the dataflow of RL has fundamental differences compared with dataflow of neural network training as follows:\n\n+--------------------------+--------------------------------------------------+---------------------+\n| Workload                 | Node                                             | Edge                |\n+--------------------------+--------------------------------------------------+---------------------+\n| Neural Network Training  | Operator (+/-/matmul/softmax)                    | Tensor movement     |\n+--------------------------+--------------------------------------------------+---------------------+\n| Reinforcement Learning   | High-level operators (rollout/model forward)     | Data Movement       |\n+--------------------------+--------------------------------------------------+---------------------+\n\nIn the case of tabular reinforcement learning, each operator is a simple scalar math operation (e.g., bellman update). In deep reinforcement learning(DRL), each operator is a high-level neural network computation such as model inference/update. This makes RL a two-level dataflow problem:\n\n- Control flow: defines how the high-level operators are executed (e.g., In PPO, we first perform rollout. Then, we perform advantage computation. Finally, we perform training). It expresses the **core logics of RL algorithms**.\n- Computation flow: defines the dataflow of **neural network computation** (e.g., model forward/backward/optimizer).\n\n\nDesign Choices\n~~~~~~~~~~~~~~~~~~~~\nThe model size used in DRL before the LLM era is typically small. Thus, the high-level neural network computation can be done in a single process. This enables embedding the computation flow inside the control flow as a single process.\n\nHowever, in the LLM era, the computation flow (e.g., training neural network) becomes a multi-process program. This naturally leads to two design choices:\n\n1. Convert the control flow into a multi-process program as well. Then colocate with computation flow (unified multi-controller)\n\n- Advantages:\n\n  - Achieves the **optimal performance** under fixed computation flow and control flow as the communication overhead in both training and data transfer is minimized.\n\n- Disadvantages:\n\n  - The computation and/or control flow is **hard to reuse** from software perspective as computation code is coupled with specific controller code. For example, the training loop of PPO is generic. Say we have an PPO training flow implemented with a specific computation flow such as FSDP. Neither the control flow or computation flow can be reused if we want to switch the computation flow from FSDP to Megatron, due to the coupling of control and computation flows.\n  - Requires more efforts from the user under flexible and dynamic control flows, due to the multi-process nature of the program.\n\n2. Separate the flows: single process for the control flow and multi-process for computation flow\n\n- Advantages:\n\n  - The computation flow defined elsewhere can be **easily reused** after the decoupling.\n  - The controller runs on a single process. Implementing a new RL algorithm with a **different control flow is simple and easy**.\n\n- Disadvantages:\n\n  - Additional **data communication overhead** each time the controller process and computatation processes interact. The data has to be sent back and forth.\n\nIn verl, the latter strategy with separate control flow and computation flow is adopted. verl is designed to decouple the control flow of RL algorithms, and the implementation of computation engines.\n```\n\n---\n\n```mermaid\ngraph TB\n    subgraph \"Entry Points\"\n        MainPPO\"main_ppo.py<br/>CLI Entry\"\n        Config\"Hydra Config<br/>ppo_trainer.yaml\"\n    end\n    \n    subgraph \"Orchestration Layer\"\n        TaskRunner\"TaskRunner<br/>Ray Cluster Setup\"\n        RayTrainer\"RayPPOTrainer<br/>verl.trainer.ppo.ray_trainer\"\n        RPM\"ResourcePoolManager<br/>verl.utils.resource_pool_manager\"\n    end\n    \n    subgraph \"Worker Layer\"\n        FSDPWorker\"ActorRolloutRefWorker<br/>verl.workers.fsdp_workers\"\n        MegatronWorker\"ActorRolloutRefWorker<br/>verl.workers.megatron_workers\"\n        CriticWorker[\"CriticWorker\"]\n        RewardWorker[\"RewardModelWorker\"]\n    end\n    \n    subgraph \"Engine Layer\"\n        FSDPEngine\"FSDPEngine<br/>verl.trainer.fsdp.fsdp_engine\"\n        MegatronEngine\"MegatronEngine<br/>verl.trainer.megatron.megatron_engine\"\n        BaseEngine\"BaseEngine<br/>verl.model_engine.base\"\n    end\n    \n    subgraph \"Rollout/Inference Layer\"\n        AgentLoopMgr\"AgentLoopManager<br/>verl.experimental.agent_loop\"\n        vLLMRollout\"vLLMRollout<br/>verl.workers.rollout.vllm\"\n        SGLangRollout\"SGLangRollout<br/>verl.workers.rollout.sglang\"\n    end\n    \n    subgraph \"Algorithm Layer\"\n        CoreAlgos\"core_algos.py<br/>verl.utils.trainer.core_algos\"\n        AdvEstimator[\"AdvantageEstimator Registry\"]\n        PolicyLoss[\"PolicyLoss Registry\"]\n    end\n    \n    MainPPO --> Config\n    Config --> TaskRunner\n    TaskRunner --> RayTrainer\n    RayTrainer --> RPM\n    \n    RPM --> FSDPWorker\n    RPM --> MegatronWorker\n    RPM --> CriticWorker\n    RPM --> RewardWorker\n    \n    FSDPWorker --> FSDPEngine\n    MegatronWorker --> MegatronEngine\n    FSDPEngine --> BaseEngine\n    MegatronEngine --> BaseEngine\n    \n    RayTrainer --> AgentLoopMgr\n    AgentLoopMgr --> vLLMRollout\n    AgentLoopMgr --> SGLangRollout\n    \n    RayTrainer --> CoreAlgos\n    CoreAlgos --> AdvEstimator\n    CoreAlgos --> PolicyLoss\n```\n\n**Title: Core System Components and Code Mapping**\n\n**Component descriptions:**\n\n- **`main_ppo.py`**: Entry point that initializes Hydra configuration and launches training\n- **`RayPPOTrainer`**: Central orchestrator managing the complete PPO training loop, located in Source: verl/trainer/ppo/ray_trainer.py:1-80\n```python\n# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright 2023-2024 SGLang Team\n# Copyright 2025 ModelBest Inc. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     \n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nPPO Trainer with Ray-based single controller.\nThis trainer supports model-agonistic model initialization with huggingface\n\"\"\"\n\nimport json\nimport os\nimport uuid\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom dataclasses import dataclass, field\nfrom pprint import pprint\nfrom typing import Any, Optional\n\nimport numpy as np\nimport ray\nimport torch\nfrom omegaconf import OmegaConf, open_dict\nfrom torch.utils.data import Dataset, Sampler\nfrom torchdata.stateful_dataloader import StatefulDataLoader\nfrom tqdm import tqdm\n\nfrom verl import DataProto\nfrom verl.experimental.dataset.sampler import AbstractCurriculumSampler\nfrom verl.protocol import pad_dataproto_to_divisor, unpad_dataproto\nfrom verl.single_controller.ray import RayClassWithInitArgs, RayResourcePool, RayWorkerGroup\nfrom verl.single_controller.ray.base import create_colocated_worker_cls\nfrom verl.trainer.config import AlgoConfig\nfrom verl.trainer.ppo import core_algos\nfrom verl.trainer.ppo.core_algos import AdvantageEstimator, agg_loss\nfrom verl.trainer.ppo.metric_utils import (\n    compute_data_metrics,\n    compute_throughout_metrics,\n    compute_timing_metrics,\n    process_validation_metrics,\n)\nfrom verl.trainer.ppo.reward import compute_reward, compute_reward_async\nfrom verl.trainer.ppo.utils import Role, WorkerType, need_critic, need_reference_policy, need_reward_model\nfrom verl.utils import tensordict_utils as tu\nfrom verl.utils.checkpoint.checkpoint_manager import find_latest_ckpt_path, should_save_ckpt_esi\nfrom verl.utils.config import omega_conf_to_dataclass\nfrom verl.utils.debug import marked_timer\nfrom verl.utils.import_utils import load_class_from_fqn\nfrom verl.utils.metric import reduce_metrics\nfrom verl.utils.py_functional import rename_dict\nfrom verl.utils.rollout_skip import RolloutSkip\nfrom verl.utils.seqlen_balancing import calculate_workload, get_seqlen_balanced_partitions, log_seqlen_unbalance\nfrom verl.utils.torch_functional import masked_mean\nfrom verl.utils.tracking import ValidationGenerationsLogger\nfrom verl.workers.config import FSDPEngineConfig\nfrom verl.workers.utils.padding import left_right_2_no_padding, no_padding_2_padding\n\n\n@dataclass\nclass ResourcePoolManager:\n    \"\"\"\n    Define a resource pool specification. Resource pool will be initialized first.\n    \"\"\"\n\n    resource_pool_spec: dict[str, list[int]]\n    mapping: dict[Role, str]\n    resource_pool_dict: dict[str, RayResourcePool] = field(default_factory=dict)\n\n    def create_resource_pool(self):\n        \"\"\"Create Ray resource pools for distributed training.\n```\n- **`ResourcePoolManager`**: Allocates GPU resources to different worker roles (actor, critic, rollout)\n- **`ActorRolloutRefWorker`**: Unified worker interface combining actor training, rollout generation, and reference policy computation, implemented for both FSDP verl/workers/fsdp_workers/actor_rollout_ref.py and Megatron backends\n- **`FSDPEngine`/`MegatronEngine`**: Backend-specific implementations that wrap PyTorch FSDP or Megatron-LM for distributed training\n- **`AgentLoopManager`**: Orchestrates rollout generation with load balancing across vLLM or SGLang servers\n- **`core_algos.py`**: Algorithm registry containing advantage estimators (GAE, GRPO, RLOO) and policy loss functions (PPO, GSPO, GPG)\n\nSources: Source: verl/trainer/main_ppo.py:1-80\n```python\n# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     \n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nNote that we don't combine the main with ray_trainer as ray_trainer is used by other mpain.\n\"\"\"\n\nimport os\nimport socket\n\nimport hydra\nimport ray\nfrom omegaconf import OmegaConf\n\nfrom verl.experimental.dataset.sampler import AbstractSampler\nfrom verl.trainer.constants_ppo import get_ppo_ray_runtime_env\nfrom verl.trainer.ppo.ray_trainer import RayPPOTrainer\nfrom verl.trainer.ppo.reward import load_reward_manager\nfrom verl.trainer.ppo.utils import need_critic, need_reference_policy\nfrom verl.utils.config import validate_config\nfrom verl.utils.device import auto_set_ascend_device_name, is_cuda_available\nfrom verl.utils.import_utils import load_extern_object\n\n\n@hydra.main(config_path=\"config\", config_name=\"ppo_trainer\", version_base=None)\ndef main(config):\n    \"\"\"Main entry point for PPO training with Hydra configuration management.\n\n    Args:\n        config_dict: Hydra configuration dictionary containing training parameters.\n    \"\"\"\n    # Automatically set `config.trainer.device = npu` when running on Ascend NPU.\n    auto_set_ascend_device_name(config)\n\n    run_ppo(config)\n\n\n# Define a function to run the PPO-like training process\ndef run_ppo(config, task_runner_class=None) -> None:\n    \"\"\"Initialize Ray cluster and run distributed PPO training process.\n\n    Args:\n        config: Training configuration object containing all necessary parameters\n                for distributed PPO training including Ray initialization settings,\n                model paths, and training hyperparameters.\n        task_runner_class: For recipe to change TaskRunner.\n    \"\"\"\n    # Check if Ray is not initialized\n    if not ray.is_initialized():\n        # Initialize Ray with a local cluster configuration\n        # Set environment variables in the runtime environment to control tokenizer parallelism,\n        # NCCL debug level, VLLM logging level, and allow runtime LoRA updating\n        # `num_cpus` specifies the number of CPU cores Ray can use, obtained from the configuration\n        default_runtime_env = get_ppo_ray_runtime_env()\n        ray_init_kwargs = config.ray_kwargs.get(\"ray_init\", {})\n        runtime_env_kwargs = ray_init_kwargs.get(\"runtime_env\", {})\n\n        if config.transfer_queue.enable:\n            # Add runtime environment variables for transfer queue\n            runtime_env_vars = runtime_env_kwargs.get(\"env_vars\", {})\n            runtime_env_vars[\"TRANSFER_QUEUE_ENABLE\"] = \"1\"\n            runtime_env_kwargs[\"env_vars\"] = runtime_env_vars\n\n        runtime_env = OmegaConf.merge(default_runtime_env, runtime_env_kwargs)\n        ray_init_kwargs = OmegaConf.create({**ray_init_kwargs, \"runtime_env\": runtime_env})\n        print(f\"ray init kwargs: {ray_init_kwargs}\")\n        ray.init(**OmegaConf.to_container(ray_init_kwargs))\n\n    if task_runner_class is None:\n        task_runner_class = ray.remote(num_cpus=1)(TaskRunner)  # please make sure main_task is not scheduled on head\n```, Source: verl/trainer/ppo/ray_trainer.py:1-80\n```python\n# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright 2023-2024 SGLang Team\n# Copyright 2025 ModelBest Inc. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     \n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nPPO Trainer with Ray-based single controller.\nThis trainer supports model-agonistic model initialization with huggingface\n\"\"\"\n\nimport json\nimport os\nimport uuid\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom dataclasses import dataclass, field\nfrom pprint import pprint\nfrom typing import Any, Optional\n\nimport numpy as np\nimport ray\nimport torch\nfrom omegaconf import OmegaConf, open_dict\nfrom torch.utils.data import Dataset, Sampler\nfrom torchdata.stateful_dataloader import StatefulDataLoader\nfrom tqdm import tqdm\n\nfrom verl import DataProto\nfrom verl.experimental.dataset.sampler import AbstractCurriculumSampler\nfrom verl.protocol import pad_dataproto_to_divisor, unpad_dataproto\nfrom verl.single_controller.ray import RayClassWithInitArgs, RayResourcePool, RayWorkerGroup\nfrom verl.single_controller.ray.base import create_colocated_worker_cls\nfrom verl.trainer.config import AlgoConfig\nfrom verl.trainer.ppo import core_algos\nfrom verl.trainer.ppo.core_algos import AdvantageEstimator, agg_loss\nfrom verl.trainer.ppo.metric_utils import (\n    compute_data_metrics,\n    compute_throughout_metrics,\n    compute_timing_metrics,\n    process_validation_metrics,\n)\nfrom verl.trainer.ppo.reward import compute_reward, compute_reward_async\nfrom verl.trainer.ppo.utils import Role, WorkerType, need_critic, need_reference_policy, need_reward_model\nfrom verl.utils import tensordict_utils as tu\nfrom verl.utils.checkpoint.checkpoint_manager import find_latest_ckpt_path, should_save_ckpt_esi\nfrom verl.utils.config import omega_conf_to_dataclass\nfrom verl.utils.debug import marked_timer\nfrom verl.utils.import_utils import load_class_from_fqn\nfrom verl.utils.metric import reduce_metrics\nfrom verl.utils.py_functional import rename_dict\nfrom verl.utils.rollout_skip import RolloutSkip\nfrom verl.utils.seqlen_balancing import calculate_workload, get_seqlen_balanced_partitions, log_seqlen_unbalance\nfrom verl.utils.torch_functional import masked_mean\nfrom verl.utils.tracking import ValidationGenerationsLogger\nfrom verl.workers.config import FSDPEngineConfig\nfrom verl.workers.utils.padding import left_right_2_no_padding, no_padding_2_padding\n\n\n@dataclass\nclass ResourcePoolManager:\n    \"\"\"\n    Define a resource pool specification. Resource pool will be initialized first.\n    \"\"\"\n\n    resource_pool_spec: dict[str, list[int]]\n    mapping: dict[Role, str]\n    resource_pool_dict: dict[str, RayResourcePool] = field(default_factory=dict)\n\n    def create_resource_pool(self):\n        \"\"\"Create Ray resource pools for distributed training.\n```, verl/workers/fsdp_workers/, verl/model_engine/, verl/utils/trainer/core_algos.py\n\n---\n\n```mermaid\ngraph LR\n    subgraph \"Data Layer\"\n        Dataset\"RLHFDataset<br/>parquet files\"\n        DataProto\"DataProto<br/>TensorDict wrapper\"\n    end\n    \n    subgraph \"Training Orchestration\"\n        Trainer[\"RayPPOTrainer\"]\n    end\n    \n    subgraph \"Generation Phase\"\n        Rollout\"Rollout Workers<br/>vLLM/SGLang\"\n        RefPolicy\"Reference Policy<br/>Frozen Parameters\"\n    end\n    \n    subgraph \"Reward Phase\"\n        RewardFn\"Reward Function<br/>verl.utils.reward_score\"\n        RewardModel\"Reward Model<br/>Optional\"\n    end\n    \n    subgraph \"Advantage Computation\"\n        Critic\"Critic Workers<br/>Value Estimation\"\n        AdvEst\"AdvantageEstimator<br/>GAE/GRPO/RLOO\"\n    end\n    \n    subgraph \"Update Phase\"\n        Actor\"Actor Workers<br/>Policy Update\"\n        PolicyLoss\"PolicyLoss Function<br/>PPO/GSPO/GPG\"\n    end\n    \n    Dataset --> DataProto\n    DataProto --> Trainer\n    Trainer --> Rollout\n    Rollout --> RefPolicy\n    RefPolicy --> RewardFn\n    RewardFn --> RewardModel\n    RewardModel --> Critic\n    Critic --> AdvEst\n    AdvEst --> Actor\n    Actor --> PolicyLoss\n    PolicyLoss --> Trainer\n```\n\n**Title: PPO Training Data Flow with Code Components**\n\nThis diagram shows how data flows through the training pipeline:\n\n1. **Data Loading**: `RLHFDataset` reads parquet files and wraps data in `DataProto` structures Source: verl/utils/dataset/rl_dataset.py:1-80\n```python\n# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright 2023-2024 SGLang Team\n# Copyright 2025 ModelBest Inc. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     \n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\nimport logging\nimport os\nimport re\nimport traceback\nfrom collections import defaultdict\nfrom typing import Optional\n\nimport datasets\nimport numpy as np\nimport torch\nfrom omegaconf import DictConfig, ListConfig\nfrom torch.utils.data import Dataset\nfrom transformers import PreTrainedTokenizer, ProcessorMixin\n\nimport verl.utils.torch_functional as verl_F\nfrom verl.utils.model import compute_position_id_with_mask\n\nlogger = logging.getLogger(__name__)\n\n\ndef collate_fn(data_list: list[dict]) -> dict:\n    \"\"\"\n    Collate a batch of sample dicts into batched tensors and arrays.\n\n    Args:\n        data_list: List of dicts mapping feature names to torch.Tensor or other values.\n\n    Returns:\n        Dict where tensor entries are stacked into a torch.Tensor of shape\n        (batch_size, \\\\*dims) and non-tensor entries are converted to\n        np.ndarray of dtype object with shape (batch_size,).\n    \"\"\"\n    tensors = defaultdict(list)\n    non_tensors = defaultdict(list)\n\n    for data in data_list:\n        for key, val in data.items():\n            if isinstance(val, torch.Tensor):\n                tensors[key].append(val)\n            else:\n                non_tensors[key].append(val)\n\n    for key, val in tensors.items():\n        tensors[key] = torch.stack(val, dim=0)\n\n    for key, val in non_tensors.items():\n        non_tensors[key] = np.fromiter(val, dtype=object, count=len(val))\n\n    return {**tensors, **non_tensors}\n\n\nclass RLHFDataset(Dataset):\n    \"\"\"\n    Load and preprocess RLHF data from Parquet files.\n\n    - Caches files locally.\n    - Reads into a HuggingFace Dataset and tokenizes prompts.\n    - Optionally handles images/videos via a ProcessorMixin.\n    - Filters prompts over a max length.\n    - Supports resuming from checkpoints.\n\n    Args:\n        data_files (str or list): Path(s) to Parquet file(s).\n```\n2. **Generation**: `RolloutWorker` generates responses using vLLM or SGLang, while `RefPolicy` computes log probabilities for KL penalty\n3. **Reward**: Custom reward functions verl/utils/reward_score/ or reward models compute token-level rewards\n4. **Value Estimation**: `CriticWorker` predicts state values for advantage calculation\n5. **Advantage Calculation**: `AdvantageEstimator` (GAE, GRPO, etc.) computes advantages from rewards and values\n6. **Policy Update**: `ActorWorker` updates policy parameters using `PolicyLoss` functions (PPO clip, GSPO, etc.)\n\nThe entire flow is coordinated by `RayPPOTrainer` which manages worker synchronization and data batching.\n\nSources: Source: verl/trainer/ppo/ray_trainer.py:1-80\n```python\n# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright 2023-2024 SGLang Team\n# Copyright 2025 ModelBest Inc. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     \n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nPPO Trainer with Ray-based single controller.\nThis trainer supports model-agonistic model initialization with huggingface\n\"\"\"\n\nimport json\nimport os\nimport uuid\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom dataclasses import dataclass, field\nfrom pprint import pprint\nfrom typing import Any, Optional\n\nimport numpy as np\nimport ray\nimport torch\nfrom omegaconf import OmegaConf, open_dict\nfrom torch.utils.data import Dataset, Sampler\nfrom torchdata.stateful_dataloader import StatefulDataLoader\nfrom tqdm import tqdm\n\nfrom verl import DataProto\nfrom verl.experimental.dataset.sampler import AbstractCurriculumSampler\nfrom verl.protocol import pad_dataproto_to_divisor, unpad_dataproto\nfrom verl.single_controller.ray import RayClassWithInitArgs, RayResourcePool, RayWorkerGroup\nfrom verl.single_controller.ray.base import create_colocated_worker_cls\nfrom verl.trainer.config import AlgoConfig\nfrom verl.trainer.ppo import core_algos\nfrom verl.trainer.ppo.core_algos import AdvantageEstimator, agg_loss\nfrom verl.trainer.ppo.metric_utils import (\n    compute_data_metrics,\n    compute_throughout_metrics,\n    compute_timing_metrics,\n    process_validation_metrics,\n)\nfrom verl.trainer.ppo.reward import compute_reward, compute_reward_async\nfrom verl.trainer.ppo.utils import Role, WorkerType, need_critic, need_reference_policy, need_reward_model\nfrom verl.utils import tensordict_utils as tu\nfrom verl.utils.checkpoint.checkpoint_manager import find_latest_ckpt_path, should_save_ckpt_esi\nfrom verl.utils.config import omega_conf_to_dataclass\nfrom verl.utils.debug import marked_timer\nfrom verl.utils.import_utils import load_class_from_fqn\nfrom verl.utils.metric import reduce_metrics\nfrom verl.utils.py_functional import rename_dict\nfrom verl.utils.rollout_skip import RolloutSkip\nfrom verl.utils.seqlen_balancing import calculate_workload, get_seqlen_balanced_partitions, log_seqlen_unbalance\nfrom verl.utils.torch_functional import masked_mean\nfrom verl.utils.tracking import ValidationGenerationsLogger\nfrom verl.workers.config import FSDPEngineConfig\nfrom verl.workers.utils.padding import left_right_2_no_padding, no_padding_2_padding\n\n\n@dataclass\nclass ResourcePoolManager:\n    \"\"\"\n    Define a resource pool specification. Resource pool will be initialized first.\n    \"\"\"\n\n    resource_pool_spec: dict[str, list[int]]\n    mapping: dict[Role, str]\n    resource_pool_dict: dict[str, RayResourcePool] = field(default_factory=dict)\n\n    def create_resource_pool(self):\n        \"\"\"Create Ray resource pools for distributed training.\n```, Source: verl/utils/dataset/rl_dataset.py:1-80\n```python\n# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright 2023-2024 SGLang Team\n# Copyright 2025 ModelBest Inc. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     \n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\nimport logging\nimport os\nimport re\nimport traceback\nfrom collections import defaultdict\nfrom typing import Optional\n\nimport datasets\nimport numpy as np\nimport torch\nfrom omegaconf import DictConfig, ListConfig\nfrom torch.utils.data import Dataset\nfrom transformers import PreTrainedTokenizer, ProcessorMixin\n\nimport verl.utils.torch_functional as verl_F\nfrom verl.utils.model import compute_position_id_with_mask\n\nlogger = logging.getLogger(__name__)\n\n\ndef collate_fn(data_list: list[dict]) -> dict:\n    \"\"\"\n    Collate a batch of sample dicts into batched tensors and arrays.\n\n    Args:\n        data_list: List of dicts mapping feature names to torch.Tensor or other values.\n\n    Returns:\n        Dict where tensor entries are stacked into a torch.Tensor of shape\n        (batch_size, \\\\*dims) and non-tensor entries are converted to\n        np.ndarray of dtype object with shape (batch_size,).\n    \"\"\"\n    tensors = defaultdict(list)\n    non_tensors = defaultdict(list)\n\n    for data in data_list:\n        for key, val in data.items():\n            if isinstance(val, torch.Tensor):\n                tensors[key].append(val)\n            else:\n                non_tensors[key].append(val)\n\n    for key, val in tensors.items():\n        tensors[key] = torch.stack(val, dim=0)\n\n    for key, val in non_tensors.items():\n        non_tensors[key] = np.fromiter(val, dtype=object, count=len(val))\n\n    return {**tensors, **non_tensors}\n\n\nclass RLHFDataset(Dataset):\n    \"\"\"\n    Load and preprocess RLHF data from Parquet files.\n\n    - Caches files locally.\n    - Reads into a HuggingFace Dataset and tokenizes prompts.\n    - Optionally handles images/videos via a ProcessorMixin.\n    - Filters prompts over a max length.\n    - Supports resuming from checkpoints.\n\n    Args:\n        data_files (str or list): Path(s) to Parquet file(s).\n```, verl/workers/, verl/utils/trainer/core_algos.py\n\n---\n\nverl provides multiple backend options for both training and inference, allowing users to choose based on their scale and performance requirements.\n\n| Backend | Use Case | Parallelism Support | Configuration |\n|---------|----------|---------------------|---------------|\n| **PyTorch FSDP** | Research, prototyping, small-to-medium models (up to 70B) | FSDP sharding, Ulysses sequence parallelism, CPU offload | `actor_rollout_ref.actor.strategy=fsdp` |\n| **Megatron-LM** | Production, large-scale models (up to 671B+) | Tensor (TP), Pipeline (PP), Context (CP), Expert (EP), Sequence parallelism | `actor_rollout_ref.actor.strategy=megatron` |\n\n**FSDP Backend** verl/trainer/fsdp/:\n- Uses PyTorch's native FSDP2 for parameter sharding\n- Supports gradient checkpointing and optimizer offloading\n- Easier to extend for custom models\n- Recommended for models up to 70B parameters\n\n**Megatron Backend** verl/trainer/megatron/:\n- Integrates Megatron-LM Core v0.13.1\n- Supports complex parallelism strategies (3D parallelism: TP+PP+DP)\n- Distributed optimizer and expert parallelism for MoE models\n- Required for models exceeding 100B parameters\n\n| Backend | Use Case | Key Features | Configuration |\n|---------|----------|--------------|---------------|\n| **vLLM** | High-throughput generation | PagedAttention, continuous batching, async execution | `actor_rollout_ref.rollout.name=vllm` |\n| **SGLang** | Advanced features, multi-turn | RadixAttention, tool calling, multi-modal support | `actor_rollout_ref.rollout.name=sglang` |\n\n**vLLM Backend** verl/workers/rollout/vllm/:\n- Uses `ExternalZeroMQExecutor` for distributed inference\n- Tight integration with Ray through ZeroMQ messaging\n- Supports tensor parallelism for large models\n- Best for pure generation tasks\n\n**SGLang Backend** verl/workers/rollout/sglang/:\n- HTTP-based server architecture with `ServerAdapter` for weight synchronization\n- Native support for multi-turn conversations and tool calling\n- Better prefix caching with RadixAttention\n- Recommended for agentic workflows and VLMs\n\nSources: Source: docs/start/install.rst:10-32\n```text\nverl supports various backends. Currently, the following configurations are available:\n\n- **FSDP** and **Megatron-LM** (optional) for training.\n- **SGLang**, **vLLM** and **TGI** for rollout generation.\n\nChoices of Backend Engines\n----------------------------\n\n1. Training:\n\nWe recommend using **FSDP** backend to investigate, research and prototype different models, datasets and RL algorithms. The guide for using FSDP backend can be found in :doc:`FSDP Workers<../workers/fsdp_workers>`.\n\nFor users who pursue better scalability, we recommend using **Megatron-LM** backend. Currently, we support `Megatron-LM v0.13.1 < The guide for using Megatron-LM backend can be found in :doc:`Megatron-LM Workers<../workers/megatron_workers>`.\n\n\n2. Inference:\n\nFor inference, vllm 0.8.3 and later versions have been tested for stability. We recommend turning on env var `VLLM_USE_V1=1` for optimal performance.\n\nFor SGLang, refer to the :doc:`SGLang Backend<../workers/sglang_worker>` for detailed installation and usage instructions. SGLang rollout is under extensive development and offers many advanced features and optimizations. We encourage users to report any issues or provide feedback via the `SGLang Issue Tracker <\n\nFor huggingface TGI integration, it is usually used for debugging and single GPU exploration.\n```, verl/trainer/fsdp/, verl/trainer/megatron/, verl/workers/rollout/\n\n---\n\nA typical PPO training iteration in verl follows these steps:\n\n1. **Rollout Generation** (`RayPPOTrainer.train_step`):\n   - Actor model generates responses via `rollout_manager.generate_sequences()`\n   - Reference policy computes log probabilities for KL divergence\n   - Batch size: `data.train_batch_size`, controlled by configuration\n\n2. **Reward Computation**:\n   - Reward function/model processes (prompt, response) pairs\n   - KL penalty applied: `reward - kl_coef * (log_prob - ref_log_prob)`\n   - Returns token-level rewards in `DataProto.batch['rewards']`\n\n3. **Value Estimation**:\n   - Critic workers predict values: `critic.compute_values(data_proto)`\n   - Values stored in `DataProto.batch['values']`\n\n4. **Advantage Calculation** (`core_algos.py`):\n   - Advantage estimator (GAE/GRPO) computes advantages and returns\n   - Registered via `@register_adv()` decorator\n   - Output: `advantages`, `returns` tensors\n\n5. **Policy Update**:\n   - Actor workers perform PPO updates for `ppo_epochs` iterations\n   - Mini-batch size: `actor.ppo_mini_batch_size`\n   - Policy loss computed via registered loss function (PPO clip, GSPO, etc.)\n   - Gradient accumulation with micro-batches: `ppo_micro_batch_size_per_gpu`\n\n6. **Critic Update**:\n   - Value function updated to minimize `(returns - values)^2`\n   - Separate optimizer from actor\n\nThe `RayPPOTrainer` coordinates all phases, collecting metrics and checkpointing at specified intervals (`trainer.save_freq`, `trainer.test_freq`).\n\nSources: Source: verl/trainer/ppo/ray_trainer.py:200-500\n```python\n    Args:\n        data (DataProto): The data containing batched model outputs and inputs.\n        adv_estimator (AdvantageEstimator): The advantage estimator to use (e.g., GAE, GRPO, REINFORCE++).\n        gamma (float, optional): Discount factor for future rewards. Defaults to 1.0.\n        lam (float, optional): Lambda parameter for GAE. Defaults to 1.0.\n        num_repeat (int, optional): Number of times to repeat the computation. Defaults to 1.\n        norm_adv_by_std_in_grpo (bool, optional): Whether to normalize advantages by standard deviation in\n            GRPO. Defaults to True.\n        config (dict, optional): Configuration dictionary for algorithm settings. Defaults to None.\n\n    Returns:\n        DataProto: The updated data with computed advantages and returns.\n    \"\"\"\n    # Back-compatible with trainers that do not compute response mask in fit\n    if \"response_mask\" not in data.batch.keys():\n        data.batch[\"response_mask\"] = compute_response_mask(data)\n    # prepare response group\n    if adv_estimator == AdvantageEstimator.GAE:\n        # Compute advantages and returns using Generalized Advantage Estimation (GAE)\n        advantages, returns = core_algos.compute_gae_advantage_return(\n            token_level_rewards=data.batch[\"token_level_rewards\"],\n            values=data.batch[\"values\"],\n            response_mask=data.batch[\"response_mask\"],\n            gamma=gamma,\n            lam=lam,\n        )\n        data.batch[\"advantages\"] = advantages\n        data.batch[\"returns\"] = returns\n        if config.get(\"use_pf_ppo\", False):\n            data = core_algos.compute_pf_ppo_reweight_data(\n                data,\n                config.pf_ppo.get(\"reweight_method\"),\n                config.pf_ppo.get(\"weight_pow\"),\n            )\n    elif adv_estimator == AdvantageEstimator.GRPO:\n        # Initialize the mask for GRPO calculation\n        grpo_calculation_mask = data.batch[\"response_mask\"]\n\n        # Call compute_grpo_outcome_advantage with parameters matching its definition\n        advantages, returns = core_algos.compute_grpo_outcome_advantage(\n            token_level_rewards=data.batch[\"token_level_rewards\"],\n            response_mask=grpo_calculation_mask,\n            index=data.non_tensor_batch[\"uid\"],\n            norm_adv_by_std_in_grpo=norm_adv_by_std_in_grpo,\n        )\n        data.batch[\"advantages\"] = advantages\n        data.batch[\"returns\"] = returns\n    else:\n        # handle all other adv estimator type other than GAE and GRPO\n        adv_estimator_fn = core_algos.get_adv_estimator_fn(adv_estimator)\n        adv_kwargs = {\n            \"token_level_rewards\": data.batch[\"token_level_rewards\"],\n            \"response_mask\": data.batch[\"response_mask\"],\n            \"config\": config,\n        }\n        if \"uid\" in data.non_tensor_batch:  # optional\n            adv_kwargs[\"index\"] = data.non_tensor_batch[\"uid\"]\n        if \"reward_baselines\" in data.batch:  # optional\n            adv_kwargs[\"reward_baselines\"] = data.batch[\"reward_baselines\"]\n\n        # calculate advantage estimator\n        advantages, returns = adv_estimator_fn(**adv_kwargs)\n        data.batch[\"advantages\"] = advantages\n        data.batch[\"returns\"] = returns\n    return data\n\n\nclass RayPPOTrainer:\n    \"\"\"Distributed PPO trainer using Ray for scalable reinforcement learning.\n\n    This trainer orchestrates distributed PPO training across multiple nodes and GPUs,\n    managing actor rollouts, critic training, and reward computation with Ray backend.\n    Supports various model architectures including FSDP, Megatron, vLLM, and SGLang integration.\n    \"\"\"\n\n    # TODO: support each role have individual ray_worker_group_cls,\n    # i.e., support different backend of different role\n    def __init__(\n        self,\n        config,\n```, verl/utils/trainer/core_algos.py, Source: docs/start/quickstart.rst:1-80\n```text\n.. _quickstart:\n\n=========================================================\nQuickstart: PPO training on GSM8K dataset\n=========================================================\n\nPost-train a LLM using GSM8K dataset.\n\nIntroduction\n------------\n\n.. _hf_dataset_gsm8k: \n\nIn this example, we train an LLM to tackle the `GSM8k <hf_dataset_gsm8k>`_ task with function-based rewards. [1]_\n\nPrerequisite:\n\n- the latest version of ``verl`` and its dependencies installed following the installation guide. Using the docker image is recommended.\n\n- a GPU with at least 24 GB HBM\n\n\nDataset Introduction\n--------------------\n\nGSM8k is a math problem dataset. The prompt is an elementary school\nproblem. The LLM model is asked to solve the math problem. Below is an example:\n\nPrompt\n\n   Katy makes coffee using teaspoons of sugar and cups of water in the\n   ratio of 7:13. If she used a total of 120 teaspoons of sugar and cups\n   of water, calculate the number of teaspoonfuls of sugar she used.\n\nSolution\n\n   The total ratio representing the ingredients she used to make the\n   coffee is 7+13 = <<7+13=20>>20 Since the fraction representing the\n   number of teaspoons she used is 7/20, she used 7/20\\ *120 =\n   <<7/20*\\ 120=42>>42 #### 42\n\nStep 1: Prepare the dataset\n----------------------------\n\nWe preprocess the dataset in parquet format so that (1) it contains necessary fields for computing RL rewards and (2) is faster to read.\n\n.. code-block:: bash\n\n   python3 examples/data_preprocess/gsm8k.py --local_save_dir ~/data/gsm8k\n\nStep 2: Download a model for post-training\n-------------------------------------------\n\nIn this example, we start with the ``Qwen2.5-0.5B-Instruct`` model.\n\nIf you want to perform SFT before RL, refer to the :doc:`Complete GSM8K Example<../examples/gsm8k_example>`, the `sft directory < and `SFT Trainer < for further details.\n\n.. code-block:: bash\n\n   python3 -c \"import transformers; transformers.pipeline('text-generation', model='Qwen/Qwen2.5-0.5B-Instruct')\"\n\nStep 3: Perform PPO training with the instruct model\n----------------------------------------------------------------------\n\n**Reward Model/Function**\n\nWe use a pre-defined rule-based reward model. We force the model to produce a final\nanswer following 4 â€œ#â€ as shown in the solution. We extract the final\nanswer from both the solution and model's output using regular\nexpression matching. We assign a reward of 1 to correct\nanswer, 0.0 to incorrect answer and 0 to no answer. \n\nFor more details, please refer to `verl/utils/reward_score/gsm8k.py <\n\n**Training Script**\n\nNow let's run PPO training with the dataset and model above. [2]_\n\n\nSet the ``data.train_files`` ,\\ ``data.val_files``, ``actor_rollout_ref.model.path`` and ``critic.model.path`` based on your dataset and model names or paths.\n```\n\n---\n\nverl implements efficient resource management through two key mechanisms:\n\nThe `ResourcePoolManager` verl/utils/resource_pool_manager.py allocates GPUs to different worker roles:\n\n```python\n# Example resource pool configuration\nresource_pool:\n  actor_rollout_ref:\n    devices: [0, 1, 2, 3]  # GPUs for actor training and rollout\n  critic:\n    devices: [4, 5]         # GPUs for critic\n  reward_model:\n    devices: [6, 7]         # GPUs for reward model\n```\n\nWorkers are mapped to resource pools, enabling flexible deployment strategies:\n- **Colocated**: Actor and rollout share GPUs (memory-efficient mode switching)\n- **Standalone**: Separate GPU sets for training and inference\n- **Hybrid**: Dynamic allocation based on training phase\n\nThe `HybridEngine` verl/trainer/fsdp/hybrid_engine.py enables dynamic switching between training and inference modes:\n\n- **Training Mode**: Full parameter precision (FP32/BF16), gradient tracking enabled\n- **Inference Mode**: Lower precision (BF16/FP8), optimized kernels, KV cache\n\n**Mode transitions:**\n- `wake_up()`: Synchronize weights from trainer to rollout workers\n- `sleep()`: Release GPU memory from rollout to allow training\n\nThis architecture eliminates redundant parameter copies and reduces memory usage by 30-50% compared to maintaining separate model instances.\n\nFor implementation details, see Key Innovations and Design Patterns.\n\nSources: verl/utils/resource_pool_manager.py, verl/trainer/fsdp/hybrid_engine.py, verl/workers/rollout/\n\n---\n\nverl supports multiple hardware platforms:\n\n| Platform | Training | Inference | Special Considerations |\n|----------|----------|-----------|------------------------|\n| **NVIDIA GPUs** | FSDP, Megatron-LM | vLLM, SGLang | CUDA 12.8+, cuDNN 9.10+ recommended |\n| **AMD GPUs (ROCm)** | FSDP | vLLM | ROCm 6.2+, see AMD Support documentation |\n| **Huawei Ascend NPUs** | FSDP, Megatron-LM | Custom backend | CANN framework, Atlas 800T/900 series |\n\n**Multi-node support:**\n- Ray cluster setup for distributed training\n- Slurm integration for HPC clusters\n- SkyPilot for cloud deployment (GCP, AWS, Azure)\n- Docker/Podman containers for reproducibility\n\nSee Installation and Environment Setup for detailed setup instructions.\n\nSources: Source: docs/start/install.rst:1-80\n```text\nInstallation\n============\n\nRequirements\n------------\n\n- **Python**: Version >= 3.10\n- **CUDA**: Version >= 12.8\n\nverl supports various backends. Currently, the following configurations are available:\n\n- **FSDP** and **Megatron-LM** (optional) for training.\n- **SGLang**, **vLLM** and **TGI** for rollout generation.\n\nChoices of Backend Engines\n----------------------------\n\n1. Training:\n\nWe recommend using **FSDP** backend to investigate, research and prototype different models, datasets and RL algorithms. The guide for using FSDP backend can be found in :doc:`FSDP Workers<../workers/fsdp_workers>`.\n\nFor users who pursue better scalability, we recommend using **Megatron-LM** backend. Currently, we support `Megatron-LM v0.13.1 < The guide for using Megatron-LM backend can be found in :doc:`Megatron-LM Workers<../workers/megatron_workers>`.\n\n\n2. Inference:\n\nFor inference, vllm 0.8.3 and later versions have been tested for stability. We recommend turning on env var `VLLM_USE_V1=1` for optimal performance.\n\nFor SGLang, refer to the :doc:`SGLang Backend<../workers/sglang_worker>` for detailed installation and usage instructions. SGLang rollout is under extensive development and offers many advanced features and optimizations. We encourage users to report any issues or provide feedback via the `SGLang Issue Tracker <\n\nFor huggingface TGI integration, it is usually used for debugging and single GPU exploration.\n\nInstall from docker image\n-------------------------\n\nStart from v0.6.0, we use vllm and sglang release image as our base image.\n\nBase Image\n::::::::::\n\n- vLLM: \n- SGLang: \n\nApplication Image\n:::::::::::::::::\n\nUpon base image, the following packages are added:\n\n- flash_attn\n- Megatron-LM\n- Apex\n- TransformerEngine\n- DeepEP\n\nLatest docker file:\n\n- `Dockerfile.stable.vllm <\n- `Dockerfile.stable.sglang <\n\nAll pre-built images are available in dockerhub: `verlai/verl < For example, ``verlai/verl:sgl055.latest``, ``verlai/verl:vllm011.latest``.\n\nYou can find the latest images used for development and ci in our github workflows:\n\n- `.github/workflows/vllm.yml <\n- `.github/workflows/sgl.yml <\n\n\nInstallation from Docker\n::::::::::::::::::::::::\n\nAfter pulling the desired Docker image and installing desired inference and training frameworks, you can run it with the following steps:\n\n1. Launch the desired Docker image and attach into it:\n\n.. code:: bash\n\n    docker create --runtime=nvidia --gpus all --net=host --shm-size=\"10g\" --cap-add=SYS_ADMIN -v .:/workspace/verl --name verl <image:tag> sleep infinity\n    docker start verl\n    docker exec -it verl bash\n```, Source: docs/start/multinode.rst:1-80\n```text\nMultinode Training\n==================\n\nLast updated: 06/10/2025.\n\n.. _wuxibin89: \n\nAuthor: `Xibin Wu < `Yusheng Su <\n\nOption 1: Launch Manually\n------------------------------\n\nSet up multinode ray cluster\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n1. Start head node with ``ray start --head --dashboard-host=0.0.0.0``, there're 2 address you should care about:\n\n- GCS address: ``ray start --address=<address>``, where worker node should connect to.\n- Dashboard address: ``<address>:8265``, where you should submit job to the cluster.\n\n.. image:: \n\n2. Start worker node with ``ray start --address=<address>`` you get above.\n\n.. image:: \n\n3. Now you should see the cluster have 2 nodes with ``ray status``.\n\n.. image:: \n\n4. Additionally, you can access dashboard in the browser with the address you get above. \n\n*Firewall rules maybe need configure to access the dashboard, if there's any trouble, please contact your network administrator.*\n\n.. image:: \n\nSubmit job to ray cluster\n~~~~~~~~~~~~~~~~~~~~~~~~~\n1. Submit ray job to cluster with the dashboard address you get above.\n\n.. code-block:: bash\n\n    ray job submit --address=\" \\\n        --runtime-env=verl/trainer/runtime_env.yaml \\\n        --no-wait \\\n        -- \\\n        python3 -m verl.trainer.main_ppo \\\n        trainer.n_gpus_per_node=8 \\\n        trainer.nnodes=2 \\\n        ...\n\n.. image:: \n\n2. Then you can check the job status with the following commands:\n\n- ray job list: list all jobs submitted to the cluster.\n- ray job logs <Submission ID>: query the logs of the job.\n- ray job status <Submission ID>: query the status of the job.\n- ray job stop <Submission ID>: request the job to be stopped.\n- ray job list | grep submission_id | grep JobStatus | grep RUNNING | grep -oP 'raysubmit_^'\\''\"+' | head -n 1: get the latest job submission ID of the running job.\n- ray job logs <Submission ID> --follow: added ``--follow`` parameter to ray job logs command to enable continuous log streaming.\n\n3. You can also access driver/task/actor logs in ``/tmp/ray/session_latest/logs/``, driver log is ``job-driver-raysubmit_<Submission ID>.log``.\n\n4. We strongly recommend you to view job detail from dashboard in multinode training, because it provide more structure way to view the job information.\n\n.. image:: \n.. image:: \n\nOption 2: Launch via SkyPilot on Kubernetes or clouds\n------------------------------------------------------\n\n.. note::\n   Ready-to-use SkyPilot example configurations are available in the `examples/skypilot/ < directory:\n   \n   - ``verl-ppo.yaml`` - PPO training with GSM8K dataset\n   - ``verl-grpo.yaml`` - GRPO training with MATH dataset  \n   - ``verl-multiturn-tools.yaml`` - Multi-turn tool usage training\n   \n   See the `SkyPilot examples README < for detailed usage instructions.\n```, docs/ascend_tutorial/\n\n---\n\nverl uses Hydra for hierarchical configuration management. The main configuration file `ppo_trainer.yaml` verl/trainer/ppo/ppo_trainer.yaml defines:\n\n- **Data**: Dataset paths, batch sizes, sequence lengths\n- **Actor/Critic/Reference**: Model paths, optimization parameters, FSDP/Megatron settings\n- **Rollout**: Inference backend (vLLM/SGLang), parallelism configuration\n- **Algorithm**: KL controller, advantage estimator, policy loss function\n- **Trainer**: Number of GPUs, epochs, checkpoint frequency, logging\n\nExample configuration override:\n\n```bash\npython -m verl.trainer.main_ppo \\\n    data.train_batch_size=256 \\\n    actor_rollout_ref.model.path=Qwen/Qwen2.5-7B \\\n    actor_rollout_ref.rollout.name=vllm \\\n    algorithm.kl_ctrl.kl_coef=0.001\n```\n\nFor comprehensive configuration documentation, see Configuration System.\n\nSources: verl/trainer/ppo/ppo_trainer.yaml, Source: docs/examples/config.rst:1-80\n```text\n.. _config-explain-page:\n\nConfig Explanation\n===================\n\nLast updated: 06/18/2025.\n\nppo_trainer.yaml for RL FSDP Backend\n-------------------------------------\n\nData\n~~~~\n\n.. code:: yaml\n\n   data:\n     tokenizer: null\n     train_files: ~/data/rlhf/gsm8k/train.parquet\n     val_files: ~/data/rlhf/gsm8k/test.parquet\n     train_max_samples: -1  # set to -1 to use full dataset\n     val_max_samples: -1  # set to -1 to use full dataset\n     prompt_key: prompt\n     max_prompt_length: 512\n     max_response_length: 512\n     train_batch_size: 1024\n     return_raw_input_ids: False  # This should be set to true when the tokenizer between policy and rm differs\n     return_raw_chat: False\n     return_full_prompt: False\n     shuffle: True\n     seed: 42\n     filter_overlong_prompts: False\n     filter_overlong_prompts_workers: 1\n     truncation: error\n     image_key: images\n     trust_remote_code: True\n     custom_cls:\n        path: null\n        name: null\n\n- ``data.train_files``: Training set parquet. Can be a list or a single\n  file. The program will read all files into memory, so it can't be too\n  large (< 100GB). The path can be either local path or HDFS path. For\n  HDFS path, we provide utils to download it to DRAM and convert the\n  HDFS path to local path.\n- ``data.val_files``: Validation parquet. Can be a list or a single\n  file.\n- ``data.train_max_samples``: Maximum number of samples to use from the\n  training dataset. Set to -1 to use the full dataset.\n- ``data.val_max_samples``: Maximum number of samples to use from the\n  validation dataset. Set to -1 to use the full dataset.\n- ``data.prompt_key``: The field in the dataset where the prompt is\n  located. Default is 'prompt'.\n- ``data.max_prompt_length``: Maximum prompt length. All prompts will be\n  left-padded to this length. An error will be reported if the length is\n  too long\n- ``data.max_response_length``: Maximum response length. Rollout in RL\n  algorithms (e.g. PPO) generates up to this length\n- ``data.train_batch_size``: Batch size sampled for one training\n  iteration of different RL algorithms.\n- ``data.return_raw_input_ids``: Whether to return the original\n  input_ids without adding chat template. This is mainly used to\n  accommodate situations where the reward model's chat template differs\n  from the policy. It needs to be decoded first, then apply the RM's\n  chat template. If using a model-based RM, and the policy and RM\n  chat_templates are different, this flag needs to be set\n- ``data.return_raw_chat``: Whether to return the original chat (prompt)\n  without applying chat template.\n- ``data.return_full_prompt``: Whether to return the full prompt with chat template\n- ``data.shuffle``: Whether to shuffle the data in the dataloader.\n- ``data.seed``: An integer seed to use when shuffling the data. If not set or set to\n  `null`, the data shuffling will not be seeded, resulting in a different data order on each run.\n- ``data.filter_overlong_prompts``: Default don't filter.\n- ``data.filter_overlong_prompts_workers``: For large-scale dataset, filtering\n  overlong prompts could be timeconsuming. You cat set the ``filter_overlong_prompts_workers``\n  to use multiprocessing for speed up. Default to 1.\n- ``data.truncation``: Truncate the input_ids or prompt length if they\n  exceed max_prompt_length. Default is 'error', not allow exceed the\n  max_prompt_length. The users should increase the max_prompt_length if\n  throwing the error. You can also set ``left``, ``right`` and ``middle``. \n  When ``middle`` is selected, the logic splits the allowed max length roughly in half\n```, Source: docs/start/quickstart.rst:80-110\n```text\nSet the ``data.train_files`` ,\\ ``data.val_files``, ``actor_rollout_ref.model.path`` and ``critic.model.path`` based on your dataset and model names or paths.\nYou may set ``VERL_USE_MODELSCOPE=True`` to download models from `modelscope < instead of `huggingface <\n\n.. code-block:: bash\n\n   PYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \\\n    data.train_files=$HOME/data/gsm8k/train.parquet \\\n    data.val_files=$HOME/data/gsm8k/test.parquet \\\n    data.train_batch_size=256 \\\n    data.max_prompt_length=512 \\\n    data.max_response_length=512 \\\n    actor_rollout_ref.model.path=Qwen/Qwen2.5-0.5B-Instruct \\\n    actor_rollout_ref.actor.optim.lr=1e-6 \\\n    actor_rollout_ref.actor.ppo_mini_batch_size=64 \\\n    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \\\n    actor_rollout_ref.rollout.name=vllm \\\n    actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 \\\n    actor_rollout_ref.rollout.tensor_model_parallel_size=1 \\\n    actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \\\n    actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \\\n    critic.optim.lr=1e-5 \\\n    critic.model.path=Qwen/Qwen2.5-0.5B-Instruct \\\n    critic.ppo_micro_batch_size_per_gpu=4 \\\n    algorithm.kl_ctrl.kl_coef=0.001 \\\n    trainer.logger=console \\\n    trainer.val_before_train=False \\\n    trainer.n_gpus_per_node=1 \\\n    trainer.nnodes=1 \\\n    trainer.save_freq=10 \\\n    trainer.test_freq=10 \\\n    trainer.total_epochs=15 2>&1 | tee verl_demo.log\n```\n\n---\n\nTo begin using verl:\n\n1. **Install**: Follow Installation and Environment Setup to install via Docker or custom environment\n2. **Quick Start**: Run the Quick Start Guide to train on GSM8K dataset\n3. **Choose Backend**: Select FSDP or Megatron based on Backend Selection and Hardware Support\n4. **Configure**: Customize training via Configuration System\n5. **Extend**: Add custom models, algorithms, or reward functions following extension guides in sections 5, 10, and 11\n\nThe framework provides example scripts in `examples/` directory:\n- `examples/ppo_trainer/`: PPO training scripts\n- `examples/grpo_trainer/`: GRPO (critic-free) training\n- `examples/sft/`: Supervised fine-tuning examples\n- `examples/data_preprocess/`: Dataset preparation scripts\n\nSources: Source: docs/index.rst:25-37\n```text\n.. _Contents:\n\n.. toctree::\n   :maxdepth: 2\n   :caption: Quickstart\n\n   start/install\n   start/quickstart\n   start/multinode\n   start/ray_debug_tutorial\n   start/more_resources\n   start/agentic_rl\n```, Source: docs/start/quickstart.rst:1-80\n```text\n.. _quickstart:\n\n=========================================================\nQuickstart: PPO training on GSM8K dataset\n=========================================================\n\nPost-train a LLM using GSM8K dataset.\n\nIntroduction\n------------\n\n.. _hf_dataset_gsm8k: \n\nIn this example, we train an LLM to tackle the `GSM8k <hf_dataset_gsm8k>`_ task with function-based rewards. [1]_\n\nPrerequisite:\n\n- the latest version of ``verl`` and its dependencies installed following the installation guide. Using the docker image is recommended.\n\n- a GPU with at least 24 GB HBM\n\n\nDataset Introduction\n--------------------\n\nGSM8k is a math problem dataset. The prompt is an elementary school\nproblem. The LLM model is asked to solve the math problem. Below is an example:\n\nPrompt\n\n   Katy makes coffee using teaspoons of sugar and cups of water in the\n   ratio of 7:13. If she used a total of 120 teaspoons of sugar and cups\n   of water, calculate the number of teaspoonfuls of sugar she used.\n\nSolution\n\n   The total ratio representing the ingredients she used to make the\n   coffee is 7+13 = <<7+13=20>>20 Since the fraction representing the\n   number of teaspoons she used is 7/20, she used 7/20\\ *120 =\n   <<7/20*\\ 120=42>>42 #### 42\n\nStep 1: Prepare the dataset\n----------------------------\n\nWe preprocess the dataset in parquet format so that (1) it contains necessary fields for computing RL rewards and (2) is faster to read.\n\n.. code-block:: bash\n\n   python3 examples/data_preprocess/gsm8k.py --local_save_dir ~/data/gsm8k\n\nStep 2: Download a model for post-training\n-------------------------------------------\n\nIn this example, we start with the ``Qwen2.5-0.5B-Instruct`` model.\n\nIf you want to perform SFT before RL, refer to the :doc:`Complete GSM8K Example<../examples/gsm8k_example>`, the `sft directory < and `SFT Trainer < for further details.\n\n.. code-block:: bash\n\n   python3 -c \"import transformers; transformers.pipeline('text-generation', model='Qwen/Qwen2.5-0.5B-Instruct')\"\n\nStep 3: Perform PPO training with the instruct model\n----------------------------------------------------------------------\n\n**Reward Model/Function**\n\nWe use a pre-defined rule-based reward model. We force the model to produce a final\nanswer following 4 â€œ#â€ as shown in the solution. We extract the final\nanswer from both the solution and model's output using regular\nexpression matching. We assign a reward of 1 to correct\nanswer, 0.0 to incorrect answer and 0 to no answer. \n\nFor more details, please refer to `verl/utils/reward_score/gsm8k.py <\n\n**Training Script**\n\nNow let's run PPO training with the dataset and model above. [2]_\n\n\nSet the ``data.train_files`` ,\\ ``data.val_files``, ``actor_rollout_ref.model.path`` and ``critic.model.path`` based on your dataset and model names or paths.\n```, Source: README.md:102-157\n```markdown\n## Getting Started\n\n<a href=\"\n\n**Quickstart:**\n\n- Installation\n- Quickstart\n- Programming Guide & Tech Talk (in Chinese)\n- PPO in verl\n- GRPO in verl\n\n**Running a PPO example step-by-step:**\n\n- Prepare Data for Post-Training\n- Implement Reward Function for Dataset\n- PPO Example Architecture\n- Config Explanation\n\n**Reproducible algorithm baselines:**\n\n- RL performance on coding, math\n\n**For code explanation and advance usage (extension):**\n\n- PPO Trainer and Workers\n  - PPO Ray Trainer\n  - PyTorch FSDP Backend\n  - Megatron-LM Backend\n\n- Advanced Usage and Extension\n  - Add Models with the FSDP Backend\n  - Add Models with the Megatron-LM Backend\n  - Multi-turn Rollout Support\n  - Search Tool Integration\n  - Sandbox Fusion Integration\n  - Deployment using Separate GPU Resources\n  - Extend to Other RL(HF) algorithms\n  - Ray API design tutorial\n\n**Blogs from the community**\n\n- When Reasoning Models Break Tokenization: The Hidden Complexity of Multiturn Training\n- verl deployment on AWS SageMaker\n- verl x SGLang Multi-turn Code Walkthrough\n- Optimizing SGLang Memory Usage in verl\n- SGLang, verl, OpenBMB and Tsinghua University: Pioneering End-to-End Multi-Turn RLHF\n- Reinforcement Learning from Human Feedback on AMD GPUs with verl and ROCm Integration\n- veMLP x verl ï¼šçŽ©è½¬å¼ºåŒ–å­¦ä¹ è®­ç»ƒ\n- ä½¿ç”¨ verl è¿›è¡Œ GRPO åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœ€ä½³å®žè·µ\n- HybridFlow verl åŽŸæ–‡æµ…æž\n- æœ€é«˜æå‡ 20 å€åžåé‡ï¼è±†åŒ…å¤§æ¨¡åž‹å›¢é˜Ÿå‘å¸ƒå…¨æ–° RLHF æ¡†æž¶ï¼ŒçŽ°å·²å¼€æºï¼\n\n## Performance Tuning Guide\n\nThe performance is essential for on-policy RL algorithm. We have written a detailed performance tuning guide to help you optimize performance.\n```\n\n```markdown\n<div align=\"center\">\n ðŸ‘‹ Hi, everyone!\n    verl is a RL training library initiated by <b>ByteDance Seed team</b> and maintained by the verl community.\n    <br>\n    <br>\n</div>\n\n<div align=\"center\">\n\n<a href=\" src=\" alt=\"Ask DeepWiki.com\" style=\"height:20px;\"></a>\nGitHub Repo stars\nTwitter\n<a href=\" src=\"\n<a href=\" src=\"\nDocumentation\n<a href=\" src=\"\n\n</div>\n\nseed logo\n\n<h1 style=\"text-align: center;\">verl: Volcano Engine Reinforcement Learning for LLMs</h1>\n\nverl is a flexible, efficient and production-ready RL training library for large language models (LLMs).\n\nverl is the open-source version of **HybridFlow: A Flexible and Efficient RLHF Framework** paper.\n```\n\nSource: README.md:1-80\n```markdown\n<div align=\"center\">\n ðŸ‘‹ Hi, everyone!\n    verl is a RL training library initiated by <b>ByteDance Seed team</b> and maintained by the verl community.\n    <br>\n    <br>\n</div>\n\n<div align=\"center\">\n\n<a href=\" src=\" alt=\"Ask DeepWiki.com\" style=\"height:20px;\"></a>\nGitHub Repo stars\nTwitter\n<a href=\" src=\"\n<a href=\" src=\"\nDocumentation\n<a href=\" src=\"\n\n</div>\n\nseed logo\n\n<h1 style=\"text-align: center;\">verl: Volcano Engine Reinforcement Learning for LLMs</h1>\n\nverl is a flexible, efficient and production-ready RL training library for large language models (LLMs).\n\nverl is the open-source version of **HybridFlow: A Flexible and Efficient RLHF Framework** paper.\n\nverl is flexible and easy to use with:\n\n- **Easy extension of diverse RL algorithms**: The hybrid-controller programming model enables flexible representation and efficient execution of complex post-training dataflows. Build RL dataflows such as GRPO, PPO in a few lines of code.\n\n- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as FSDP, Megatron-LM, vLLM, SGLang, etc\n\n- **Flexible device mapping**: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.\n\n- Ready integration with popular HuggingFace models\n\nverl is fast with:\n\n- **State-of-the-art throughput**: SOTA LLM training and inference engine integrations and SOTA RL throughput.\n\n- **Efficient actor model resharding with 3D-HybridEngine**: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.\n\n</p>\n\n## News\n- 2025/11 recipes have been moved to a new repository: verl-recipe\n- 2025/10 verl is presented in the PyTorch Conference 2025.\n- 2025/08 verl is presented in the PyTorch Expert Exchange Webinar. Slides available.\n- 2025/07 The ReTool recipe is fully open sourced. Blog\n- 2025/07 The first verl meetup will be held at ICML Vancouver on July 16th! Please join us if you are at ICML! (onsite only)\n- 2025/06 verl with Megatron backend enables large MoE models such as DeepSeek-671B and Qwen3-235B.\n- 2025/03 DAPO is the open-sourced SOTA RL algorithm that achieves 50 points on AIME 2024 based on the Qwen2.5-32B pre-trained model, surpassing the previous SOTA achieved by DeepSeek's GRPO (DeepSeek-R1-Zero-Qwen-32B). DAPO's training is fully powered by verl and the reproduction code is available in `recipe/dapo` now.\n<details><summary> more... </summary>\n<ul>\n  <li>2025/04 Seed-Thinking-v1.5 tech report is released! Trained with verl, Seed-Thinking-v1.5 achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on GPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond reasoning tasks, the method demonstrates notable generalization across diverse domains.</li>\n  <li>2025/07 verl keynote at AWS AI Hours Singapore on 7/8, verl & verl-agent project updates at Agent for SWE meetup by LF AI & Data Singapore on 7/11.</li>\n  <li>2025/06 verl team will provide latest project updates at PyTorch Day China on June 7th. Meet our dev team in Beijing!</li>\n  <li> 2025/04 VAPO (value-based augmented PPO) paper covers our latest RL method for reasoning models. Trained from Qwen-32B-base model, VAPO achieves 60.4 on AIME 2024, outperforming DAPO-32B.</li>\n  <li>2025/05 PF-PPO, accepted to ICML 2025, is now supported in verl! PF-PPO enhances policy learning efficiency and robustness by filtering potentially noisy reward signals and reusing high-quality experiences via a replay buffer.</li>\n  <li>2025/04 We will give a tutorial about latest post-training techniques and programming guide for verl at ICLR 2025 Expo, SCI-FM workshop and LMSys afterparty. Talk materials available here. </li>\n  <li>2025/03 verl v0.3.0.post1 is released! See release note for details. It achieves ~1.4x speedup compared to prev versions.</li>\n  <li>2025/05 verl will be presented at A2M Shanghai on 5/16 - 5/17.</li>\n  <li>2025/05 verl will be presented at GOSIM x PyTorch Day 2025. See you in Paris! </li>\n  <li>2025/03 We introduced the programming model of verl at the vLLM Beijing Meetup and verl intro and updates at the SGLang-LMSYS Org Meetup in Sunnyvale mid-March.</li>\n  <li>2025/03 We will present verl(HybridFlow) at EuroSys 2025. See you in Rotterdam!</li>\n  <li>2025/02 verl v0.2.0.post2 is released!</li>\n  <li>2025/02 We presented verl in the <a href=\" Ray Meetup</a>. See you in San Jose!</li>\n  <li>2025/01 Doubao-1.5-pro is released with SOTA-level performance on LLM & VLM. The RL scaling preview model is trained using verl, reaching OpenAI O1-level performance on math benchmarks (70.0 pass@1 on AIME).</li>\n  <li>2024/12 verl is presented at Ray Forward 2024. Slides available <a href=\"\n  <li>2024/12 The team presented <a href=\" LLMs: From Algorithms to Infrastructure</a> at NeurIPS 2024. <a href=\" and <a href=\" available.</li>\n  <li>2024/10 verl is presented at Ray Summit. <a href=\" video</a> available.</li>\n  <li>2024/08 HybridFlow (verl) is accepted to EuroSys 2025.</li>\n</ul>\n</details>\n\n## Key Features\n\n- **FSDP**, **FSDP2** and **Megatron-LM** for training.\n- **vLLM**, **SGLang** and **HF Transformers** for rollout generation.\n```\n\nSource: docs/examples/gsm8k_example.rst:1-80\n```text\nGSM8K Example\n=============\n\nLast updated: 03/25/2025.\n\nIntroduction\n------------\n\nIn this example, we train an LLM to tackle the GSM8k task.\n\nPaper: \n\nDataset: \n\nNote that the original paper mainly focuses on training a verifier (a\nreward model) to solve math problems via Best-of-N sampling. In this\nexample, we train an RLHF agent using a rule-based reward model.\n\nDataset Introduction\n--------------------\n\nGSM8k is a math problem dataset. The prompt is an elementary school\nproblem. The LLM model is required to answer the math problem.\n\nThe training set contains 7473 samples and the test set contains 1319\nsamples.\n\n**An example**\n\nPrompt\n\n   Katy makes coffee using teaspoons of sugar and cups of water in the\n   ratio of 7:13. If she used a total of 120 teaspoons of sugar and cups\n   of water, calculate the number of teaspoonfuls of sugar she used.\n\nSolution\n\n   The total ratio representing the ingredients she used to make the\n   coffee is 7+13 = <<7+13=20>>20 Since the fraction representing the\n   number of teaspoons she used is 7/20, she used 7/20\\ *120 =\n   <<7/20*\\ 120=42>>42 #### 42\n\nStep 1: Prepare dataset\n-----------------------\n\n.. code:: bash\n\n   cd examples/data_preprocess\n   python3 gsm8k.py --local_save_dir ~/data/gsm8k\n\nStep 2: Download Model\n----------------------\n\nThere're three ways to prepare the model checkpoints for post-training:\n\n- Download the required models from huggingface or modelscope\n\n.. code:: bash\n\n   huggingface-cli download deepseek-ai/deepseek-math-7b-instruct --local-dir ~/models/deepseek-math-7b-instruct --local-dir-use-symlinks False\n   # or\n   modelscope download --model deepseek-ai/deepseek-math-7b-instruct --local_dir ~/models/deepseek-math-7b-instruct\n\n- Already store your store model in the local directory or HDFS path.\n- Also, you can directly use the model name in huggingface (e.g.,\n  deepseek-ai/deepseek-math-7b-instruct) in\n  ``actor_rollout_ref.model.path`` and ``critic.model.path`` field in\n  the run script. You can also download models from modelscope by setting environmental variable ``VERL_USE_MODELSCOPE=True``.\n  See examples/ppo_trainer/run_deepseek7b_llm_modelscope.sh for example.\n\nNoted that users should prepare checkpoints for actor, critic and reward\nmodel.\n\n[Optional] Step 3: SFT your Model\n---------------------------------\n\nWe provide a SFT Trainer using PyTorch FSDP in\n`fsdp_sft_trainer.py < \nUsers can customize their own SFT\nscript using our FSDP SFT Trainer.\n```\n\nSource: docs/index.rst:1-80\n```text\nWelcome to verl's documentation!\n================================================\n\nverl is a flexible, efficient and production-ready RL training framework designed for large language models (LLMs) post-training. It is an open source implementation of the `HybridFlow < paper.\n\nverl is flexible and easy to use with:\n\n- **Easy extension of diverse RL algorithms**: The hybrid programming model combines the strengths of single-controller and multi-controller paradigms to enable flexible representation and efficient execution of complex Post-Training dataflows. Allowing users to build RL dataflows in a few lines of code.\n\n- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as PyTorch FSDP, Megatron-LM, vLLM and SGLang. Moreover, users can easily extend to other LLM training and inference frameworks.\n\n- **Flexible device mapping and parallelism**: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.\n\n- Ready integration with popular HuggingFace models\n\n\nverl is fast with:\n\n- **State-of-the-art throughput**: By seamlessly integrating existing SOTA LLM training and inference frameworks, verl achieves high generation and training throughput.\n\n- **Efficient actor model resharding with 3D-HybridEngine**: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.\n\n--------------------------------------------\n\n.. _Contents:\n\n.. toctree::\n   :maxdepth: 2\n   :caption: Quickstart\n\n   start/install\n   start/quickstart\n   start/multinode\n   start/ray_debug_tutorial\n   start/more_resources\n   start/agentic_rl\n\n.. toctree::\n   :maxdepth: 2\n   :caption: Programming guide\n\n   hybrid_flow\n   single_controller\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Data Preparation\n\n   preparation/prepare_data\n   preparation/reward_function\n\n.. toctree::\n   :maxdepth: 2\n   :caption: Configurations\n\n   examples/config\n\n.. toctree::\n   :maxdepth: 1\n   :caption: PPO Example\n\n   examples/ppo_code_architecture\n   examples/gsm8k_example\n   examples/multi_modal_example\n   examples/skypilot_examples\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Algorithms\n\n   algo/ppo.md\n   algo/grpo.md\n   algo/collabllm.md\n   algo/dapo.md\n   algo/spin.md\n   algo/sppo.md\n   algo/entropy.md\n   algo/opo.md\n   algo/baseline.md\n   algo/gpg.md\n```\n\nSource: docs/start/install.rst:1-80\n```text\nInstallation\n============\n\nRequirements\n------------\n\n- **Python**: Version >= 3.10\n- **CUDA**: Version >= 12.8\n\nverl supports various backends. Currently, the following configurations are available:\n\n- **FSDP** and **Megatron-LM** (optional) for training.\n- **SGLang**, **vLLM** and **TGI** for rollout generation.\n\nChoices of Backend Engines\n----------------------------\n\n1. Training:\n\nWe recommend using **FSDP** backend to investigate, research and prototype different models, datasets and RL algorithms. The guide for using FSDP backend can be found in :doc:`FSDP Workers<../workers/fsdp_workers>`.\n\nFor users who pursue better scalability, we recommend using **Megatron-LM** backend. Currently, we support `Megatron-LM v0.13.1 < The guide for using Megatron-LM backend can be found in :doc:`Megatron-LM Workers<../workers/megatron_workers>`.\n\n\n2. Inference:\n\nFor inference, vllm 0.8.3 and later versions have been tested for stability. We recommend turning on env var `VLLM_USE_V1=1` for optimal performance.\n\nFor SGLang, refer to the :doc:`SGLang Backend<../workers/sglang_worker>` for detailed installation and usage instructions. SGLang rollout is under extensive development and offers many advanced features and optimizations. We encourage users to report any issues or provide feedback via the `SGLang Issue Tracker <\n\nFor huggingface TGI integration, it is usually used for debugging and single GPU exploration.\n\nInstall from docker image\n-------------------------\n\nStart from v0.6.0, we use vllm and sglang release image as our base image.\n\nBase Image\n::::::::::\n\n- vLLM: \n- SGLang: \n\nApplication Image\n:::::::::::::::::\n\nUpon base image, the following packages are added:\n\n- flash_attn\n- Megatron-LM\n- Apex\n- TransformerEngine\n- DeepEP\n\nLatest docker file:\n\n- `Dockerfile.stable.vllm <\n- `Dockerfile.stable.sglang <\n\nAll pre-built images are available in dockerhub: `verlai/verl < For example, ``verlai/verl:sgl055.latest``, ``verlai/verl:vllm011.latest``.\n\nYou can find the latest images used for development and ci in our github workflows:\n\n- `.github/workflows/vllm.yml <\n- `.github/workflows/sgl.yml <\n\n\nInstallation from Docker\n::::::::::::::::::::::::\n\nAfter pulling the desired Docker image and installing desired inference and training frameworks, you can run it with the following steps:\n\n1. Launch the desired Docker image and attach into it:\n\n.. code:: bash\n\n    docker create --runtime=nvidia --gpus all --net=host --shm-size=\"10g\" --cap-add=SYS_ADMIN -v .:/workspace/verl --name verl <image:tag> sleep infinity\n    docker start verl\n    docker exec -it verl bash\n```\n\nSource: docs/start/multinode.rst:1-80\n```text\nMultinode Training\n==================\n\nLast updated: 06/10/2025.\n\n.. _wuxibin89: \n\nAuthor: `Xibin Wu < `Yusheng Su <\n\nOption 1: Launch Manually\n------------------------------\n\nSet up multinode ray cluster\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n1. Start head node with ``ray start --head --dashboard-host=0.0.0.0``, there're 2 address you should care about:\n\n- GCS address: ``ray start --address=<address>``, where worker node should connect to.\n- Dashboard address: ``<address>:8265``, where you should submit job to the cluster.\n\n.. image:: \n\n2. Start worker node with ``ray start --address=<address>`` you get above.\n\n.. image:: \n\n3. Now you should see the cluster have 2 nodes with ``ray status``.\n\n.. image:: \n\n4. Additionally, you can access dashboard in the browser with the address you get above. \n\n*Firewall rules maybe need configure to access the dashboard, if there's any trouble, please contact your network administrator.*\n\n.. image:: \n\nSubmit job to ray cluster\n~~~~~~~~~~~~~~~~~~~~~~~~~\n1. Submit ray job to cluster with the dashboard address you get above.\n\n.. code-block:: bash\n\n    ray job submit --address=\" \\\n        --runtime-env=verl/trainer/runtime_env.yaml \\\n        --no-wait \\\n        -- \\\n        python3 -m verl.trainer.main_ppo \\\n        trainer.n_gpus_per_node=8 \\\n        trainer.nnodes=2 \\\n        ...\n\n.. image:: \n\n2. Then you can check the job status with the following commands:\n\n- ray job list: list all jobs submitted to the cluster.\n- ray job logs <Submission ID>: query the logs of the job.\n- ray job status <Submission ID>: query the status of the job.\n- ray job stop <Submission ID>: request the job to be stopped.\n- ray job list | grep submission_id | grep JobStatus | grep RUNNING | grep -oP 'raysubmit_^'\\''\"+' | head -n 1: get the latest job submission ID of the running job.\n- ray job logs <Submission ID> --follow: added ``--follow`` parameter to ray job logs command to enable continuous log streaming.\n\n3. You can also access driver/task/actor logs in ``/tmp/ray/session_latest/logs/``, driver log is ``job-driver-raysubmit_<Submission ID>.log``.\n\n4. We strongly recommend you to view job detail from dashboard in multinode training, because it provide more structure way to view the job information.\n\n.. image:: \n.. image:: \n\nOption 2: Launch via SkyPilot on Kubernetes or clouds\n------------------------------------------------------\n\n.. note::\n   Ready-to-use SkyPilot example configurations are available in the `examples/skypilot/ < directory:\n   \n   - ``verl-ppo.yaml`` - PPO training with GSM8K dataset\n   - ``verl-grpo.yaml`` - GRPO training with MATH dataset  \n   - ``verl-multiturn-tools.yaml`` - Multi-turn tool usage training\n   \n   See the `SkyPilot examples README < for detailed usage instructions.\n```\n\nSource: docs/start/quickstart.rst:1-80\n```text\n.. _quickstart:\n\n=========================================================\nQuickstart: PPO training on GSM8K dataset\n=========================================================\n\nPost-train a LLM using GSM8K dataset.\n\nIntroduction\n------------\n\n.. _hf_dataset_gsm8k: \n\nIn this example, we train an LLM to tackle the `GSM8k <hf_dataset_gsm8k>`_ task with function-based rewards. [1]_\n\nPrerequisite:\n\n- the latest version of ``verl`` and its dependencies installed following the installation guide. Using the docker image is recommended.\n\n- a GPU with at least 24 GB HBM\n\n\nDataset Introduction\n--------------------\n\nGSM8k is a math problem dataset. The prompt is an elementary school\nproblem. The LLM model is asked to solve the math problem. Below is an example:\n\nPrompt\n\n   Katy makes coffee using teaspoons of sugar and cups of water in the\n   ratio of 7:13. If she used a total of 120 teaspoons of sugar and cups\n   of water, calculate the number of teaspoonfuls of sugar she used.\n\nSolution\n\n   The total ratio representing the ingredients she used to make the\n   coffee is 7+13 = <<7+13=20>>20 Since the fraction representing the\n   number of teaspoons she used is 7/20, she used 7/20\\ *120 =\n   <<7/20*\\ 120=42>>42 #### 42\n\nStep 1: Prepare the dataset\n----------------------------\n\nWe preprocess the dataset in parquet format so that (1) it contains necessary fields for computing RL rewards and (2) is faster to read.\n\n.. code-block:: bash\n\n   python3 examples/data_preprocess/gsm8k.py --local_save_dir ~/data/gsm8k\n\nStep 2: Download a model for post-training\n-------------------------------------------\n\nIn this example, we start with the ``Qwen2.5-0.5B-Instruct`` model.\n\nIf you want to perform SFT before RL, refer to the :doc:`Complete GSM8K Example<../examples/gsm8k_example>`, the `sft directory < and `SFT Trainer < for further details.\n\n.. code-block:: bash\n\n   python3 -c \"import transformers; transformers.pipeline('text-generation', model='Qwen/Qwen2.5-0.5B-Instruct')\"\n\nStep 3: Perform PPO training with the instruct model\n----------------------------------------------------------------------\n\n**Reward Model/Function**\n\nWe use a pre-defined rule-based reward model. We force the model to produce a final\nanswer following 4 â€œ#â€ as shown in the solution. We extract the final\nanswer from both the solution and model's output using regular\nexpression matching. We assign a reward of 1 to correct\nanswer, 0.0 to incorrect answer and 0 to no answer. \n\nFor more details, please refer to `verl/utils/reward_score/gsm8k.py <\n\n**Training Script**\n\nNow let's run PPO training with the dataset and model above. [2]_\n\n\nSet the ``data.train_files`` ,\\ ``data.val_files``, ``actor_rollout_ref.model.path`` and ``critic.model.path`` based on your dataset and model names or paths.\n```\n\nSource: docs/start/ray_debug_tutorial.rst:1-80\n```text\nRay Debug Tutorial\n==================\n\nLast updated: 04/23/2025\n\n\n.. _wuxibin89: \n\nAuthor: `Ao Shen <\n\nHow to debug?\n---------------------\n\n\nRay Distributed Debugger VSCode Extension (Recommended)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n1. Starting with RayÂ 2.39, Anyscale has introduced the `Ray Distributed Debugger < VSCode extension. Follow the extensionâ€™s installation instructions, then add your cluster using the dashboard URL you obtained earlier.\n\n   .. image:: \n      :alt: Ray Distributed Debugger VSCode extension screenshot\n\n2. Prerequisites.\n\n   Ensure the following are installed (see the extension README for more detail):\n\n   - Visual Studio Code  \n   - `ray[default]`Â >=Â 2.9.1  \n   - `debugpy`Â >=Â 1.8.0  \n\n   .. image:: \n      :alt: VSCode with Ray prerequisites\n\n3. Environment Variables.\n\n   To enable postâ€‘mortem debugging, set:\n\n   .. code-block:: bash\n\n      export RAY_DEBUG_POST_MORTEM=1\n\n   .. admonition:: Note\n      :class: important\n\n      Be sure to remove any legacy flags before starting Ray:\n\n      - `RAY_DEBUG=legacy`  \n      - `--ray-debugger-external`\n\n4. Configuring BreakpointsSet up breakpoint() in your code, and submit job to cluster. Then the extension will show the breakpoint information.\n\n\n   1. Insert `breakpoint()` calls into your remote functions.  \n   2. Submit your job to the cluster.  \n\n   The extension will detect active breakpoints and display them in VSCode.\n\n   **Note:** Breakpoints are only supported inside functions decorated with `@ray.remote`.\n\n5. Launching the Debugger.\n\n   Run your job directly from the command line (do not use a `launch.json`):\n\n   .. code-block:: bash\n\n      python job.py\n\n6. Attaching to a Breakpoint.\n\n Once the process hits the first `breakpoint()`, click the Ray Distributed Debugger icon in the VSCode sidebar to attach the debugger.\n\n   .. image:: \n      :alt: Attaching VSCode debugger to Ray process\n\n7. Debugging With Multiple breakpoint().\n\n   For each subsequent task, first disconnect the current debugger session, then click the extension icon again to attach to the next breakpoint.\n\n   .. image:: \n      :alt: Disconnecting and reconnecting the debugger\n```\n\nSource: README.md:1-27\n```markdown\n<div align=\"center\">\n ðŸ‘‹ Hi, everyone!\n    verl is a RL training library initiated by <b>ByteDance Seed team</b> and maintained by the verl community.\n    <br>\n    <br>\n</div>\n\n<div align=\"center\">\n\n<a href=\" src=\" alt=\"Ask DeepWiki.com\" style=\"height:20px;\"></a>\nGitHub Repo stars\nTwitter\n<a href=\" src=\"\n<a href=\" src=\"\nDocumentation\n<a href=\" src=\"\n\n</div>\n\nseed logo\n\n<h1 style=\"text-align: center;\">verl: Volcano Engine Reinforcement Learning for LLMs</h1>\n\nverl is a flexible, efficient and production-ready RL training library for large language models (LLMs).\n\nverl is the open-source version of **HybridFlow: A Flexible and Efficient RLHF Framework** paper.\n```\n\nSource: docs/index.rst:1-22\n```text\nWelcome to verl's documentation!\n================================================\n\nverl is a flexible, efficient and production-ready RL training framework designed for large language models (LLMs) post-training. It is an open source implementation of the `HybridFlow < paper.\n\nverl is flexible and easy to use with:\n\n- **Easy extension of diverse RL algorithms**: The hybrid programming model combines the strengths of single-controller and multi-controller paradigms to enable flexible representation and efficient execution of complex Post-Training dataflows. Allowing users to build RL dataflows in a few lines of code.\n\n- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as PyTorch FSDP, Megatron-LM, vLLM and SGLang. Moreover, users can easily extend to other LLM training and inference frameworks.\n\n- **Flexible device mapping and parallelism**: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.\n\n- Ready integration with popular HuggingFace models\n\n\nverl is fast with:\n\n- **State-of-the-art throughput**: By seamlessly integrating existing SOTA LLM training and inference frameworks, verl achieves high generation and training throughput.\n\n- **Efficient actor model resharding with 3D-HybridEngine**: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.\n```\n\nSource: README.md:24-44\n```markdown\nverl is a flexible, efficient and production-ready RL training library for large language models (LLMs).\n\nverl is the open-source version of **HybridFlow: A Flexible and Efficient RLHF Framework** paper.\n\nverl is flexible and easy to use with:\n\n- **Easy extension of diverse RL algorithms**: The hybrid-controller programming model enables flexible representation and efficient execution of complex post-training dataflows. Build RL dataflows such as GRPO, PPO in a few lines of code.\n\n- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as FSDP, Megatron-LM, vLLM, SGLang, etc\n\n- **Flexible device mapping**: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.\n\n- Ready integration with popular HuggingFace models\n\nverl is fast with:\n\n- **State-of-the-art throughput**: SOTA LLM training and inference engine integrations and SOTA RL throughput.\n\n- **Efficient actor model resharding with 3D-HybridEngine**: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.\n\n</p>\n```\n\nSource: docs/index.rst:4-22\n```text\nverl is a flexible, efficient and production-ready RL training framework designed for large language models (LLMs) post-training. It is an open source implementation of the `HybridFlow < paper.\n\nverl is flexible and easy to use with:\n\n- **Easy extension of diverse RL algorithms**: The hybrid programming model combines the strengths of single-controller and multi-controller paradigms to enable flexible representation and efficient execution of complex Post-Training dataflows. Allowing users to build RL dataflows in a few lines of code.\n\n- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as PyTorch FSDP, Megatron-LM, vLLM and SGLang. Moreover, users can easily extend to other LLM training and inference frameworks.\n\n- **Flexible device mapping and parallelism**: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.\n\n- Ready integration with popular HuggingFace models\n\n\nverl is fast with:\n\n- **State-of-the-art throughput**: By seamlessly integrating existing SOTA LLM training and inference frameworks, verl achieves high generation and training throughput.\n\n- **Efficient actor model resharding with 3D-HybridEngine**: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.\n```\n\nSource: README.md:28-34\n```markdown\nverl is flexible and easy to use with:\n\n- **Easy extension of diverse RL algorithms**: The hybrid-controller programming model enables flexible representation and efficient execution of complex post-training dataflows. Build RL dataflows such as GRPO, PPO in a few lines of code.\n\n- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as FSDP, Megatron-LM, vLLM, SGLang, etc\n\n- **Flexible device mapping**: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.\n```\n\nSource: docs/index.rst:6-10\n```text\nverl is flexible and easy to use with:\n\n- **Easy extension of diverse RL algorithms**: The hybrid programming model combines the strengths of single-controller and multi-controller paradigms to enable flexible representation and efficient execution of complex Post-Training dataflows. Allowing users to build RL dataflows in a few lines of code.\n\n- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as PyTorch FSDP, Megatron-LM, vLLM and SGLang. Moreover, users can easily extend to other LLM training and inference frameworks.\n```\n\nSource: docs/hybrid_flow.rst:1-80\n```text\n=========================================================\nHybridFlow Programming Guide\n=========================================================\n\nLast updated: 06/02/2025.\n\n.. _vermouth: \n\nAuthor: `Chi Zhang <\n\nverl is an open source implementation of the paper `HybridFlow < [1]_. In this section, we will introduce the basic concepts of HybridFlow, the motivation and how to program with verl APIs.\n\nMotivation and Design\n------------------------\nWe use dataflow to represent RL systems. [4]_.\n\nDataFlow\n~~~~~~~~~~~~~~~~~~~~\n\nDataflow is an abstraction of computations. Neural Network training is a typical dataflow. It can be represented by computational graph. \n\n.. image:: \n   :alt: The dataflow graph from CS231n 2024 lecture 4\n\nThis figure [2]_ represents the computation graph of a polynomial function followed by a sigmoid function. In the data flow of neural network computation, each node represents an operator, and each edge represents the direction of forward/backward propagation. The computation graph determines the architecture of the neural network.\n\nRL as a dataflow problem\n++++++++++++++++++++++++++++++++++++++++++++++\n\nReinforcement learning (RL) training can also be represented as a dataflow. Below is the dataflow graph that represents the PPO algorithm used in RLHF [3]_:\n\n.. image:: \n  :alt: PPO dataflow graph, credit to Zhihu ä½Žçº§ç‚¼ä¸¹å¸ˆ\n\nHowever, the dataflow of RL has fundamental differences compared with dataflow of neural network training as follows:\n\n+--------------------------+--------------------------------------------------+---------------------+\n| Workload                 | Node                                             | Edge                |\n+--------------------------+--------------------------------------------------+---------------------+\n| Neural Network Training  | Operator (+/-/matmul/softmax)                    | Tensor movement     |\n+--------------------------+--------------------------------------------------+---------------------+\n| Reinforcement Learning   | High-level operators (rollout/model forward)     | Data Movement       |\n+--------------------------+--------------------------------------------------+---------------------+\n\nIn the case of tabular reinforcement learning, each operator is a simple scalar math operation (e.g., bellman update). In deep reinforcement learning(DRL), each operator is a high-level neural network computation such as model inference/update. This makes RL a two-level dataflow problem:\n\n- Control flow: defines how the high-level operators are executed (e.g., In PPO, we first perform rollout. Then, we perform advantage computation. Finally, we perform training). It expresses the **core logics of RL algorithms**.\n- Computation flow: defines the dataflow of **neural network computation** (e.g., model forward/backward/optimizer).\n\n\nDesign Choices\n~~~~~~~~~~~~~~~~~~~~\nThe model size used in DRL before the LLM era is typically small. Thus, the high-level neural network computation can be done in a single process. This enables embedding the computation flow inside the control flow as a single process.\n\nHowever, in the LLM era, the computation flow (e.g., training neural network) becomes a multi-process program. This naturally leads to two design choices:\n\n1. Convert the control flow into a multi-process program as well. Then colocate with computation flow (unified multi-controller)\n\n- Advantages:\n\n  - Achieves the **optimal performance** under fixed computation flow and control flow as the communication overhead in both training and data transfer is minimized.\n\n- Disadvantages:\n\n  - The computation and/or control flow is **hard to reuse** from software perspective as computation code is coupled with specific controller code. For example, the training loop of PPO is generic. Say we have an PPO training flow implemented with a specific computation flow such as FSDP. Neither the control flow or computation flow can be reused if we want to switch the computation flow from FSDP to Megatron, due to the coupling of control and computation flows.\n  - Requires more efforts from the user under flexible and dynamic control flows, due to the multi-process nature of the program.\n\n2. Separate the flows: single process for the control flow and multi-process for computation flow\n\n- Advantages:\n\n  - The computation flow defined elsewhere can be **easily reused** after the decoupling.\n  - The controller runs on a single process. Implementing a new RL algorithm with a **different control flow is simple and easy**.\n\n- Disadvantages:\n\n  - Additional **data communication overhead** each time the controller process and computatation processes interact. The data has to be sent back and forth.\n\nIn verl, the latter strategy with separate control flow and computation flow is adopted. verl is designed to decouple the control flow of RL algorithms, and the implementation of computation engines.\n```\n\nSource: verl/trainer/ppo/ray_trainer.py:1-80\n```python\n# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright 2023-2024 SGLang Team\n# Copyright 2025 ModelBest Inc. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     \n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nPPO Trainer with Ray-based single controller.\nThis trainer supports model-agonistic model initialization with huggingface\n\"\"\"\n\nimport json\nimport os\nimport uuid\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom dataclasses import dataclass, field\nfrom pprint import pprint\nfrom typing import Any, Optional\n\nimport numpy as np\nimport ray\nimport torch\nfrom omegaconf import OmegaConf, open_dict\nfrom torch.utils.data import Dataset, Sampler\nfrom torchdata.stateful_dataloader import StatefulDataLoader\nfrom tqdm import tqdm\n\nfrom verl import DataProto\nfrom verl.experimental.dataset.sampler import AbstractCurriculumSampler\nfrom verl.protocol import pad_dataproto_to_divisor, unpad_dataproto\nfrom verl.single_controller.ray import RayClassWithInitArgs, RayResourcePool, RayWorkerGroup\nfrom verl.single_controller.ray.base import create_colocated_worker_cls\nfrom verl.trainer.config import AlgoConfig\nfrom verl.trainer.ppo import core_algos\nfrom verl.trainer.ppo.core_algos import AdvantageEstimator, agg_loss\nfrom verl.trainer.ppo.metric_utils import (\n    compute_data_metrics,\n    compute_throughout_metrics,\n    compute_timing_metrics,\n    process_validation_metrics,\n)\nfrom verl.trainer.ppo.reward import compute_reward, compute_reward_async\nfrom verl.trainer.ppo.utils import Role, WorkerType, need_critic, need_reference_policy, need_reward_model\nfrom verl.utils import tensordict_utils as tu\nfrom verl.utils.checkpoint.checkpoint_manager import find_latest_ckpt_path, should_save_ckpt_esi\nfrom verl.utils.config import omega_conf_to_dataclass\nfrom verl.utils.debug import marked_timer\nfrom verl.utils.import_utils import load_class_from_fqn\nfrom verl.utils.metric import reduce_metrics\nfrom verl.utils.py_functional import rename_dict\nfrom verl.utils.rollout_skip import RolloutSkip\nfrom verl.utils.seqlen_balancing import calculate_workload, get_seqlen_balanced_partitions, log_seqlen_unbalance\nfrom verl.utils.torch_functional import masked_mean\nfrom verl.utils.tracking import ValidationGenerationsLogger\nfrom verl.workers.config import FSDPEngineConfig\nfrom verl.workers.utils.padding import left_right_2_no_padding, no_padding_2_padding\n\n\n@dataclass\nclass ResourcePoolManager:\n    \"\"\"\n    Define a resource pool specification. Resource pool will be initialized first.\n    \"\"\"\n\n    resource_pool_spec: dict[str, list[int]]\n    mapping: dict[Role, str]\n    resource_pool_dict: dict[str, RayResourcePool] = field(default_factory=dict)\n\n    def create_resource_pool(self):\n        \"\"\"Create Ray resource pools for distributed training.\n```\n\nSource: verl/trainer/main_ppo.py:1-80\n```python\n# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     \n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nNote that we don't combine the main with ray_trainer as ray_trainer is used by other mpain.\n\"\"\"\n\nimport os\nimport socket\n\nimport hydra\nimport ray\nfrom omegaconf import OmegaConf\n\nfrom verl.experimental.dataset.sampler import AbstractSampler\nfrom verl.trainer.constants_ppo import get_ppo_ray_runtime_env\nfrom verl.trainer.ppo.ray_trainer import RayPPOTrainer\nfrom verl.trainer.ppo.reward import load_reward_manager\nfrom verl.trainer.ppo.utils import need_critic, need_reference_policy\nfrom verl.utils.config import validate_config\nfrom verl.utils.device import auto_set_ascend_device_name, is_cuda_available\nfrom verl.utils.import_utils import load_extern_object\n\n\n@hydra.main(config_path=\"config\", config_name=\"ppo_trainer\", version_base=None)\ndef main(config):\n    \"\"\"Main entry point for PPO training with Hydra configuration management.\n\n    Args:\n        config_dict: Hydra configuration dictionary containing training parameters.\n    \"\"\"\n    # Automatically set `config.trainer.device = npu` when running on Ascend NPU.\n    auto_set_ascend_device_name(config)\n\n    run_ppo(config)\n\n\n# Define a function to run the PPO-like training process\ndef run_ppo(config, task_runner_class=None) -> None:\n    \"\"\"Initialize Ray cluster and run distributed PPO training process.\n\n    Args:\n        config: Training configuration object containing all necessary parameters\n                for distributed PPO training including Ray initialization settings,\n                model paths, and training hyperparameters.\n        task_runner_class: For recipe to change TaskRunner.\n    \"\"\"\n    # Check if Ray is not initialized\n    if not ray.is_initialized():\n        # Initialize Ray with a local cluster configuration\n        # Set environment variables in the runtime environment to control tokenizer parallelism,\n        # NCCL debug level, VLLM logging level, and allow runtime LoRA updating\n        # `num_cpus` specifies the number of CPU cores Ray can use, obtained from the configuration\n        default_runtime_env = get_ppo_ray_runtime_env()\n        ray_init_kwargs = config.ray_kwargs.get(\"ray_init\", {})\n        runtime_env_kwargs = ray_init_kwargs.get(\"runtime_env\", {})\n\n        if config.transfer_queue.enable:\n            # Add runtime environment variables for transfer queue\n            runtime_env_vars = runtime_env_kwargs.get(\"env_vars\", {})\n            runtime_env_vars[\"TRANSFER_QUEUE_ENABLE\"] = \"1\"\n            runtime_env_kwargs[\"env_vars\"] = runtime_env_vars\n\n        runtime_env = OmegaConf.merge(default_runtime_env, runtime_env_kwargs)\n        ray_init_kwargs = OmegaConf.create({**ray_init_kwargs, \"runtime_env\": runtime_env})\n        print(f\"ray init kwargs: {ray_init_kwargs}\")\n        ray.init(**OmegaConf.to_container(ray_init_kwargs))\n\n    if task_runner_class is None:\n        task_runner_class = ray.remote(num_cpus=1)(TaskRunner)  # please make sure main_task is not scheduled on head\n```\n\nSource: verl/utils/dataset/rl_dataset.py:1-80\n```python\n# Copyright 2024 Bytedance Ltd. and/or its affiliates\n# Copyright 2023-2024 SGLang Team\n# Copyright 2025 ModelBest Inc. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     \n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\nimport logging\nimport os\nimport re\nimport traceback\nfrom collections import defaultdict\nfrom typing import Optional\n\nimport datasets\nimport numpy as np\nimport torch\nfrom omegaconf import DictConfig, ListConfig\nfrom torch.utils.data import Dataset\nfrom transformers import PreTrainedTokenizer, ProcessorMixin\n\nimport verl.utils.torch_functional as verl_F\nfrom verl.utils.model import compute_position_id_with_mask\n\nlogger = logging.getLogger(__name__)\n\n\ndef collate_fn(data_list: list[dict]) -> dict:\n    \"\"\"\n    Collate a batch of sample dicts into batched tensors and arrays.\n\n    Args:\n        data_list: List of dicts mapping feature names to torch.Tensor or other values.\n\n    Returns:\n        Dict where tensor entries are stacked into a torch.Tensor of shape\n        (batch_size, \\\\*dims) and non-tensor entries are converted to\n        np.ndarray of dtype object with shape (batch_size,).\n    \"\"\"\n    tensors = defaultdict(list)\n    non_tensors = defaultdict(list)\n\n    for data in data_list:\n        for key, val in data.items():\n            if isinstance(val, torch.Tensor):\n                tensors[key].append(val)\n            else:\n                non_tensors[key].append(val)\n\n    for key, val in tensors.items():\n        tensors[key] = torch.stack(val, dim=0)\n\n    for key, val in non_tensors.items():\n        non_tensors[key] = np.fromiter(val, dtype=object, count=len(val))\n\n    return {**tensors, **non_tensors}\n\n\nclass RLHFDataset(Dataset):\n    \"\"\"\n    Load and preprocess RLHF data from Parquet files.\n\n    - Caches files locally.\n    - Reads into a HuggingFace Dataset and tokenizes prompts.\n    - Optionally handles images/videos via a ProcessorMixin.\n    - Filters prompts over a max length.\n    - Supports resuming from checkpoints.\n\n    Args:\n        data_files (str or list): Path(s) to Parquet file(s).\n```\n\nSource: docs/start/install.rst:10-32\n```text\nverl supports various backends. Currently, the following configurations are available:\n\n- **FSDP** and **Megatron-LM** (optional) for training.\n- **SGLang**, **vLLM** and **TGI** for rollout generation.\n\nChoices of Backend Engines\n----------------------------\n\n1. Training:\n\nWe recommend using **FSDP** backend to investigate, research and prototype different models, datasets and RL algorithms. The guide for using FSDP backend can be found in :doc:`FSDP Workers<../workers/fsdp_workers>`.\n\nFor users who pursue better scalability, we recommend using **Megatron-LM** backend. Currently, we support `Megatron-LM v0.13.1 < The guide for using Megatron-LM backend can be found in :doc:`Megatron-LM Workers<../workers/megatron_workers>`.\n\n\n2. Inference:\n\nFor inference, vllm 0.8.3 and later versions have been tested for stability. We recommend turning on env var `VLLM_USE_V1=1` for optimal performance.\n\nFor SGLang, refer to the :doc:`SGLang Backend<../workers/sglang_worker>` for detailed installation and usage instructions. SGLang rollout is under extensive development and offers many advanced features and optimizations. We encourage users to report any issues or provide feedback via the `SGLang Issue Tracker <\n\nFor huggingface TGI integration, it is usually used for debugging and single GPU exploration.\n```\n\nSource: verl/trainer/ppo/ray_trainer.py:200-500\n```python\n    Args:\n        data (DataProto): The data containing batched model outputs and inputs.\n        adv_estimator (AdvantageEstimator): The advantage estimator to use (e.g., GAE, GRPO, REINFORCE++).\n        gamma (float, optional): Discount factor for future rewards. Defaults to 1.0.\n        lam (float, optional): Lambda parameter for GAE. Defaults to 1.0.\n        num_repeat (int, optional): Number of times to repeat the computation. Defaults to 1.\n        norm_adv_by_std_in_grpo (bool, optional): Whether to normalize advantages by standard deviation in\n            GRPO. Defaults to True.\n        config (dict, optional): Configuration dictionary for algorithm settings. Defaults to None.\n\n    Returns:\n        DataProto: The updated data with computed advantages and returns.\n    \"\"\"\n    # Back-compatible with trainers that do not compute response mask in fit\n    if \"response_mask\" not in data.batch.keys():\n        data.batch[\"response_mask\"] = compute_response_mask(data)\n    # prepare response group\n    if adv_estimator == AdvantageEstimator.GAE:\n        # Compute advantages and returns using Generalized Advantage Estimation (GAE)\n        advantages, returns = core_algos.compute_gae_advantage_return(\n            token_level_rewards=data.batch[\"token_level_rewards\"],\n            values=data.batch[\"values\"],\n            response_mask=data.batch[\"response_mask\"],\n            gamma=gamma,\n            lam=lam,\n        )\n        data.batch[\"advantages\"] = advantages\n        data.batch[\"returns\"] = returns\n        if config.get(\"use_pf_ppo\", False):\n            data = core_algos.compute_pf_ppo_reweight_data(\n                data,\n                config.pf_ppo.get(\"reweight_method\"),\n                config.pf_ppo.get(\"weight_pow\"),\n            )\n    elif adv_estimator == AdvantageEstimator.GRPO:\n        # Initialize the mask for GRPO calculation\n        grpo_calculation_mask = data.batch[\"response_mask\"]\n\n        # Call compute_grpo_outcome_advantage with parameters matching its definition\n        advantages, returns = core_algos.compute_grpo_outcome_advantage(\n            token_level_rewards=data.batch[\"token_level_rewards\"],\n            response_mask=grpo_calculation_mask,\n            index=data.non_tensor_batch[\"uid\"],\n            norm_adv_by_std_in_grpo=norm_adv_by_std_in_grpo,\n        )\n        data.batch[\"advantages\"] = advantages\n        data.batch[\"returns\"] = returns\n    else:\n        # handle all other adv estimator type other than GAE and GRPO\n        adv_estimator_fn = core_algos.get_adv_estimator_fn(adv_estimator)\n        adv_kwargs = {\n            \"token_level_rewards\": data.batch[\"token_level_rewards\"],\n            \"response_mask\": data.batch[\"response_mask\"],\n            \"config\": config,\n        }\n        if \"uid\" in data.non_tensor_batch:  # optional\n            adv_kwargs[\"index\"] = data.non_tensor_batch[\"uid\"]\n        if \"reward_baselines\" in data.batch:  # optional\n            adv_kwargs[\"reward_baselines\"] = data.batch[\"reward_baselines\"]\n\n        # calculate advantage estimator\n        advantages, returns = adv_estimator_fn(**adv_kwargs)\n        data.batch[\"advantages\"] = advantages\n        data.batch[\"returns\"] = returns\n    return data\n\n\nclass RayPPOTrainer:\n    \"\"\"Distributed PPO trainer using Ray for scalable reinforcement learning.\n\n    This trainer orchestrates distributed PPO training across multiple nodes and GPUs,\n    managing actor rollouts, critic training, and reward computation with Ray backend.\n    Supports various model architectures including FSDP, Megatron, vLLM, and SGLang integration.\n    \"\"\"\n\n    # TODO: support each role have individual ray_worker_group_cls,\n    # i.e., support different backend of different role\n    def __init__(\n        self,\n        config,\n```\n\nSource: docs/examples/config.rst:1-80\n```text\n.. _config-explain-page:\n\nConfig Explanation\n===================\n\nLast updated: 06/18/2025.\n\nppo_trainer.yaml for RL FSDP Backend\n-------------------------------------\n\nData\n~~~~\n\n.. code:: yaml\n\n   data:\n     tokenizer: null\n     train_files: ~/data/rlhf/gsm8k/train.parquet\n     val_files: ~/data/rlhf/gsm8k/test.parquet\n     train_max_samples: -1  # set to -1 to use full dataset\n     val_max_samples: -1  # set to -1 to use full dataset\n     prompt_key: prompt\n     max_prompt_length: 512\n     max_response_length: 512\n     train_batch_size: 1024\n     return_raw_input_ids: False  # This should be set to true when the tokenizer between policy and rm differs\n     return_raw_chat: False\n     return_full_prompt: False\n     shuffle: True\n     seed: 42\n     filter_overlong_prompts: False\n     filter_overlong_prompts_workers: 1\n     truncation: error\n     image_key: images\n     trust_remote_code: True\n     custom_cls:\n        path: null\n        name: null\n\n- ``data.train_files``: Training set parquet. Can be a list or a single\n  file. The program will read all files into memory, so it can't be too\n  large (< 100GB). The path can be either local path or HDFS path. For\n  HDFS path, we provide utils to download it to DRAM and convert the\n  HDFS path to local path.\n- ``data.val_files``: Validation parquet. Can be a list or a single\n  file.\n- ``data.train_max_samples``: Maximum number of samples to use from the\n  training dataset. Set to -1 to use the full dataset.\n- ``data.val_max_samples``: Maximum number of samples to use from the\n  validation dataset. Set to -1 to use the full dataset.\n- ``data.prompt_key``: The field in the dataset where the prompt is\n  located. Default is 'prompt'.\n- ``data.max_prompt_length``: Maximum prompt length. All prompts will be\n  left-padded to this length. An error will be reported if the length is\n  too long\n- ``data.max_response_length``: Maximum response length. Rollout in RL\n  algorithms (e.g. PPO) generates up to this length\n- ``data.train_batch_size``: Batch size sampled for one training\n  iteration of different RL algorithms.\n- ``data.return_raw_input_ids``: Whether to return the original\n  input_ids without adding chat template. This is mainly used to\n  accommodate situations where the reward model's chat template differs\n  from the policy. It needs to be decoded first, then apply the RM's\n  chat template. If using a model-based RM, and the policy and RM\n  chat_templates are different, this flag needs to be set\n- ``data.return_raw_chat``: Whether to return the original chat (prompt)\n  without applying chat template.\n- ``data.return_full_prompt``: Whether to return the full prompt with chat template\n- ``data.shuffle``: Whether to shuffle the data in the dataloader.\n- ``data.seed``: An integer seed to use when shuffling the data. If not set or set to\n  `null`, the data shuffling will not be seeded, resulting in a different data order on each run.\n- ``data.filter_overlong_prompts``: Default don't filter.\n- ``data.filter_overlong_prompts_workers``: For large-scale dataset, filtering\n  overlong prompts could be timeconsuming. You cat set the ``filter_overlong_prompts_workers``\n  to use multiprocessing for speed up. Default to 1.\n- ``data.truncation``: Truncate the input_ids or prompt length if they\n  exceed max_prompt_length. Default is 'error', not allow exceed the\n  max_prompt_length. The users should increase the max_prompt_length if\n  throwing the error. You can also set ``left``, ``right`` and ``middle``. \n  When ``middle`` is selected, the logic splits the allowed max length roughly in half\n```\n\nSource: docs/start/quickstart.rst:80-110\n```text\nSet the ``data.train_files`` ,\\ ``data.val_files``, ``actor_rollout_ref.model.path`` and ``critic.model.path`` based on your dataset and model names or paths.\nYou may set ``VERL_USE_MODELSCOPE=True`` to download models from `modelscope < instead of `huggingface <\n\n.. code-block:: bash\n\n   PYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \\\n    data.train_files=$HOME/data/gsm8k/train.parquet \\\n    data.val_files=$HOME/data/gsm8k/test.parquet \\\n    data.train_batch_size=256 \\\n    data.max_prompt_length=512 \\\n    data.max_response_length=512 \\\n    actor_rollout_ref.model.path=Qwen/Qwen2.5-0.5B-Instruct \\\n    actor_rollout_ref.actor.optim.lr=1e-6 \\\n    actor_rollout_ref.actor.ppo_mini_batch_size=64 \\\n    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \\\n    actor_rollout_ref.rollout.name=vllm \\\n    actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 \\\n    actor_rollout_ref.rollout.tensor_model_parallel_size=1 \\\n    actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \\\n    actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \\\n    critic.optim.lr=1e-5 \\\n    critic.model.path=Qwen/Qwen2.5-0.5B-Instruct \\\n    critic.ppo_micro_batch_size_per_gpu=4 \\\n    algorithm.kl_ctrl.kl_coef=0.001 \\\n    trainer.logger=console \\\n    trainer.val_before_train=False \\\n    trainer.n_gpus_per_node=1 \\\n    trainer.nnodes=1 \\\n    trainer.save_freq=10 \\\n    trainer.test_freq=10 \\\n    trainer.total_epochs=15 2>&1 | tee verl_demo.log\n```\n\nSource: docs/index.rst:25-37\n```text\n.. _Contents:\n\n.. toctree::\n   :maxdepth: 2\n   :caption: Quickstart\n\n   start/install\n   start/quickstart\n   start/multinode\n   start/ray_debug_tutorial\n   start/more_resources\n   start/agentic_rl\n```\n\nSource: README.md:102-157\n```markdown\n## Getting Started\n\n<a href=\"\n\n**Quickstart:**\n\n- Installation\n- Quickstart\n- Programming Guide & Tech Talk (in Chinese)\n- PPO in verl\n- GRPO in verl\n\n**Running a PPO example step-by-step:**\n\n- Prepare Data for Post-Training\n- Implement Reward Function for Dataset\n- PPO Example Architecture\n- Config Explanation\n\n**Reproducible algorithm baselines:**\n\n- RL performance on coding, math\n\n**For code explanation and advance usage (extension):**\n\n- PPO Trainer and Workers\n  - PPO Ray Trainer\n  - PyTorch FSDP Backend\n  - Megatron-LM Backend\n\n- Advanced Usage and Extension\n  - Add Models with the FSDP Backend\n  - Add Models with the Megatron-LM Backend\n  - Multi-turn Rollout Support\n  - Search Tool Integration\n  - Sandbox Fusion Integration\n  - Deployment using Separate GPU Resources\n  - Extend to Other RL(HF) algorithms\n  - Ray API design tutorial\n\n**Blogs from the community**\n\n- When Reasoning Models Break Tokenization: The Hidden Complexity of Multiturn Training\n- verl deployment on AWS SageMaker\n- verl x SGLang Multi-turn Code Walkthrough\n- Optimizing SGLang Memory Usage in verl\n- SGLang, verl, OpenBMB and Tsinghua University: Pioneering End-to-End Multi-Turn RLHF\n- Reinforcement Learning from Human Feedback on AMD GPUs with verl and ROCm Integration\n- veMLP x verl ï¼šçŽ©è½¬å¼ºåŒ–å­¦ä¹ è®­ç»ƒ\n- ä½¿ç”¨ verl è¿›è¡Œ GRPO åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœ€ä½³å®žè·µ\n- HybridFlow verl åŽŸæ–‡æµ…æž\n- æœ€é«˜æå‡ 20 å€åžåé‡ï¼è±†åŒ…å¤§æ¨¡åž‹å›¢é˜Ÿå‘å¸ƒå…¨æ–° RLHF æ¡†æž¶ï¼ŒçŽ°å·²å¼€æºï¼\n\n## Performance Tuning Guide\n\nThe performance is essential for on-policy RL algorithm. We have written a detailed performance tuning guide to help you optimize performance.\n```\n\nContinue with: System Architecture and HybridFlow Design.",
    