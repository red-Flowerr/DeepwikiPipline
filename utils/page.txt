# System Architecture and HybridFlow Design

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [README.md](README.md)
- [docs/examples/config.rst](docs/examples/config.rst)
- [docs/examples/gsm8k_example.rst](docs/examples/gsm8k_example.rst)
- [docs/index.rst](docs/index.rst)
- [docs/start/install.rst](docs/start/install.rst)
- [docs/start/multinode.rst](docs/start/multinode.rst)
- [docs/start/quickstart.rst](docs/start/quickstart.rst)
- [docs/start/ray_debug_tutorial.rst](docs/start/ray_debug_tutorial.rst)
- [verl/trainer/config/ppo_megatron_trainer.yaml](verl/trainer/config/ppo_megatron_trainer.yaml)
- [verl/trainer/config/ppo_trainer.yaml](verl/trainer/config/ppo_trainer.yaml)
- [verl/trainer/main_ppo.py](verl/trainer/main_ppo.py)
- [verl/trainer/ppo/core_algos.py](verl/trainer/ppo/core_algos.py)
- [verl/trainer/ppo/ray_trainer.py](verl/trainer/ppo/ray_trainer.py)

</details>

This document describes verl's system architecture and the HybridFlow design pattern that enables flexible, efficient reinforcement learning training for large language models. It covers the Ray-based orchestration layer, modular backend architecture, resource management, and the 3D-HybridEngine that enables efficient transitions between training and inference modes.

For algorithm-specific details (PPO, GRPO, etc.), see [#4](#4) and [#5](#5). For worker implementation details, see [#6](#6). For engine-level abstractions (FSDP, Megatron), see [#8](#8). For rollout/inference systems, see [#7](#7).

verl is the open-source implementation of the **HybridFlow** paper, which introduces a hybrid programming model that combines single-controller and multi-controller paradigms for distributed RL training. The key innovation is separating control flow (orchestration) from computation (worker execution) to enable flexible composition of different frameworks.

The system uses a **single-controller** architecture where one driver process orchestrates all distributed workers through Ray. This design provides:

- **Centralized control flow**: The `RayPPOTrainer` on the driver node sequences all training phases
- **Decoupled computation**: Workers execute in parallel without inter-worker coordination
- **Flexible scheduling**: Ray dynamically assigns tasks to available workers
- **Easy debugging**: Centralized logs and error handling

```mermaid
graph TB
    Driver["Driver Process<br/>(RayPPOTrainer)"]
    Ray["Ray Cluster<br/>(ray.init)"]
    
    subgraph "Resource Pools"
        GlobalPool["global_pool<br/>(ResourcePoolManager)"]
        RewardPool["reward_pool<br/>(optional)"]
    end
    
    subgraph "Worker Groups"
        ActorRollout["ActorRolloutRef<br/>WorkerGroup"]
        Critic["Critic<br/>WorkerGroup"]
        RM["RewardModel<br/>WorkerGroup"]
    end
    
    Driver --> Ray
    Ray --> GlobalPool
    Ray --> RewardPool
    
    GlobalPool --> ActorRollout
    GlobalPool --> Critic
    RewardPool --> RM
    
    ActorRollout --> |"Training: update_actor()"| ActorWorkers["Actor Workers<br/>(FSDP/Megatron)"]
    ActorRollout --> |"Inference: generate_sequences()"| RolloutServers["Rollout Servers<br/>(vLLM/SGLang)"]
    Critic --> CriticWorkers["Critic Workers<br/>(FSDP/Megatron)"]
    RM --> RMWorkers["RM Workers<br/>(FSDP/Megatron)"]
```

**Sources:** [README.md:26-32](), [verl/trainer/main_ppo.py:48-106](), [verl/trainer/ppo/ray_trainer.py:267-342]()

The **hybrid engine** refers to the ability to use different backends for different phases of training:

| Phase | Backend Options | Purpose |
|-------|----------------|---------|
| **Training** | FSDP, Megatron-LM | Parameter updates with gradient computation |
| **Rollout** | vLLM, SGLang, HuggingFace | High-throughput sequence generation |
| **Reference Policy** | Same as training backend | KL divergence computation |
| **Critic** | FSDP, Megatron-LM | Value function training |

The key insight is that training requires full gradients and optimizer states, while rollout only needs forward passes with efficient KV-cache management. By using specialized backends for each phase, verl achieves higher throughput than systems that use a single backend for everything.

**Sources:** [README.md:38-43](), [verl/trainer/config/ppo_trainer.yaml:44-57]()

The `RayPPOTrainer` class is the heart of the system, implementing the single-controller pattern. It runs on the driver node and orchestrates the entire training loop.

```mermaid
graph TB
    subgraph "RayPPOTrainer"
        Init["__init__()<br/>Setup: config, tokenizer,<br/>resource pools, datasets"]
        InitWorkers["init_workers()<br/>Create WorkerGroups<br/>for each role"]
        Fit["fit()<br/>Main training loop"]
        
        subgraph "Training Iteration"
            Gen["generate_sequences()<br/>â ActorRollout WG"]
            Reward["compute_reward()<br/>â RewardModel WG"]
            RefLog["compute_ref_log_prob()<br/>â RefPolicy WG"]
            Values["compute_values()<br/>â Critic WG"]
            Adv["compute_advantage()<br/>Local: core_algos"]
            UpdateActor["update_actor()<br/>â Actor WG"]
            UpdateCritic["update_critic()<br/>â Critic WG"]
        end
    end
    
    Init --> InitWorkers
    InitWorkers --> Fit
    Fit --> Gen
    Gen --> Reward
    Reward --> RefLog
    RefLog --> Values
    Values --> Adv
    Adv --> UpdateActor
    UpdateActor --> UpdateCritic
    UpdateCritic --> |"Next iteration"| Gen
```

**Key responsibilities:**

1. **Initialization** ([ray_trainer.py:277-357]()):
   - Store configuration, tokenizer, reward functions
   - Create dataloaders with curriculum sampling support
   - Initialize KL controllers for reward shaping

2. **Worker Management** ([ray_trainer.py:734-890]()):
   - Create `RayWorkerGroup` instances for each role
   - Allocate GPU resources via `ResourcePoolManager`
   - Initialize workers with model checkpoints and configurations

3. **Training Loop Orchestration** ([ray_trainer.py:1081-1390]()):
   - Sequence generation via rollout workers
   - Reward computation (model-based or function-based)
   - Reference policy log-prob computation
   - Value function computation via critic
   - Advantage estimation using various algorithms (GAE, GRPO, etc.)
   - Policy and value function updates with mini-batch splitting

4. **Validation and Checkpointing** ([ray_trainer.py:607-718]()):
   - Periodic validation on held-out dataset
   - Checkpoint saving with configurable frequency
   - Metrics logging to WandB/TensorBoard

**Sources:** [verl/trainer/ppo/ray_trainer.py:267-1511](), [Diagram 2 from high-level diagrams]()

The `ResourcePoolManager` manages GPU resource allocation across different worker types. It maps logical roles to physical GPU resources.

```mermaid
graph LR
    subgraph "ResourcePoolManager"
        Spec["resource_pool_spec<br/>{pool_name: [gpus_per_node]}"]
        Mapping["mapping<br/>{Role: pool_name}"]
        Pools["resource_pool_dict<br/>{pool_name: RayResourcePool}"]
    end
    
    subgraph "Example Configuration"
        GlobalPool["global_pool: [8, 8]<br/>(2 nodes, 8 GPUs each)"]
        RewardPoolEx["reward_pool: [4]<br/>(1 node, 4 GPUs)"]
    end
    
    subgraph "Role Mapping"
        ActorRole["Role.ActorRollout<br/>â global_pool"]
        CriticRole["Role.Critic<br/>â global_pool"]
        RMRole["Role.RewardModel<br/>â reward_pool"]
    end
    
    Spec --> GlobalPool
    Spec --> RewardPoolEx
    Mapping --> ActorRole
    Mapping --> CriticRole
    Mapping --> RMRole
    
    GlobalPool --> |"create_resource_pool()"| Pools
    RewardPoolEx --> |"create_resource_pool()"| Pools
```

The `ResourcePoolManager` is initialized in `TaskRunner` based on configuration:

| Configuration Field | Purpose |
|---------------------|---------|
| `trainer.nnodes` | Number of nodes in cluster |
| `trainer.n_gpus_per_node` | GPUs per node for main pool |
| `reward_model.enable_resource_pool` | Whether to create separate reward model pool |
| `reward_model.nnodes`, `reward_model.n_gpus_per_node` | Separate pool for reward model |

**Key methods:**

- `create_resource_pool()` ([ray_trainer.py:79-96]()): Creates Ray resource pools with `max_colocate_count=3` to allow actor, rollout, and reference to share resources
- `get_resource_pool(role)` ([ray_trainer.py:99-101]()): Returns the pool assigned to a given role
- `get_n_gpus()` ([ray_trainer.py:103-105]()): Returns total GPU count for validation

**Sources:** [verl/trainer/ppo/ray_trainer.py:69-124](), [verl/trainer/main_ppo.py:206-226]()

verl uses a **role-based** architecture where each worker type has a specific responsibility. Roles are defined in the `Role` enum and mapped to Ray remote worker classes.

```mermaid
graph TB
    subgraph "Role Enum"
        ActorRollout["Role.ActorRollout"]
        ActorRolloutRef["Role.ActorRolloutRef"]
        RefPolicy["Role.RefPolicy"]
        Critic["Role.Critic"]
        RewardModel["Role.RewardModel"]
    end
    
    subgraph "Worker Classes (FSDP Backend)"
        ActorRolloutWorkerFSDP["ActorRolloutRefWorker<br/>(fsdp_workers)"]
        CriticWorkerFSDP["CriticWorker<br/>(fsdp_workers)"]
        RMWorkerFSDP["RewardModelWorker<br/>(fsdp_workers)"]
    end
    
    subgraph "Worker Classes (Megatron Backend)"
        ActorRolloutWorkerMega["ActorRolloutRefWorker<br/>(megatron_workers)"]
        CriticWorkerMega["CriticWorker<br/>(megatron_workers)"]
        RMWorkerMega["RewardModelWorker<br/>(megatron_workers)"]
    end
    
    subgraph "Worker Classes (New Engine)"
        ActorRolloutWorkerNew["ActorRolloutRefWorker<br/>(engine_workers)"]
        TrainingWorkerNew["TrainingWorker<br/>(engine_workers)"]
    end
    
    ActorRollout --> |"use_legacy_worker_impl=enable"| ActorRolloutWorkerFSDP
    ActorRollout --> |"strategy=megatron"| ActorRolloutWorkerMega
    ActorRollout --> |"use_legacy_worker_impl=disable"| ActorRolloutWorkerNew
    
    Critic --> |"use_legacy_worker_impl=enable"| CriticWorkerFSDP
    Critic --> |"strategy=megatron"| CriticWorkerMega
    Critic --> |"use_legacy_worker_impl=disable"| TrainingWorkerNew
    
    RewardModel --> RMWorkerFSDP
    RewardModel --> RMWorkerMega
```

**Role mapping logic** in `TaskRunner`:

1. **ActorRollout/ActorRolloutRef** ([main_ppo.py:123-177]()):
   - Combines actor training, rollout generation, and optionally reference policy
   - Uses `ActorRolloutRefWorker` or `AsyncActorRolloutRefWorker` based on rollout mode
   - In new engine (`use_legacy_worker_impl=disable`), reference is fused into same worker

2. **Critic** ([main_ppo.py:179-204]()):
   - Separate value function model
   - Can use `CriticWorker` (legacy) or `TrainingWorker` (new engine)

3. **RewardModel** ([main_ppo.py:228-252]()):
   - Optional model-based reward computation
   - Separate resource pool supported for isolation

4. **RefPolicy** ([main_ppo.py:254-266]()):
   - Only created if KL loss or KL reward is enabled
   - Not needed in new engine (fused into ActorRolloutRef)

**Sources:** [verl/trainer/main_ppo.py:108-302](), [verl/trainer/ppo/utils.py:10-43]()

The `TaskRunner` is a Ray remote class that encapsulates the entire setup and execution workflow. It's instantiated as a Ray actor by the main entry point.

```mermaid
sequenceDiagram
    participant Main as main_ppo.py<br/>main()
    participant Ray as Ray Cluster
    participant TaskRunner as TaskRunner<br/>(Ray Actor)
    participant Trainer as RayPPOTrainer
    
    Main->>Ray: ray.init()
    Note over Ray: Initialize Ray with<br/>runtime_env settings
    
    Main->>TaskRunner: ray.remote(TaskRunner).remote()
    Note over TaskRunner: Create remote actor<br/>on driver node
    
    Main->>TaskRunner: runner.run.remote(config)
    Note over TaskRunner: Execute setup and training
    
    TaskRunner->>TaskRunner: add_actor_rollout_worker()
    TaskRunner->>TaskRunner: add_critic_worker()
    TaskRunner->>TaskRunner: add_reward_model_worker()
    TaskRunner->>TaskRunner: add_ref_policy_worker()
    
    Note over TaskRunner: Build role_worker_mapping<br/>and resource pool mapping
    
    TaskRunner->>TaskRunner: init_resource_pool_mgr()
    Note over TaskRunner: Create ResourcePoolManager
    
    TaskRunner->>TaskRunner: Load tokenizer, datasets
    TaskRunner->>TaskRunner: Load reward functions
    
    TaskRunner->>Trainer: RayPPOTrainer(...)
    Note over Trainer: Initialize trainer with<br/>workers and resources
    
    TaskRunner->>Trainer: trainer.init_workers()
    Note over Trainer: Create all worker groups
    
    TaskRunner->>Trainer: trainer.fit()
    Note over Trainer: Start training loop
```

**Key setup steps:**

1. **Worker Registration** ([main_ppo.py:123-266]()):
   - Dynamically add worker classes based on configuration
   - Supports FSDP, Megatron, and new engine implementations
   - Creates `role_worker_mapping` dict: `{Role: ray.remote(WorkerClass)}`

2. **Resource Pool Initialization** ([main_ppo.py:206-226]()):
   - Creates pools based on `trainer.nnodes` and `trainer.n_gpus_per_node`
   - Optional separate pool for reward model
   - Maps roles to pools via `mapping` dict

3. **Data and Reward Setup** ([main_ppo.py:312-353]()):
   - Load tokenizer and optional processor (for multimodal)
   - Create train/val datasets with `create_rl_dataset()`
   - Load reward manager for training and validation

4. **Trainer Initialization** ([main_ppo.py:356-374]()):
   - Instantiate `RayPPOTrainer` with all components
   - Call `trainer.init_workers()` to create worker groups
   - Call `trainer.fit()` to start training

**Sources:** [verl/trainer/main_ppo.py:108-374](), [verl/trainer/main_ppo.py:35-106]()

verl supports two distributed training backends with different trade-offs:

| Backend | Use Case | Parallelism | Scale | Complexity |
|---------|----------|-------------|-------|------------|
| **FSDP** | Research, prototyping, 7B-70B models | ZeRO-3 data parallel, Ulysses sequence parallel | 8-64 GPUs | Low (PyTorch native) |
| **Megatron-LM** | Production, 70B+ models, MoE | Tensor (TP), Pipeline (PP), Context (CP), Expert (EP), Data (DP) | 100s-1000s GPUs | High (Megatron-Core) |

Both backends implement the same worker interface, allowing seamless switching via configuration:

```yaml
# FSDP Backend
actor_rollout_ref:
  actor:
    strategy: fsdp
    fsdp_config:
      param_offload: False
      optimizer_offload: False
```

```yaml
# Megatron Backend
actor_rollout_ref:
  actor:
    strategy: megatron
    megatron_config:
      tensor_model_parallel_size: 4
      pipeline_model_parallel_size: 2
```

**Sources:** [docs/examples/config.rst:258-262](), [verl/trainer/config/ppo_trainer.yaml:12-13](), [verl/trainer/config/ppo_megatron_trainer.yaml:4-5]()

Similarly, rollout (inference) can use different backends optimized for throughput:

| Backend | Strengths | Integration Method | Memory Management |
|---------|-----------|-------------------|-------------------|
| **vLLM** | High throughput, PagedAttention, continuous batching | ZeroMQ distributed executor | Manual weight sync |
| **SGLang** | Advanced features (RadixAttention, constrained decoding, multi-turn) | HTTP server with ServerAdapter | Manual weight sync |

```mermaid
graph TB
    subgraph "Training Phase"
        ActorParams["Actor Parameters<br/>(FSDP/Megatron)"]
    end
    
    subgraph "Rollout Phase"
        vLLMEngine["vLLM Engine<br/>(ZeroMQ)"]
        SGLangEngine["SGLang Engine<br/>(HTTP)"]
    end
    
    ActorParams --> |"wake_up()<br/>Weight sync"| vLLMEngine
    ActorParams --> |"wake_up()<br/>Weight sync"| SGLangEngine
    
    vLLMEngine --> |"generate_sequences()"| Responses["Generated Responses"]
    SGLangEngine --> |"generate_sequences()"| Responses
    
    Responses --> |"sleep()<br/>Free memory"| ActorParams
```

**Weight synchronization** is handled by the HybridEngine:

- **vLLM**: Uses custom `ExternalZeroMQDistributedExecutor` that receives weights via ZeroMQ
- **SGLang**: Uses `ServerAdapter` that uploads weights via HTTP API

**Sources:** [Diagram 4 from high-level diagrams](), [docs/start/install.rst:26-31]()

The **3D-HybridEngine** is verl's key innovation for efficient actor model management. It enables switching between three modes:

1. **Training Mode**: Full model with gradients, optimizer states, FSDP/Megatron sharding
2. **Inference Mode**: Forward-only model loaded in vLLM/SGLang with different sharding (e.g., tensor parallel)
3. **Reference Mode**: Frozen model for KL divergence computation

**Mode transition workflow:**

```mermaid
stateDiagram-v2
    [*] --> Training: init_workers()
    
    Training --> Inference: wake_up()
    Note right of Inference: 1. Sync weights to rollout<br/>2. Free training memory<br/>3. Allocate inference memory
    
    Inference --> Training: sleep()
    Note left of Training: 1. Free inference memory<br/>2. Reallocate training memory<br/>3. Resume optimizer states
    
    Training --> Reference: compute_ref_log_prob()
    Note right of Reference: Actor without LoRA<br/>or separate ref model
    
    Reference --> Training: End computation
```

**Key optimizations:**

1. **Memory Efficiency** ([README.md:42]()): 
   - Training and inference share GPUs but not simultaneously
   - `sleep()` frees inference KV cache and model weights
   - `wake_up()` frees training intermediate activations

2. **Reduced Communication**:
   - Weights transferred once per iteration, not per sample
   - Efficient serialization using dtensor format for FSDP
   - For Megatron, weights already in correct TP/PP layout

3. **Rollout Correction** ([ray_trainer.py:126-165]()):
   - Handles distribution mismatch between training (e.g., FP32) and rollout (e.g., BF16)
   - Importance sampling and rejection sampling supported
   - Configurable via `algorithm.rollout_correction` settings

**Sources:** [README.md:38-43](), [docs/examples/config.rst:131-137](), [Diagram 3 from high-level diagrams]()

Backend selection is entirely configuration-driven. The `TaskRunner.add_actor_rollout_worker()` method dynamically imports the appropriate worker class:

```python
# Simplified logic from main_ppo.py
if config.actor_rollout_ref.actor.strategy in {"fsdp", "fsdp2"}:
    from verl.workers.fsdp_workers import ActorRolloutRefWorker
elif config.actor_rollout_ref.actor.strategy == "megatron":
    from verl.workers.megatron_workers import ActorRolloutRefWorker
```

For rollout:

```python
# Rollout backend selection
if config.actor_rollout_ref.rollout.name == "vllm":
    # vLLM with ZeroMQ executor
elif config.actor_rollout_ref.rollout.name == "sglang":
    # SGLang with HTTP server
```

**Sources:** [verl/trainer/main_ppo.py:146-173](), [docs/examples/config.rst:340-413]()

Ray is initialized in `main_ppo.py` with a custom runtime environment that sets essential environment variables for distributed training:

```python
def get_ppo_ray_runtime_env():
    return {
        "env_vars": {
            "TOKENIZERS_PARALLELISM": "true",
            "NCCL_DEBUG": "WARN", 
            "VLLM_LOGGING_LEVEL": "WARNING",
            "VLLM_ALLOW_RUNTIME_LORA_UPDATING": "1",
        }
    }
```

**Key initialization parameters:**

| Parameter | Purpose | Configuration |
|-----------|---------|---------------|
| `num_cpus` | CPU resources Ray can use | `ray_kwargs.ray_init.num_cpus` |
| `runtime_env` | Environment variables | Merged with default runtime env |
| `address` | Head node address (for workers) | Auto-detected in single-node |

For multi-node setup:
1. Head node starts with `ray start --head`
2. Worker nodes join with `ray start --address=<head_ip>:6379`
3. Driver submits job with `ray job submit`

**Sources:** [verl/trainer/main_ppo.py:59-77](), [verl/trainer/constants_ppo.py:1-31](), [docs/start/multinode.rst:13-66]()

verl uses two key abstractions from its single-controller framework:

**RayResourcePool** ([verl/single_controller/ray/base.py]()):
- Represents a pool of GPUs across multiple nodes
- Specified as `process_on_nodes`: list of GPU counts per node, e.g., `[8, 8]` = 2 nodes with 8 GPUs each
- Supports `max_colocate_count`: number of worker groups that can share the pool (default 3 for actor, rollout, ref)

**RayWorkerGroup** ([verl/single_controller/ray/base.py]()):
- Manages a group of Ray actors performing the same role
- Methods called on worker group are broadcast to all actors
- Returns aggregated results (e.g., metrics from all workers)

```mermaid
graph TB
    subgraph "ResourcePoolManager"
        GlobalPool["global_pool<br/>RayResourcePool<br/>process_on_nodes=[8,8]"]
    end
    
    subgraph "Worker Groups on global_pool"
        ActorWG["actor_rollout_wg<br/>RayWorkerGroup<br/>world_size=16"]
        CriticWG["critic_wg<br/>RayWorkerGroup<br/>world_size=16"]
    end
    
    subgraph "Ray Actors (16 total)"
        Actor1["Actor Ray Actor<br/>GPU 0, Node 1"]
        Actor2["Actor Ray Actor<br/>GPU 1, Node 1"]
        ActorN["Actor Ray Actor<br/>GPU 7, Node 2"]
        
        Critic1["Critic Ray Actor<br/>GPU 0, Node 1"]
        Critic2["Critic Ray Actor<br/>GPU 1, Node 1"]
        CriticN["Critic Ray Actor<br/>GPU 7, Node 2"]
    end
    
    GlobalPool --> ActorWG
    GlobalPool --> CriticWG
    
    ActorWG --> Actor1
    ActorWG --> Actor2
    ActorWG --> ActorN
    
    CriticWG --> Critic1
    CriticWG --> Critic2
    CriticWG --> CriticN
```

**Worker group operations:**

```python
# Broadcast: call method on all workers
self.actor_rollout_wg.update_actor(batch)

# Reduce: aggregate results from all workers  
metrics = self.actor_rollout_wg.get_metrics()
# Returns: combined dict with metrics from all actors

# Scatter-Gather: split data, process in parallel, recombine
outputs = self.actor_rollout_wg.generate_sequences(split_batch)
```

**Sources:** [verl/trainer/ppo/ray_trainer.py:734-890](), [verl/single_controller/ray/base.py:1-500]()

The worker initialization process in `RayPPOTrainer.init_workers()` follows these steps:

```mermaid
sequenceDiagram
    participant Trainer as RayPPOTrainer
    participant RPM as ResourcePoolManager
    participant ActorWG as actor_rollout_wg
    participant CriticWG as critic_wg
    participant RMWG as reward_model_wg
    
    Trainer->>RPM: create_resource_pool()
    Note over RPM: Initialize Ray resource pools
    
    Trainer->>Trainer: _init_actors()
    Trainer->>RPM: get_resource_pool(Role.ActorRollout)
    Trainer->>ActorWG: Create RayWorkerGroup
    Note over ActorWG: Instantiate Ray actors<br/>on allocated GPUs
    ActorWG->>ActorWG: init_model()
    ActorWG->>ActorWG: init_tokenizer()
    
    Trainer->>Trainer: _init_critic()
    Trainer->>RPM: get_resource_pool(Role.Critic)
    Trainer->>CriticWG: Create RayWorkerGroup
    CriticWG->>CriticWG: init_model()
    
    alt Reward model enabled
        Trainer->>Trainer: _init_reward_model()
        Trainer->>RPM: get_resource_pool(Role.RewardModel)
        Trainer->>RMWG: Create RayWorkerGroup
        RMWG->>RMWG: init_model()
    end
    
    Trainer->>Trainer: Load checkpoint if resuming
    Trainer->>ActorWG: load_checkpoint()
    Trainer->>CriticWG: load_checkpoint()
```

**Key initialization methods:**

1. `_init_actors()` ([ray_trainer.py:734-788]()): Creates actor/rollout/ref worker group with hybrid engine configuration
2. `_init_critic()` ([ray_trainer.py:790-833]()): Creates critic worker group if needed
3. `_init_reward_model()` ([ray_trainer.py:835-869]()): Creates reward model worker group if enabled
4. `_resume_from_checkpoint()` ([ray_trainer.py:948-1079]()): Loads checkpoints for resuming training

**Sources:** [verl/trainer/ppo/ray_trainer.py:892-1079]()

verl uses Hydra for hierarchical configuration composition. The main config file (`ppo_trainer.yaml`) uses the `defaults` list to compose component configs:

```yaml
defaults:
  - actor@actor_rollout_ref.actor: dp_actor
  - data@data: legacy_data
  - reward_manager@reward_manager
  - ref@actor_rollout_ref.ref: dp_ref
  - rollout@actor_rollout_ref.rollout: rollout
  - model@actor_rollout_ref.model: hf_model
  - critic@critic: dp_critic
  - reward_model@reward_model: dp_reward_loop
  - algorithm@algorithm.rollout_correction: rollout_correction
  - _self_
```

**Syntax explanation:**
- `actor@actor_rollout_ref.actor: dp_actor` loads `actor/dp_actor.yaml` and places it at `config.actor_rollout_ref.actor`
- `_self_` means fields in current file override composed configs

**Component config locations:**

| Component | Config Directory | Purpose |
|-----------|-----------------|---------|
| Actor | `trainer/config/actor/` | Training strategy (FSDP/Megatron) |
| Data | `trainer/config/data/` | Dataset paths and preprocessing |
| Model | `trainer/config/model/` | Model path and architecture overrides |
| Rollout | `trainer/config/rollout/` | Inference engine settings (vLLM/SGLang) |
| Critic | `trainer/config/critic/` | Value function training |
| Algorithm | `trainer/config/algorithm/` | RL algorithm parameters |

**Sources:** [verl/trainer/config/ppo_trainer.yaml:8-41](), [docs/examples/config.rst:1-10]()

Configs can be overridden at runtime using Hydra's command-line syntax:

```bash
python3 -m verl.trainer.main_ppo \
    data.train_batch_size=512 \
    actor_rollout_ref.actor.strategy=fsdp \
    trainer.n_gpus_per_node=8 \
    trainer.nnodes=2
```

This enables easy hyperparameter sweeps and environment-specific settings without modifying config files.

**Sources:** [docs/start/quickstart.rst:83-110](), [verl/trainer/main_ppo.py:35-45]()

The complete PPO training iteration involves coordination between multiple worker groups:

```mermaid
graph TB
    Start["fit() starts iteration"]
    
    subgraph "Phase 1: Rollout"
        GetBatch["Get batch from dataloader"]
        GenSeq["actor_rollout_wg.generate_sequences()"]
        Responses["DataProto with responses,<br/>old_log_probs"]
    end
    
    subgraph "Phase 2: Reward & Value"
        ComputeReward["compute_reward()<br/>(RewardModel WG or function)"]
        RefLogProb["actor_rollout_wg.compute_ref_log_prob()"]
        KLPenalty["apply_kl_penalty()<br/>(local computation)"]
        ComputeValues["critic_wg.compute_values()"]
    end
    
    subgraph "Phase 3: Advantage"
        CompAdv["compute_advantage()<br/>(local: core_algos.GAE/GRPO)"]
        AddedFields["DataProto with advantages,<br/>returns, token_level_rewards"]
    end
    
    subgraph "Phase 4: Update"
        BalanceBatch["Balance batch by sequence length"]
        SplitBatch["Split into mini-batches<br/>(size=ppo_mini_batch_size)"]
        
        loop "ppo_epochs"
            UpdateCritic["critic_wg.update_critic()"]
            UpdateActor["actor_rollout_wg.update_actor()"]
        end
        
        Metrics["Aggregate metrics from all workers"]
    end
    
    Start --> GetBatch
    GetBatch --> GenSeq
    GenSeq --> Responses
    
    Responses --> ComputeReward
    ComputeReward --> RefLogProb
    RefLogProb --> KLPenalty
    KLPenalty --> ComputeValues
    
    ComputeValues --> CompAdv
    CompAdv --> AddedFields
    
    AddedFields --> BalanceBatch
    BalanceBatch --> SplitBatch
    SplitBatch --> UpdateCritic
    UpdateCritic --> UpdateActor
    UpdateActor --> Metrics
    
    Metrics --> |"Next iteration"| Start
```

**Key data structures:**

- **DataProto** ([verl/protocol.py]()): Unified data structure containing:
  - `batch`: Tensor dict (prompts, responses, masks, log_probs, values, etc.)
  - `non_tensor_batch`: Non-tensor metadata (uids, data_source, etc.)
  - `meta_info`: Control flags (eos_token_id, do_sample, etc.)

**Sources:** [verl/trainer/ppo/ray_trainer.py:1081-1390](), [Diagram 2 from high-level diagrams]()

The verl architecture achieves flexibility through several design principles:

1. **Separation of Concerns**: Orchestration (RayPPOTrainer) is decoupled from computation (workers)
2. **Backend Abstraction**: Common interfaces allow swapping FSDP â Megatron and vLLM â SGLang
3. **Resource Management**: ResourcePoolManager enables flexible GPU allocation and isolation
4. **Hybrid Engine**: 3D-HybridEngine efficiently switches between training and inference modes
5. **Configuration Composition**: Hydra enables modular, composable configurations

This design makes verl suitable for both research (quick prototyping with FSDP) and production (scaling to 671B models with Megatron).

**Sources:** [README.md:1-244](), All architecture diagrams provided